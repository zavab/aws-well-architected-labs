{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"README.html","text":"AWS Well-Architected Labs Introduction The Well-Architected framework has been developed to help cloud architects build the most secure, high-performing, resilient, and efficient infrastructure possible for their applications. This framework provides a consistent approach for customers and partners to evaluate architectures, and provides guidance to help implement designs that will scale with your application needs over time. This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Labs: The labs are structured around the five pillars of the Well-Architected Framework : Operational Excellence Security Reliability Performance Efficiency Cost Optimization Well-Architected Tool License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018-2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Home"},{"location":"README.html#aws-well-architected-labs","text":"","title":"AWS Well-Architected Labs"},{"location":"README.html#introduction","text":"The Well-Architected framework has been developed to help cloud architects build the most secure, high-performing, resilient, and efficient infrastructure possible for their applications. This framework provides a consistent approach for customers and partners to evaluate architectures, and provides guidance to help implement designs that will scale with your application needs over time. This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced.","title":"Introduction"},{"location":"README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites:"},{"location":"README.html#labs","text":"The labs are structured around the five pillars of the Well-Architected Framework : Operational Excellence Security Reliability Performance Efficiency Cost Optimization Well-Architected Tool","title":"Labs:"},{"location":"README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018-2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/README.html","text":"AWS Well-Architected Cost Optimization Labs http://wellarchitectedlabs.com Introduction Cost optimization is a continual process of refinement and improvement of a system over its entire lifecycle. By using these labs, you gain practical experience on how to implement the Cost Optimization best practices and ensure your workloads are Well-Architected. For more information about cost optimization best practices in the cloud, visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected cost optimization whitepaper. Getting Started - Cost Optimization Fundamentals The first step in your Cost Optimization journey is to setup your account correctly, and get to know the tools and data available for Cost Optimization. These are a collection of labs that are accessible to anyone that will be working with the cloud, including non-technical roles. Fundamentals covers the following: Account setup, AWS billing console, AWS Budgets, AWS Cost Explorer, Reserved Instances (RIs), Cost and Usage Report (CUR), cost and usage analysis, and cost and usage visualization. Expenditure Awareness The capability to attribute resource costs to the individual organization or product owners drives efficient usage behavior and helps reduce waste. Awareness broken into three areas: Usage Governance Monitoring Cost and Usage Decommissioning Resources It covers the following topics: Organizational policies, account structure, groups and roles, cost controls, tracking project lifecycle, cost and usage report, cost attribution categories, organization metrics, tagging, billing and cost management tools, reporting and notification, proactive monitoring, allocating cost, decommissioning process. Cost Effective Resources Using the appropriate resources for your workload is key to cost savings, this is not only the size and type of resource but may include managed or application level services. Using the best pricing models for the resources/services will also ensure the lowest costs. Cost effective resources consists of: Selection of Services Resource Type and Size Pricing Models - Savings Plans (SP) and Reserved Instances (RI) Data Transfer Charges It covers the following topics: Organization cost requirements, analyze workload components, cost modeling, resource type and size, pricing models (Reserved Instances, Spot, OnDemand), region cost, data transfer modeling, data transfer services. Manage Demand and Supply Resources Demand on your systems may or may not be predictable, it can also be static or dynamic. By analyzing and understanding your system demand you can supply resources as required to achieve minimal costs: Manage Demand and Supply Resources It covers the following topics: demand analysis, reactive resource provisioning, dynamic resource provisioning (Auto Scaling, buffers). Optimizing Over Time As AWS releases new services and features, it is a best practice to review your existing architectural decisions to ensure they continue to be the most cost-effective: Evaluate new Services It covers the following topics: evaluating new services.","title":"Overview"},{"location":"Cost/README.html#aws-well-architected-cost-optimization-labs","text":"http://wellarchitectedlabs.com","title":"AWS Well-Architected Cost Optimization Labs"},{"location":"Cost/README.html#introduction","text":"Cost optimization is a continual process of refinement and improvement of a system over its entire lifecycle. By using these labs, you gain practical experience on how to implement the Cost Optimization best practices and ensure your workloads are Well-Architected. For more information about cost optimization best practices in the cloud, visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected cost optimization whitepaper.","title":"Introduction"},{"location":"Cost/README.html#getting-started-cost-optimization-fundamentals","text":"The first step in your Cost Optimization journey is to setup your account correctly, and get to know the tools and data available for Cost Optimization. These are a collection of labs that are accessible to anyone that will be working with the cloud, including non-technical roles. Fundamentals covers the following: Account setup, AWS billing console, AWS Budgets, AWS Cost Explorer, Reserved Instances (RIs), Cost and Usage Report (CUR), cost and usage analysis, and cost and usage visualization.","title":"Getting Started - Cost Optimization Fundamentals"},{"location":"Cost/README.html#expenditure-awareness","text":"The capability to attribute resource costs to the individual organization or product owners drives efficient usage behavior and helps reduce waste. Awareness broken into three areas: Usage Governance Monitoring Cost and Usage Decommissioning Resources It covers the following topics: Organizational policies, account structure, groups and roles, cost controls, tracking project lifecycle, cost and usage report, cost attribution categories, organization metrics, tagging, billing and cost management tools, reporting and notification, proactive monitoring, allocating cost, decommissioning process.","title":"Expenditure Awareness"},{"location":"Cost/README.html#cost-effective-resources","text":"Using the appropriate resources for your workload is key to cost savings, this is not only the size and type of resource but may include managed or application level services. Using the best pricing models for the resources/services will also ensure the lowest costs. Cost effective resources consists of: Selection of Services Resource Type and Size Pricing Models - Savings Plans (SP) and Reserved Instances (RI) Data Transfer Charges It covers the following topics: Organization cost requirements, analyze workload components, cost modeling, resource type and size, pricing models (Reserved Instances, Spot, OnDemand), region cost, data transfer modeling, data transfer services.","title":"Cost Effective Resources"},{"location":"Cost/README.html#manage-demand-and-supply-resources","text":"Demand on your systems may or may not be predictable, it can also be static or dynamic. By analyzing and understanding your system demand you can supply resources as required to achieve minimal costs: Manage Demand and Supply Resources It covers the following topics: demand analysis, reactive resource provisioning, dynamic resource provisioning (Auto Scaling, buffers).","title":"Manage Demand and Supply Resources"},{"location":"Cost/README.html#optimizing-over-time","text":"As AWS releases new services and features, it is a best practice to review your existing architectural decisions to ensure they continue to be the most cost-effective: Evaluate new Services It covers the following topics: evaluating new services.","title":"Optimizing Over Time"},{"location":"Cost/CostEffectiveResources.html","text":"Cost Effective Resources http://wellarchitectedlabs.com About cost effective resources Using the appropriate services, instances and resources for your workload is key to cost savings. A well-architected workload uses the most cost-effective resources, which can have a significant and positive economic impact. You also have the opportunity to use managed services to reduce costs. AWS offers a variety of flexible and cost-effective pricing options to acquire instances from EC2 and other services in a way that best fits your needs. Step 1 - Pricing Models By using the right pricing model for your workload resources, you pay the lowest price for that resource. 100 Level Lab : This lab will introduce you to working with Savings Plans (SP's), utilizing AWS Cost Explorer to make low risk, high return SP purchases for your business. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business. 200 Level Lab : This lab will will create a Savings Plan analysis dashboard. Using this you can completely analyze your usage and select the best commitment for your business. Step 2 - Right Sizing This lab will show you how to use Cost Explorer to perform right sizing recommendations on your resources. This lab will show you how to install the memory agent.","title":"Cost Effective Resources"},{"location":"Cost/CostEffectiveResources.html#cost-effective-resources","text":"http://wellarchitectedlabs.com","title":"Cost Effective Resources"},{"location":"Cost/CostEffectiveResources.html#about-cost-effective-resources","text":"Using the appropriate services, instances and resources for your workload is key to cost savings. A well-architected workload uses the most cost-effective resources, which can have a significant and positive economic impact. You also have the opportunity to use managed services to reduce costs. AWS offers a variety of flexible and cost-effective pricing options to acquire instances from EC2 and other services in a way that best fits your needs.","title":"About cost effective resources"},{"location":"Cost/CostEffectiveResources.html#step-1-pricing-models","text":"By using the right pricing model for your workload resources, you pay the lowest price for that resource. 100 Level Lab : This lab will introduce you to working with Savings Plans (SP's), utilizing AWS Cost Explorer to make low risk, high return SP purchases for your business. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business. 200 Level Lab : This lab will will create a Savings Plan analysis dashboard. Using this you can completely analyze your usage and select the best commitment for your business.","title":"Step 1 - Pricing Models"},{"location":"Cost/CostEffectiveResources.html#step-2-right-sizing","text":"This lab will show you how to use Cost Explorer to perform right sizing recommendations on your resources. This lab will show you how to install the memory agent.","title":"Step 2 - Right Sizing"},{"location":"Cost/ExpenditureAwareness.html","text":"Expenditure Awareness http://wellarchitectedlabs.com About expenditure awareness The capability to attribute resource costs to the individual organization or product owners drives efficient usage behavior and helps reduce waste. Accurate cost attribution allows you to know which products are truly profitable, and allows you to make more informed decisions about where to allocate budget. Step 1 - Cost and Usage Governance - Notifications Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend. Step 2 - Monitor Usage and Cost - Analysis Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts. Step 3 - Monitor Usage and Cost - Visualization Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights. Step 4 - Govern Usage and Cost - Controls Implementing usage controls will ensure excess usage and accompanying costs does not occur. 200 Level Lab : This lab will extend the permissions of the Cost Optimization team, then utilize Identity and Access Management (IAM) policies to control and restrict usage. Step 5 - Monitor Usage and Cost - Advanced Analysis Advanced analysis using your Cost and Usage Report (CUR) will allow you to answer the most challenging questions on your usage and cost. It is the most detailed source of information on your cost and usage available. 200 Level Lab : This lab will utilize Amazon Athena to provide an interface to query the CUR, provide you the most common customer queries, and help you to build your own queries. Step 6 - Monitor Usage and Cost - Advanced Visualization Utilizing the CUR data source in the previous step, you can provide more detailed and custom visualizations and dashboards. 200 Level Lab : This Lab extends the previous step, utilizing Amazon Quicksight to visualize the CUR data source. Step 7 (Optional) - Automated CUR Updates and Ingestion This hands-on lab will guide you through the steps to enable automated updates of your CUR files into Athena, every time a new CUR file is delivered. 300 Level Lab : This lab uses s3 events and Lambda to trigger a Glue crawler and update Athena when a new CUR is delivered. Step 8 (Optional) - Multi-Account CUR Access This hands-on lab will guide you through the different methods to share and analyze cost and usage data across accounts. 300 Level Lab : This Lab demonstrates 3 different ways to access your CUR from another account. Step 9 (Optional) - Splitting and sharing the CUR This hands-on lab will guide you on how to automatically extract part of your CUR file, and then deliver it to another S3 bucket and folder to allow another account to access it. Ideal for partners to deliver a sub-account only CUR to each of its customers, or large enterprises. 300 Level Lab : This Lab uses S3 events, Lambda and Athena to extract part of a CUR file and deliver it to an S3 bucket for another account. Step 10 (Optional) - Automated CUR Reports and email delivery 300 Level Lab : This Lab uses CloudWatch events, Lambda, Athena and SES to run queries against the CUR file, and send financial reports to recipients.","title":"Expenditure Awareness"},{"location":"Cost/ExpenditureAwareness.html#expenditure-awareness","text":"http://wellarchitectedlabs.com","title":"Expenditure Awareness"},{"location":"Cost/ExpenditureAwareness.html#about-expenditure-awareness","text":"The capability to attribute resource costs to the individual organization or product owners drives efficient usage behavior and helps reduce waste. Accurate cost attribution allows you to know which products are truly profitable, and allows you to make more informed decisions about where to allocate budget.","title":"About expenditure awareness"},{"location":"Cost/ExpenditureAwareness.html#step-1-cost-and-usage-governance-notifications","text":"Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend.","title":"Step 1 - Cost and Usage Governance - Notifications"},{"location":"Cost/ExpenditureAwareness.html#step-2-monitor-usage-and-cost-analysis","text":"Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts.","title":"Step 2 - Monitor Usage and Cost - Analysis"},{"location":"Cost/ExpenditureAwareness.html#step-3-monitor-usage-and-cost-visualization","text":"Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights.","title":"Step 3 - Monitor Usage and Cost - Visualization"},{"location":"Cost/ExpenditureAwareness.html#step-4-govern-usage-and-cost-controls","text":"Implementing usage controls will ensure excess usage and accompanying costs does not occur. 200 Level Lab : This lab will extend the permissions of the Cost Optimization team, then utilize Identity and Access Management (IAM) policies to control and restrict usage.","title":"Step 4 - Govern Usage and Cost - Controls"},{"location":"Cost/ExpenditureAwareness.html#step-5-monitor-usage-and-cost-advanced-analysis","text":"Advanced analysis using your Cost and Usage Report (CUR) will allow you to answer the most challenging questions on your usage and cost. It is the most detailed source of information on your cost and usage available. 200 Level Lab : This lab will utilize Amazon Athena to provide an interface to query the CUR, provide you the most common customer queries, and help you to build your own queries.","title":"Step 5 - Monitor Usage and Cost - Advanced Analysis"},{"location":"Cost/ExpenditureAwareness.html#step-6-monitor-usage-and-cost-advanced-visualization","text":"Utilizing the CUR data source in the previous step, you can provide more detailed and custom visualizations and dashboards. 200 Level Lab : This Lab extends the previous step, utilizing Amazon Quicksight to visualize the CUR data source.","title":"Step 6 - Monitor Usage and Cost - Advanced Visualization"},{"location":"Cost/ExpenditureAwareness.html#step-7-optional-automated-cur-updates-and-ingestion","text":"This hands-on lab will guide you through the steps to enable automated updates of your CUR files into Athena, every time a new CUR file is delivered. 300 Level Lab : This lab uses s3 events and Lambda to trigger a Glue crawler and update Athena when a new CUR is delivered.","title":"Step 7 (Optional) - Automated CUR Updates and Ingestion"},{"location":"Cost/ExpenditureAwareness.html#step-8-optional-multi-account-cur-access","text":"This hands-on lab will guide you through the different methods to share and analyze cost and usage data across accounts. 300 Level Lab : This Lab demonstrates 3 different ways to access your CUR from another account.","title":"Step 8 (Optional) - Multi-Account CUR Access"},{"location":"Cost/ExpenditureAwareness.html#step-9-optional-splitting-and-sharing-the-cur","text":"This hands-on lab will guide you on how to automatically extract part of your CUR file, and then deliver it to another S3 bucket and folder to allow another account to access it. Ideal for partners to deliver a sub-account only CUR to each of its customers, or large enterprises. 300 Level Lab : This Lab uses S3 events, Lambda and Athena to extract part of a CUR file and deliver it to an S3 bucket for another account.","title":"Step 9 (Optional) - Splitting and sharing the CUR"},{"location":"Cost/ExpenditureAwareness.html#step-10-optional-automated-cur-reports-and-email-delivery","text":"300 Level Lab : This Lab uses CloudWatch events, Lambda, Athena and SES to run queries against the CUR file, and send financial reports to recipients.","title":"Step 10 (Optional) - Automated CUR Reports and email delivery"},{"location":"Cost/Fundamentals.html","text":"Cost Optimization Fundamentals http://wellarchitectedlabs.com About cost optimization fundamentals The first step in your Cost Optimization journey is to setup your account correctly, and get to know the tools and data available for Cost Optimization. These are a collection of labs that are accessible to anyone that will be working with the cloud, including non-technical roles. Step 1 - Account Setup This first step will help you to you build a basic account structure, and make sure your account is configured correctly. This will ensure you are collecting data for cost optimization, and this data is accessible to the right people within your organization. This is a 100 level lab which requires root access. It must be completed for each AWS account in your organization. Step 2 - Cost and Usage Governance - Notifications Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend. Step 3 - Pricing Models - Reserved Instances By using the right pricing model for your workload resources, you pay the lowest price for that resource. 100 Level Lab : This lab will introduce you to working with Savings Plans (SP's), utilizing AWS Cost Explorer to make low risk, high return SP purchases for your business. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business. Step 4 - Monitor Usage and Cost - Analysis Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts. Step 5 - Monitor Usage and Cost - Visualization Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights. Step 6 - Govern Usage and Cost - Controls Implementing usage controls will ensure excess usage and accompanying costs does not occur. 200 Level Lab : This lab will extend the permissions of the Cost Optimization team, then utilize Identity and Access Management (IAM) policies to control and restrict usage. Step 7 - Monitor Usage and Cost - Advanced Analysis Advanced analysis using your Cost and Usage Report (CUR) will allow you to answer the most challenging questions on your usage and cost. It is the most detailed source of information on your cost and usage available. 200 Level Lab : This lab will utilize Amazon Athena to provide an interface to query the CUR, provide you the most common customer queries, and help you to build your own queries. Step 8 - Monitor Usage and Cost - Advanced Visualization Utilizing the CUR data source in the previous step, you can provide more detailed and custom visualizations and dashboards. 200 Level Lab : This Lab extends the previous step, utilizing Amazon Quicksight to visualize the CUR data source.","title":"Fundamentals"},{"location":"Cost/Fundamentals.html#cost-optimization-fundamentals","text":"http://wellarchitectedlabs.com","title":"Cost Optimization Fundamentals"},{"location":"Cost/Fundamentals.html#about-cost-optimization-fundamentals","text":"The first step in your Cost Optimization journey is to setup your account correctly, and get to know the tools and data available for Cost Optimization. These are a collection of labs that are accessible to anyone that will be working with the cloud, including non-technical roles.","title":"About cost optimization fundamentals"},{"location":"Cost/Fundamentals.html#step-1-account-setup","text":"This first step will help you to you build a basic account structure, and make sure your account is configured correctly. This will ensure you are collecting data for cost optimization, and this data is accessible to the right people within your organization. This is a 100 level lab which requires root access. It must be completed for each AWS account in your organization.","title":"Step 1 - Account Setup"},{"location":"Cost/Fundamentals.html#step-2-cost-and-usage-governance-notifications","text":"Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend.","title":"Step 2 - Cost and Usage Governance - Notifications"},{"location":"Cost/Fundamentals.html#step-3-pricing-models-reserved-instances","text":"By using the right pricing model for your workload resources, you pay the lowest price for that resource. 100 Level Lab : This lab will introduce you to working with Savings Plans (SP's), utilizing AWS Cost Explorer to make low risk, high return SP purchases for your business. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business.","title":"Step 3 - Pricing Models - Reserved Instances"},{"location":"Cost/Fundamentals.html#step-4-monitor-usage-and-cost-analysis","text":"Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts.","title":"Step 4 - Monitor Usage and Cost - Analysis"},{"location":"Cost/Fundamentals.html#step-5-monitor-usage-and-cost-visualization","text":"Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights.","title":"Step 5 - Monitor Usage and Cost - Visualization"},{"location":"Cost/Fundamentals.html#step-6-govern-usage-and-cost-controls","text":"Implementing usage controls will ensure excess usage and accompanying costs does not occur. 200 Level Lab : This lab will extend the permissions of the Cost Optimization team, then utilize Identity and Access Management (IAM) policies to control and restrict usage.","title":"Step 6 - Govern Usage and Cost - Controls"},{"location":"Cost/Fundamentals.html#step-7-monitor-usage-and-cost-advanced-analysis","text":"Advanced analysis using your Cost and Usage Report (CUR) will allow you to answer the most challenging questions on your usage and cost. It is the most detailed source of information on your cost and usage available. 200 Level Lab : This lab will utilize Amazon Athena to provide an interface to query the CUR, provide you the most common customer queries, and help you to build your own queries.","title":"Step 7 - Monitor Usage and Cost - Advanced Analysis"},{"location":"Cost/Fundamentals.html#step-8-monitor-usage-and-cost-advanced-visualization","text":"Utilizing the CUR data source in the previous step, you can provide more detailed and custom visualizations and dashboards. 200 Level Lab : This Lab extends the previous step, utilizing Amazon Quicksight to visualize the CUR data source.","title":"Step 8 - Monitor Usage and Cost - Advanced Visualization"},{"location":"Cost/arc204.html","text":"ARC204 Cost Optimize a Workload https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to perform a baseline on your workload, measure its efficiency and then perform Cost Optimization cycles on it. The first time you perform this lab it is recommended to use the sample files supplied, then you can use your own application and billing files. Goals Understand workload demand profile Understand workload resource profile Understand workload cost Measure workload efficiency in cost per outcome Learn how to find and perform Cost Optimization on a workload Step1 Step2 Step3 Step4 Step5 Step6 License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"ARC204 Cost Optimize a Workload"},{"location":"Cost/arc204.html#arc204-cost-optimize-a-workload","text":"https://wellarchitectedlabs.com","title":"ARC204 Cost Optimize a Workload"},{"location":"Cost/arc204.html#introduction","text":"This hands-on lab will guide you through the steps to perform a baseline on your workload, measure its efficiency and then perform Cost Optimization cycles on it. The first time you perform this lab it is recommended to use the sample files supplied, then you can use your own application and billing files.","title":"Introduction"},{"location":"Cost/arc204.html#goals","text":"Understand workload demand profile Understand workload resource profile Understand workload cost Measure workload efficiency in cost per outcome Learn how to find and perform Cost Optimization on a workload","title":"Goals"},{"location":"Cost/arc204.html#step1","text":"","title":"Step1"},{"location":"Cost/arc204.html#step2","text":"","title":"Step2"},{"location":"Cost/arc204.html#step3","text":"","title":"Step3"},{"location":"Cost/arc204.html#step4","text":"","title":"Step4"},{"location":"Cost/arc204.html#step5","text":"","title":"Step5"},{"location":"Cost/arc204.html#step6","text":"","title":"Step6"},{"location":"Cost/arc204.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/arc204.html#license_1","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/arc204a.html","text":"ARC204 Cost Optimize a Workload https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to perform a baseline on your workload, measure its efficiency and then perform Cost Optimization cycles on it. The first time you perform this lab it is recommended to use the sample files supplied, then you can use your own application and billing files. Goals Understand workload demand profile Understand workload resource profile Understand workload cost Measure workload efficiency in cost per outcome Learn how to find and perform Cost Optimization on a workload Prerequisites A subscription to Amazon QuickSight Basic understanding of SQL preferred Permissions required S3 AWS Glue Amazon Athena AWS Systems manager Amazon QuickSight Step1 Step2 Step3 Step4 Step5 Step6","title":"ARC204 Cost Optimize a Workload"},{"location":"Cost/arc204a.html#arc204-cost-optimize-a-workload","text":"https://wellarchitectedlabs.com","title":"ARC204 Cost Optimize a Workload"},{"location":"Cost/arc204a.html#introduction","text":"This hands-on lab will guide you through the steps to perform a baseline on your workload, measure its efficiency and then perform Cost Optimization cycles on it. The first time you perform this lab it is recommended to use the sample files supplied, then you can use your own application and billing files.","title":"Introduction"},{"location":"Cost/arc204a.html#goals","text":"Understand workload demand profile Understand workload resource profile Understand workload cost Measure workload efficiency in cost per outcome Learn how to find and perform Cost Optimization on a workload","title":"Goals"},{"location":"Cost/arc204a.html#prerequisites","text":"A subscription to Amazon QuickSight Basic understanding of SQL preferred","title":"Prerequisites"},{"location":"Cost/arc204a.html#permissions-required","text":"S3 AWS Glue Amazon Athena AWS Systems manager Amazon QuickSight","title":"Permissions required"},{"location":"Cost/arc204a.html#step1","text":"","title":"Step1"},{"location":"Cost/arc204a.html#step2","text":"","title":"Step2"},{"location":"Cost/arc204a.html#step3","text":"","title":"Step3"},{"location":"Cost/arc204a.html#step4","text":"","title":"Step4"},{"location":"Cost/arc204a.html#step5","text":"","title":"Step5"},{"location":"Cost/arc204a.html#step6","text":"","title":"Step6"},{"location":"Cost/arc219.html","text":"ARC219 AWS Cost Management Tools for Cost and Usage Optimization http://wellarchitectedlabs.com Governance (5min) Perform steps 3 and 4 only . You will implement an AWS Budget to provide notifications on Savings Plan coverage, and create a weekly budget report. (25min) You will use IAM Policies to enforce cost governance by controlling usage. (15min) Beta : You will use Budgets with SNS and Lambda to automatically restrict access if a budget is exceeded. Analysis (5min) Perform Step 4 only . You will use Cost Explorers hourly granularity feature, to view the elasticity in your cost and usage. (25min) You will utilize Glue and Athena to analyze your Cost and Usage Report . (30min) Up to Step 5 only You will use saved queries in Athena, and Lambda, to split a CUR file into multiple parts based on linked/member account ID, and share it with linked accounts. . (20min) You will use CloudWatch to trigger Lambda to run queries in Athena, and then compile the results into a spreadsheet, and email the results using SES. Licensing There are no labs for licensing. Right Sizing (15min) This lab will show you how to use Cost Explorer to perform right sizing recommendations on your resources. (15min) This lab will show you how to install the memory agent. Pricing Models (15min) This lab will show you how to view and analyze your Savings Plan recommendations. (15min) This lab will show you how to perform a Reserved Instance (RI) Analysis, and select low risk, high return RI's for your accounts. Well-Architected Tool Implementing usage controls will ensure excess usage and accompanying costs does not occur. (10min) This lab will walk you through the Well-Architected Tool.","title":"ARC219 AWS Cost Management Tools for Cost and Usage Optimization"},{"location":"Cost/arc219.html#arc219-aws-cost-management-tools-for-cost-and-usage-optimization","text":"http://wellarchitectedlabs.com","title":"ARC219 AWS Cost Management Tools for Cost and Usage Optimization"},{"location":"Cost/arc219.html#governance","text":"(5min) Perform steps 3 and 4 only . You will implement an AWS Budget to provide notifications on Savings Plan coverage, and create a weekly budget report. (25min) You will use IAM Policies to enforce cost governance by controlling usage. (15min) Beta : You will use Budgets with SNS and Lambda to automatically restrict access if a budget is exceeded.","title":"Governance"},{"location":"Cost/arc219.html#analysis","text":"(5min) Perform Step 4 only . You will use Cost Explorers hourly granularity feature, to view the elasticity in your cost and usage. (25min) You will utilize Glue and Athena to analyze your Cost and Usage Report . (30min) Up to Step 5 only You will use saved queries in Athena, and Lambda, to split a CUR file into multiple parts based on linked/member account ID, and share it with linked accounts. . (20min) You will use CloudWatch to trigger Lambda to run queries in Athena, and then compile the results into a spreadsheet, and email the results using SES.","title":"Analysis"},{"location":"Cost/arc219.html#licensing","text":"There are no labs for licensing.","title":"Licensing"},{"location":"Cost/arc219.html#right-sizing","text":"(15min) This lab will show you how to use Cost Explorer to perform right sizing recommendations on your resources. (15min) This lab will show you how to install the memory agent.","title":"Right Sizing"},{"location":"Cost/arc219.html#pricing-models","text":"(15min) This lab will show you how to view and analyze your Savings Plan recommendations. (15min) This lab will show you how to perform a Reserved Instance (RI) Analysis, and select low risk, high return RI's for your accounts.","title":"Pricing Models"},{"location":"Cost/arc219.html#well-architected-tool","text":"Implementing usage controls will ensure excess usage and accompanying costs does not occur. (10min) This lab will walk you through the Well-Architected Tool.","title":"Well-Architected Tool"},{"location":"Cost/ent206.html","text":"ENT206 Optimize AWS costs and utilization with AWS managment tools http://wellarchitectedlabs.com Governance You will implement AWS Budgets to provide notifications on cost and usage, and weekly reports on budget performance. You will use IAM Policies to manage costs and usage. Visualization and Analysis You will use Cost Explorer to visualize your cost and usage. In this additional step you will configure the Cost and Usage Report , the most detailed source of your Cost and Usage information. You will utilize Glue and Athena to analyze your Cost and Usage Report . You will use QuickSight to create Cost and Usage dashboards and visualizations of your usage. Licensing There are no labs for licensing. Right Sizing This lab will show you how to use Cost Explorer to perform right sizing recommendations on your resources. This lab will show you how to install the memory agent. Pricing Models This lab will show you how to view and analyze your Savings Plan recommendations. This lab will show you how to perform a Reserved Instance (RI) Analysis , and select low risk, high return RI's for your accounts. Well-Architected Tool This lab will walk you through the Well-Architected Tool.","title":"ENT206 Optimize AWS costs and utilization with AWS managment tools"},{"location":"Cost/ent206.html#ent206-optimize-aws-costs-and-utilization-with-aws-managment-tools","text":"http://wellarchitectedlabs.com","title":"ENT206 Optimize AWS costs and utilization with AWS managment tools"},{"location":"Cost/ent206.html#governance","text":"You will implement AWS Budgets to provide notifications on cost and usage, and weekly reports on budget performance. You will use IAM Policies to manage costs and usage.","title":"Governance"},{"location":"Cost/ent206.html#visualization-and-analysis","text":"You will use Cost Explorer to visualize your cost and usage. In this additional step you will configure the Cost and Usage Report , the most detailed source of your Cost and Usage information. You will utilize Glue and Athena to analyze your Cost and Usage Report . You will use QuickSight to create Cost and Usage dashboards and visualizations of your usage.","title":"Visualization and Analysis"},{"location":"Cost/ent206.html#licensing","text":"There are no labs for licensing.","title":"Licensing"},{"location":"Cost/ent206.html#right-sizing","text":"This lab will show you how to use Cost Explorer to perform right sizing recommendations on your resources. This lab will show you how to install the memory agent.","title":"Right Sizing"},{"location":"Cost/ent206.html#pricing-models","text":"This lab will show you how to view and analyze your Savings Plan recommendations. This lab will show you how to perform a Reserved Instance (RI) Analysis , and select low risk, high return RI's for your accounts.","title":"Pricing Models"},{"location":"Cost/ent206.html#well-architected-tool","text":"This lab will walk you through the Well-Architected Tool.","title":"Well-Architected Tool"},{"location":"Cost/thankyou.html","text":"Thankyou for your feedback Return to the Cost Labs","title":"Thankyou for your feedback"},{"location":"Cost/thankyou.html#thankyou-for-your-feedback","text":"","title":"Thankyou for your feedback"},{"location":"Cost/thankyou.html#return-to-the-cost-labs","text":"","title":"Return to the Cost Labs"},{"location":"Cost/CostOpt_Workload/README.html","text":"Cost Optimize a Workload https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to perform a baseline on your workload. It will start broad and high level, to provide an insight into the profile of the workload, its demand, and provide an overall efficiency measure. It will then iterate in detail, to categorize different workload requests and provide more detailed insight on the cost efficiency of a workload and its components. The first time you perform this lab it is recommended to use the sample files supplied, then you can use your own application and billing files. Goals Understand workload demand profile Understand workload resource profile Understand workload cost Measure workload efficiency in cost per outcome Prerequisites A subscription to Amazon QuickSight Basic understanding of SQL preferred Permissions required S3 AWS Glue Amazon Athena AWS Systems manager Step1 - Only do 1 - setup Athena Step2 - Only do 2 - Workload Demand and Cost Step3 Step4 Step5 Step6 License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Cost Optimize a Workload"},{"location":"Cost/CostOpt_Workload/README.html#cost-optimize-a-workload","text":"https://wellarchitectedlabs.com","title":"Cost Optimize a Workload"},{"location":"Cost/CostOpt_Workload/README.html#introduction","text":"This hands-on lab will guide you through the steps to perform a baseline on your workload. It will start broad and high level, to provide an insight into the profile of the workload, its demand, and provide an overall efficiency measure. It will then iterate in detail, to categorize different workload requests and provide more detailed insight on the cost efficiency of a workload and its components. The first time you perform this lab it is recommended to use the sample files supplied, then you can use your own application and billing files.","title":"Introduction"},{"location":"Cost/CostOpt_Workload/README.html#goals","text":"Understand workload demand profile Understand workload resource profile Understand workload cost Measure workload efficiency in cost per outcome","title":"Goals"},{"location":"Cost/CostOpt_Workload/README.html#prerequisites","text":"A subscription to Amazon QuickSight Basic understanding of SQL preferred","title":"Prerequisites"},{"location":"Cost/CostOpt_Workload/README.html#permissions-required","text":"S3 AWS Glue Amazon Athena AWS Systems manager","title":"Permissions required"},{"location":"Cost/CostOpt_Workload/README.html#step1-only-do-1-setup-athena","text":"","title":"Step1 - Only do 1 - setup Athena"},{"location":"Cost/CostOpt_Workload/README.html#step2-only-do-2-workload-demand-and-cost","text":"","title":"Step2 - Only do 2 - Workload Demand and Cost"},{"location":"Cost/CostOpt_Workload/README.html#step3","text":"","title":"Step3"},{"location":"Cost/CostOpt_Workload/README.html#step4","text":"","title":"Step4"},{"location":"Cost/CostOpt_Workload/README.html#step5","text":"","title":"Step5"},{"location":"Cost/CostOpt_Workload/README.html#step6","text":"","title":"Step6"},{"location":"Cost/CostOpt_Workload/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/CostOpt_Workload/Setup.html","text":"Title Text","title":"Title"},{"location":"Cost/CostOpt_Workload/Setup.html#title","text":"Text","title":"Title"},{"location":"Cost/CostOpt_Workload/Step1.html","text":"Environment Setup Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com 1 View our application log files Lets look at our application log files from our web servers. Download the sample log file to your local PC and open it in a text editor: Step1_access_log You can see the various columns in the file and the many requests: You can see the health checks: Scroll through and look for application requests, there should be two different types of requests, for index and image : Look at the components of the log lines, there are 9 components to each line: Client IP Ident Auth HTTP Timestamp* Request Response Bytes Referrer Agent When it is loaded for analysis we have created 11 comonents by splitting the Timestamp into Date , Time and Timezone : Client IP Ident Auth Date* Time* Timezone* Request Response Bytes Referrer Agent 2 View our cost files To measure efficiency we need to know the cost of the workload, so we will use the Cost and Usage Report. Download the sample log file to your local PC and open it in a text editor: Step1CUR You can see the various columns in the file: Scroll down and you can see the EC2 usage: You can see the DataTransfer usage. Open up a spreadsheet application and copy 30 lines into a spreadsheet, it is a CSV and the columns are separated by space characters: Scroll across and see the different columns available Scroll and see some of the data available View the columns and some sample values here: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/enhanced-product-columns.html","title":"Environment Setup"},{"location":"Cost/CostOpt_Workload/Step1.html#environment-setup","text":"","title":"Environment Setup"},{"location":"Cost/CostOpt_Workload/Step1.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/CostOpt_Workload/Step1.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/CostOpt_Workload/Step1.html#1-view-our-application-log-files","text":"Lets look at our application log files from our web servers. Download the sample log file to your local PC and open it in a text editor: Step1_access_log You can see the various columns in the file and the many requests: You can see the health checks: Scroll through and look for application requests, there should be two different types of requests, for index and image : Look at the components of the log lines, there are 9 components to each line: Client IP Ident Auth HTTP Timestamp* Request Response Bytes Referrer Agent When it is loaded for analysis we have created 11 comonents by splitting the Timestamp into Date , Time and Timezone : Client IP Ident Auth Date* Time* Timezone* Request Response Bytes Referrer Agent","title":"1 View our application log files"},{"location":"Cost/CostOpt_Workload/Step1.html#2-view-our-cost-files","text":"To measure efficiency we need to know the cost of the workload, so we will use the Cost and Usage Report. Download the sample log file to your local PC and open it in a text editor: Step1CUR You can see the various columns in the file: Scroll down and you can see the EC2 usage: You can see the DataTransfer usage. Open up a spreadsheet application and copy 30 lines into a spreadsheet, it is a CSV and the columns are separated by space characters: Scroll across and see the different columns available Scroll and see some of the data available View the columns and some sample values here: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/enhanced-product-columns.html","title":"2 View our cost files"},{"location":"Cost/CostOpt_Workload/Step1a.html","text":"Environment Setup Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Goals Setup a data source for your application log files Setup a data source with your Cost and Usage Reports Table of Contents Setup Athena Setup QuickSight Tear Down Rate Lab 1. Setup Athena data sources We need to setup our data sources in Athena. This will allow us to query and analyze our application log files and Cost and Usage reports. To set up our data sources, we place our application log files into S3 and then we use Glue to crawl them and create a database. Athena can then be used to run queries against them. Perform all steps in us-east-1 N. Virginia 1.1 Copy application log files into S3 The first step is to get the application log files into Athena to be analyzed. For the provided files, you will copy the sample files to your S3 bucket. NOTE Please read the steps carefully, as the naming is critical and any mistakes will require you to rebuild the lab. Log into the AWS console as an IAM user with the required permissions: Make sure you are in us-east-1 Create a new S3 Bucket - it can have any name Create a folder in the new bucket with a name: applogfiles_reinventworkshop . NOTE : You MUST name the folder applogfiles_reinventworkshop Upload the first application log file to the folder: Step1_access_log.gz READ ONLY If you will be using your own application log files, systems manager can be used to run commands across your environment and copy files from multiple servers to S3. This lab will be re-written with full instructions post re:invent. READ ONLY Depending on your operating system, you can execute some CLI on your application servers to copy the application log files to your S3 bucket . The following Linux sample will copy all access logs from the httpd log directory to the s3 bucket created above using the hostname to separate each servers logs: HOSTNAME=$(hostname) aws s3 cp --recursive /var/log/httpd/ s3://applogfiles-reinventworkshop/$HOSTNAME --exclude \"*\" --include \"access_log*\" 1.2 Crawl log files with Glue We will create a database with the uploaded log file with AWS Glue. We show you how to write a custom classifier, so you can handle any log file format. For our sample Apache web server log files, the in-bulit AWS Glue classifier COMBINEDAPACHELOG will recognize the timestamp as a single string. We will customize the interpreter to break this up into a date column, timestamp column and timezone column. This will demonstrate how to write a customer classifier. The reference for classifiers is here: https://docs.aws.amazon.com/glue/latest/dg/custom-classifier.html A sample log file line is: 10.0.1.80 - - [26/Nov/2019:00:00:07 +0000] \"GET /health.html HTTP/1.1\" 200 55 \"-\" \"ELB-HealthChecker/2.0\" The columns would usually be: - Client IP - Ident - Auth - HTTP Timestamp* - Request - Response - Bytes - Referrer - Agent We will make it build the following columns - Client IP - Ident - Auth - Date* - Time* - Timezone* - Request - Response - Bytes - Referrer - Agent Go to the Glue console and click Classifiers : Click Add classifier and create it with the following details: Classifier name: WebLogs Classifier type: Grok Classification: Logs Grok pattern: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{DATE:logdate}\\:%{TIME:logtime} %{INT:tz}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{Bytes:bytes=%{NUMBER}|-}) %{QS:referrer} %{QS:agent} Custom patterns: DATE %{MONTHDAY}/%{MONTH}/%{YEAR} Click Create A classifier tells Glue how to interpret the log file lines, and how to create columns. Each column is contained within %{}, and has the pattern , the separator ':' , and the column name . By using the custom classifier, we have separated the single column timestamp into 3 columns of logdate, logtime and tz. You can compare the custom classifier we wrote with the COMBINEDAPACHELOG classifier: Custom - %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{DATE:logdate}\\:%{TIME:logtime} %{INT:tz}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{Bytes:bytes=%{NUMBER}|-}) %{QS:referrer} %{QS:agent} Builtin - %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{Bytes:bytes=%{NUMBER}|-}) %{QS:referrer} %{QS:agent} Next we will create a crawler to read the log files, and build a database. Click on Crawlers and click Add crawler : Crawler name will be ApplicationLogs , expand Tags, description.. next to our Weblogs classifier, cilck Add , then click Next : Crawler source type is Data stores, click Next : Click the folder icon and expand your bucket created above, select the radio button next to the applogfiles_reinventworkshop . Do NOT select the actual file or bucket, select the folder. Click Select . Click Next Select No to not add another data store, click Next Create an IAM role named AWSGlueServiceRole- WebLogs and click Next : Frequency will be run on demand, click Next Click Add database , you MUST name it webserverlogs , click Create . Click Next : Click Finish Select and Run crawler , this will create a single database and table with our log files, lets confirm. We need to wait until the crawler has finished , this will take 1-2 minutes. Click refresh to check if it has finished. Click Databases on the left, and click on the database webserverlogs , you may need to click refresh : Click Tables in webserverlogs , and click the table applogfiles_reinventworkshop You can see the table is created, the Name , the Location , and the recordCount has a large number of records in it (the number may be different to the image below): Scroll down and you can see the columns, and that they are all string . This will be a small hurdle for columns like bytes if you want to perform a function on it: Go to the Athena service console, and select the webserverlogs database: Click the three dots next to the table applogfiles_reinventworkshop , and click Preview table : View the results which will show 10 lines of your log. Note how there are separate columns logdate logtime and tz that we created. The default classifier would have had a single column of text for the timestamp. 1.3 Create a database of your cost files To measure efficiency we need to know the cost of the workload, so we will use the Cost and Usage Report. If you are using your own Cost and Usage Reports, you will need to have them already configured and delivered as per this lab: https://wellarchitectedlabs.com/Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#CUR To use the files from this lab, follow the steps below: Go to the S3 Console and create a folder named costusagefiles-reinventworkshop inside the bucket you created above. You MUST name the folder costusagefiles_reinventworkshop , this will make pasting the code faster. Copy the sample file to your bucket: Step1CUR.gz Go into the Glue console, and click Add crawler Use the crawler name CostUsage and click Next Select Data stores as the crawler source type, click Next Click the folder icon , Select the S3 folder created above costusagefiles-reinventworkshop as the data store, make sure you dont select the bucket or file. Click Select , then click Next Select No do not another data store, click Next Create an IAM role named AWSGlueServiceRole- costusage , click Next Set the frequency to run on demand , click Next Cilck Add database , it MUST be named CostUsage , and click Create click Next Review and click Finish Run the crawler, then check the database was created and has records in it as per the previous step. Go into Databases , select the costusage database, and then select the table costusagefiles_reinventworkshop . Click Edit Schema : Make sure the column line_item_unblended_cost has a data type of double , you may need to change it from string: Also change or confirm the following columns: line_item_usage_start_date: timestamp line_item_usage_end_date: timestamp line_item_usage_amount: double Click on save , and confirm it is of the correct type. 2 Setup QuickSight - Optional for Visualization We will use QuickSight as the analysis tool to visualize data. You could query the data in Athena and export the results to be used in a spreadsheet application for graphs, however QuickSight offers the advantages of being the specific tool for the job, and you can easily create additional data fields from existing fields for analysis. 2.1 Application files We will create a data set from the application log files. Go into QuickSight Click on your user in the top right, and click Manage QuickSight : Click on Security and permissions and click Add or remove : Under Amazon S3 click Details : Click Select S3 buckets : Select the buckets that have the application log files and the cost and usage files, click Select buckets Select Amazon Athena and click Update : From the QuickSight home page , click Manage data : Click New data set : Click Athena : Enter the Data source name AppLogs and click Create data source Select the WebserverLogs Database and the Table and click Select Select Directly query your data and click Edit/Preview data Make sure there is data in the bottom pane. You can see that we have separated our date and time and they are in separate columns with a string data type. What we will now do is create a custom datetime column with the right data type, this will show how you can build custom fields and types for your log files: First we will combine the logdate and logtime columns into a single value separated by a space. Click Add calculated field : To do this we will use the concat function. Lets call the field DateTime , click Create : concat({logdate}, ' ', {logtime}) You will now see a new column with the correct text on the far right: Next we will convert it into the right data type so we can treat it like a timestamp in QuickSight. We will use the parseDate function, on the output of the concatenate function. So we will put the concatenate function inside the parseDate function. Select the down arrow next to the calculated field, and select Edit DateTime : Change the formula to the one below. It puts the parseDate function around the concatenate and we specify the date format so that it is correctly recognized. Click Apply changes : parseDate(concat({logdate}, ' ', {logtime}), 'dd/MMM/yyyy HH:mm:ss') You should see the column on the far right is now of the Date data type. You can do this with any field, for example converting the bytes column to a number by removing and replacing the '-' character with a '0' using the if function. Click Save & visualize : 2.1 Cost and Usage data From the QuickSight homepage , click Manage data : Click New data set Select Athena Name the datasource WorkloadCost , click Create Select the costusage database and table that contains your cost data you setup in Glue, and click Edit/Preview data : Verify you have cost data, click Save , and you will see your new data set next to the existing one: 3. Tear down Complete the teardown only after you have finished all steps in this lab 1. Remove the Data Sets in QuickSIght 2. Delete cost and application log databases in Glue 3. Delete S3 buckets containing the application and CUR files 4. Delete the IAM roles that were created to allow Glue access to S3","title":"Environment Setup"},{"location":"Cost/CostOpt_Workload/Step1a.html#environment-setup","text":"","title":"Environment Setup"},{"location":"Cost/CostOpt_Workload/Step1a.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/CostOpt_Workload/Step1a.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/CostOpt_Workload/Step1a.html#goals","text":"Setup a data source for your application log files Setup a data source with your Cost and Usage Reports","title":"Goals"},{"location":"Cost/CostOpt_Workload/Step1a.html#table-of-contents","text":"Setup Athena Setup QuickSight Tear Down Rate Lab","title":"Table of Contents"},{"location":"Cost/CostOpt_Workload/Step1a.html#1-setup-athena-data-sources","text":"We need to setup our data sources in Athena. This will allow us to query and analyze our application log files and Cost and Usage reports. To set up our data sources, we place our application log files into S3 and then we use Glue to crawl them and create a database. Athena can then be used to run queries against them. Perform all steps in us-east-1 N. Virginia","title":"1. Setup Athena data sources"},{"location":"Cost/CostOpt_Workload/Step1a.html#11-copy-application-log-files-into-s3","text":"The first step is to get the application log files into Athena to be analyzed. For the provided files, you will copy the sample files to your S3 bucket. NOTE Please read the steps carefully, as the naming is critical and any mistakes will require you to rebuild the lab. Log into the AWS console as an IAM user with the required permissions: Make sure you are in us-east-1 Create a new S3 Bucket - it can have any name Create a folder in the new bucket with a name: applogfiles_reinventworkshop . NOTE : You MUST name the folder applogfiles_reinventworkshop Upload the first application log file to the folder: Step1_access_log.gz READ ONLY If you will be using your own application log files, systems manager can be used to run commands across your environment and copy files from multiple servers to S3. This lab will be re-written with full instructions post re:invent. READ ONLY Depending on your operating system, you can execute some CLI on your application servers to copy the application log files to your S3 bucket . The following Linux sample will copy all access logs from the httpd log directory to the s3 bucket created above using the hostname to separate each servers logs: HOSTNAME=$(hostname) aws s3 cp --recursive /var/log/httpd/ s3://applogfiles-reinventworkshop/$HOSTNAME --exclude \"*\" --include \"access_log*\"","title":"1.1 Copy application log files into S3"},{"location":"Cost/CostOpt_Workload/Step1a.html#12-crawl-log-files-with-glue","text":"We will create a database with the uploaded log file with AWS Glue. We show you how to write a custom classifier, so you can handle any log file format. For our sample Apache web server log files, the in-bulit AWS Glue classifier COMBINEDAPACHELOG will recognize the timestamp as a single string. We will customize the interpreter to break this up into a date column, timestamp column and timezone column. This will demonstrate how to write a customer classifier. The reference for classifiers is here: https://docs.aws.amazon.com/glue/latest/dg/custom-classifier.html A sample log file line is: 10.0.1.80 - - [26/Nov/2019:00:00:07 +0000] \"GET /health.html HTTP/1.1\" 200 55 \"-\" \"ELB-HealthChecker/2.0\" The columns would usually be: - Client IP - Ident - Auth - HTTP Timestamp* - Request - Response - Bytes - Referrer - Agent We will make it build the following columns - Client IP - Ident - Auth - Date* - Time* - Timezone* - Request - Response - Bytes - Referrer - Agent Go to the Glue console and click Classifiers : Click Add classifier and create it with the following details: Classifier name: WebLogs Classifier type: Grok Classification: Logs Grok pattern: %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{DATE:logdate}\\:%{TIME:logtime} %{INT:tz}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{Bytes:bytes=%{NUMBER}|-}) %{QS:referrer} %{QS:agent} Custom patterns: DATE %{MONTHDAY}/%{MONTH}/%{YEAR} Click Create A classifier tells Glue how to interpret the log file lines, and how to create columns. Each column is contained within %{}, and has the pattern , the separator ':' , and the column name . By using the custom classifier, we have separated the single column timestamp into 3 columns of logdate, logtime and tz. You can compare the custom classifier we wrote with the COMBINEDAPACHELOG classifier: Custom - %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{DATE:logdate}\\:%{TIME:logtime} %{INT:tz}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{Bytes:bytes=%{NUMBER}|-}) %{QS:referrer} %{QS:agent} Builtin - %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{Bytes:bytes=%{NUMBER}|-}) %{QS:referrer} %{QS:agent} Next we will create a crawler to read the log files, and build a database. Click on Crawlers and click Add crawler : Crawler name will be ApplicationLogs , expand Tags, description.. next to our Weblogs classifier, cilck Add , then click Next : Crawler source type is Data stores, click Next : Click the folder icon and expand your bucket created above, select the radio button next to the applogfiles_reinventworkshop . Do NOT select the actual file or bucket, select the folder. Click Select . Click Next Select No to not add another data store, click Next Create an IAM role named AWSGlueServiceRole- WebLogs and click Next : Frequency will be run on demand, click Next Click Add database , you MUST name it webserverlogs , click Create . Click Next : Click Finish Select and Run crawler , this will create a single database and table with our log files, lets confirm. We need to wait until the crawler has finished , this will take 1-2 minutes. Click refresh to check if it has finished. Click Databases on the left, and click on the database webserverlogs , you may need to click refresh : Click Tables in webserverlogs , and click the table applogfiles_reinventworkshop You can see the table is created, the Name , the Location , and the recordCount has a large number of records in it (the number may be different to the image below): Scroll down and you can see the columns, and that they are all string . This will be a small hurdle for columns like bytes if you want to perform a function on it: Go to the Athena service console, and select the webserverlogs database: Click the three dots next to the table applogfiles_reinventworkshop , and click Preview table : View the results which will show 10 lines of your log. Note how there are separate columns logdate logtime and tz that we created. The default classifier would have had a single column of text for the timestamp.","title":"1.2 Crawl log files with Glue"},{"location":"Cost/CostOpt_Workload/Step1a.html#13-create-a-database-of-your-cost-files","text":"To measure efficiency we need to know the cost of the workload, so we will use the Cost and Usage Report. If you are using your own Cost and Usage Reports, you will need to have them already configured and delivered as per this lab: https://wellarchitectedlabs.com/Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#CUR To use the files from this lab, follow the steps below: Go to the S3 Console and create a folder named costusagefiles-reinventworkshop inside the bucket you created above. You MUST name the folder costusagefiles_reinventworkshop , this will make pasting the code faster. Copy the sample file to your bucket: Step1CUR.gz Go into the Glue console, and click Add crawler Use the crawler name CostUsage and click Next Select Data stores as the crawler source type, click Next Click the folder icon , Select the S3 folder created above costusagefiles-reinventworkshop as the data store, make sure you dont select the bucket or file. Click Select , then click Next Select No do not another data store, click Next Create an IAM role named AWSGlueServiceRole- costusage , click Next Set the frequency to run on demand , click Next Cilck Add database , it MUST be named CostUsage , and click Create click Next Review and click Finish Run the crawler, then check the database was created and has records in it as per the previous step. Go into Databases , select the costusage database, and then select the table costusagefiles_reinventworkshop . Click Edit Schema : Make sure the column line_item_unblended_cost has a data type of double , you may need to change it from string: Also change or confirm the following columns: line_item_usage_start_date: timestamp line_item_usage_end_date: timestamp line_item_usage_amount: double Click on save , and confirm it is of the correct type.","title":"1.3 Create a database of your cost files"},{"location":"Cost/CostOpt_Workload/Step1a.html#2-setup-quicksight-optional-for-visualization","text":"We will use QuickSight as the analysis tool to visualize data. You could query the data in Athena and export the results to be used in a spreadsheet application for graphs, however QuickSight offers the advantages of being the specific tool for the job, and you can easily create additional data fields from existing fields for analysis.","title":"2 Setup QuickSight - Optional for Visualization"},{"location":"Cost/CostOpt_Workload/Step1a.html#21-application-files","text":"We will create a data set from the application log files. Go into QuickSight Click on your user in the top right, and click Manage QuickSight : Click on Security and permissions and click Add or remove : Under Amazon S3 click Details : Click Select S3 buckets : Select the buckets that have the application log files and the cost and usage files, click Select buckets Select Amazon Athena and click Update : From the QuickSight home page , click Manage data : Click New data set : Click Athena : Enter the Data source name AppLogs and click Create data source Select the WebserverLogs Database and the Table and click Select Select Directly query your data and click Edit/Preview data Make sure there is data in the bottom pane. You can see that we have separated our date and time and they are in separate columns with a string data type. What we will now do is create a custom datetime column with the right data type, this will show how you can build custom fields and types for your log files: First we will combine the logdate and logtime columns into a single value separated by a space. Click Add calculated field : To do this we will use the concat function. Lets call the field DateTime , click Create : concat({logdate}, ' ', {logtime}) You will now see a new column with the correct text on the far right: Next we will convert it into the right data type so we can treat it like a timestamp in QuickSight. We will use the parseDate function, on the output of the concatenate function. So we will put the concatenate function inside the parseDate function. Select the down arrow next to the calculated field, and select Edit DateTime : Change the formula to the one below. It puts the parseDate function around the concatenate and we specify the date format so that it is correctly recognized. Click Apply changes : parseDate(concat({logdate}, ' ', {logtime}), 'dd/MMM/yyyy HH:mm:ss') You should see the column on the far right is now of the Date data type. You can do this with any field, for example converting the bytes column to a number by removing and replacing the '-' character with a '0' using the if function. Click Save & visualize :","title":"2.1 Application files"},{"location":"Cost/CostOpt_Workload/Step1a.html#21-cost-and-usage-data","text":"From the QuickSight homepage , click Manage data : Click New data set Select Athena Name the datasource WorkloadCost , click Create Select the costusage database and table that contains your cost data you setup in Glue, and click Edit/Preview data : Verify you have cost data, click Save , and you will see your new data set next to the existing one:","title":"2.1 Cost and Usage data"},{"location":"Cost/CostOpt_Workload/Step1a.html#3-tear-down","text":"Complete the teardown only after you have finished all steps in this lab 1. Remove the Data Sets in QuickSIght 2. Delete cost and application log databases in Glue 3. Delete S3 buckets containing the application and CUR files 4. Delete the IAM roles that were created to allow Glue access to S3","title":"3. Tear down"},{"location":"Cost/CostOpt_Workload/Step2.html","text":"Baseline Environment Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Goals Understand and measure your workload demand Understand and measure your workload cost Understand and measure your workload efficiency Table of Contents Workload Demand and Cost Workload Demand Visualization 1 Workload Demand and Cost We will now go deeper into the workload demand and cost, and discover exactly what comprises the demand and cost of the workload. For this we will use SQL queries in Athena against our cost database and a spreadsheet application. Make sure you perform all actions in US-EAST-1 1.1 Workload demand components Log into the console here: https://costopt.signin.aws.amazon.com/console Your username is workshop followed by your unique number, so it will be workshop1 or workshop99 : Enter the password and click Sign In Make sure you are in the US-EAST-1 region Go into the Athena console: Click the arrow , select the webserverlogs database: You will see a table applogfiles_reinventworkshop : Copy and paste the following query into the query window, and click Run query : SELECT * FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" limit 10; Review the data and look at the columns and sample data available in your log file. We will now look at the most popular requests by size. Inside the query we convert the column bytes from a string into a number, which we then divide by 1048576 to get MBytes instead of bytes. Run the following query: select distinct request, verb, response, (sum(cast(bytes as bigint)))/1048576 as Mbytes, count(*) as count FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where bytes not like '-' group by request, verb, response order by Mbytes desc limit 100 Errors or invalid requests still take resources from your systems, lets look at the number of non-successful (non 200) responses per day: select date_parse(logdate, '%d/%b/%Y') as date, count(*) as count FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where response not like '200' group by logdate order by logdate Use this technique for your workload, view the data in your columns and see what makes up the requests to your workload, and what important lines are in your application log files. This should help identify key items within the log file to analyze and measure the true demand on your workload. 2.2 Workload cost components Select the costusage database: Run the following query to view all the columns: SELECT * FROM \"costusage\".\"costusagefiles_reinventworkshop\" limit 10; View the columns available (there are > 100), most important are typically columns starting with line_item and resource_tags . For a quick listing and description of your costs, the following statement is useful as a starting point: SELECT line_item_line_item_description, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' group by line_item_line_item_description order by cost desc limit 10 Lets add some more columns to help identify what the main components of cost are, run the following statement: SELECT line_item_product_code, line_item_usage_type, line_item_operation, line_item_line_item_description, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' group by line_item_product_code, line_item_usage_type, line_item_operation, line_item_line_item_description order by cost desc limit 10 Using this data we can identify where we should look for savings in your cost optimization cycles. 2.3 Efficiency We have looked at our workload demand, and our workload costs. Lets combine the two to create a workload efficiency metric. Run the following query to return the total number of lines in our log file, this is our total demand: SELECT count(*) FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" Copy the results into a spreadsheet application and label it Total workload demand Run the following query to get the total successful responses , and valid successful responses, in the spreadsheet record these also: SELECT count(*) FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where response like '200' and (request like '%index%' or request like '%image_file%') We will now get the successful and valid responses by hour with the query below. Note the use of date_parse to turn the string into an actual date we can work with: SELECT date(date_parse(logdate, '%d/%b/%Y')) as date, hour(date_parse(logtime, '%H:%i:%s')) as hour, count(*) as requests FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where response like '200' and (request like '%index%' or request like '%image_file%') group by date_parse(logdate, '%d/%b/%Y'), hour(date_parse(logtime, '%H:%i:%s')) order by date, hour Click the icon in the top right to automatically open it in a spreadsheet: Copy the results into your original spreadsheet: Now lets get the workload cost that corresponds to the demand, run the following statement, and put it into the spreadsheet next to the demand: SELECT sum(line_item_unblended_cost) FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' Execute the following statement to get the cost by hour, copy the cost column into the spreadsheet: SELECT line_item_usage_start_date as date, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' group by line_item_usage_start_date order by date asc In the spreadsheet, calculate the efficiency by dividing the requests by the cost . This will give you the number of outcomes per dollar spent. Notice the difference when you compare total vs successful vs valid. Also notice any variation of efficiency throughout the day. You can see from the spreadsheet that the workload does: - 6,491.809 total requests per dollar, overall - 2789.539 successful customer requests per dollar, overall - from 945 to 3483 successful customer requests per dollar, throughout the day 2. Workload Demand Visualization We have our workload application logs ready to analyze and visualize. This step will be to visualize the workload demand and understand how it is used. 2.2 Demand visualization - Requests per hour Click in the top left and go to the homepage: Go to QuickSight: Enter in any valid email address and click continue : Click close on any popups Click All dashboards : Click Workload : On sheet 1 in the top image, you have the profile of the valid and successful customer outcomes of your workload: This shows you the demand that is on your workload, it shows you how many requests were made and when these requests were made. 2.3 Categorize requests Your workload may do different things, there could be different requests that consume different amounts of resources - so you would like to treat them differently. In this section we will look for specific requests and categorize them out. When you viewed the application log files you saw requests: /index.html?name=Isabella,user=sponsored,work=26 You can see there is user=sponsored , for this sample workload we have free , sponsored and paid user types. Go to sheet 2 You now have a visual of your requests by different categories. In our example there is an \"empty\" usertype with a lot of usage, and the others are similar except for the middle of the middle of the day - where paid users are much more than free users: You now have more insight into your workload, as you now know the different types of requests that come into your workload, and how they vary over time. 2.4 Quantify requests In your application log files, you may be able to take a numerical value from inside the log file, or assign a numerical value depending on the log line contents. These can then be used to perform a calculation on, which can indicate the amount of work or resource consumption for the request. When using your logs, you will need to find how different types or categories of requests can be measured in terms of resource consumption. Look to find what is in that request that indicates more resource consumption. In our sample log file it is inside the request string. A sample request string is: /index.php?name=Isabella,user=sponsored,work=26 You can see there is text work=26 which has a number, this corresponds to the amount of resources it consumes, so we will use that in this example. Go to sheet 3 You have a visual of your requests by total work : You now have more insight into your workload, as you can measure overall load in a more meaningful way. Compare the graphs on the first, second and third sheets. You can see that by including additional small amounts of information or focusing on certain parts of information in your analysis you can get a very different outcome. 2.5 Compare visualizations Go back to each of the tabs and focus on the similarities between visualizations. The similarities should indicate if there is a strong correlation between usage and cost for the categories we chose. If the cost follows the demand, then you have a scalable and elastic workload:","title":"Baseline Environment"},{"location":"Cost/CostOpt_Workload/Step2.html#baseline-environment","text":"","title":"Baseline Environment"},{"location":"Cost/CostOpt_Workload/Step2.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/CostOpt_Workload/Step2.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/CostOpt_Workload/Step2.html#goals","text":"Understand and measure your workload demand Understand and measure your workload cost Understand and measure your workload efficiency","title":"Goals"},{"location":"Cost/CostOpt_Workload/Step2.html#table-of-contents","text":"Workload Demand and Cost Workload Demand Visualization","title":"Table of Contents"},{"location":"Cost/CostOpt_Workload/Step2.html#1-workload-demand-and-cost","text":"We will now go deeper into the workload demand and cost, and discover exactly what comprises the demand and cost of the workload. For this we will use SQL queries in Athena against our cost database and a spreadsheet application. Make sure you perform all actions in US-EAST-1","title":"1 Workload Demand and Cost"},{"location":"Cost/CostOpt_Workload/Step2.html#11-workload-demand-components","text":"Log into the console here: https://costopt.signin.aws.amazon.com/console Your username is workshop followed by your unique number, so it will be workshop1 or workshop99 : Enter the password and click Sign In Make sure you are in the US-EAST-1 region Go into the Athena console: Click the arrow , select the webserverlogs database: You will see a table applogfiles_reinventworkshop : Copy and paste the following query into the query window, and click Run query : SELECT * FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" limit 10; Review the data and look at the columns and sample data available in your log file. We will now look at the most popular requests by size. Inside the query we convert the column bytes from a string into a number, which we then divide by 1048576 to get MBytes instead of bytes. Run the following query: select distinct request, verb, response, (sum(cast(bytes as bigint)))/1048576 as Mbytes, count(*) as count FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where bytes not like '-' group by request, verb, response order by Mbytes desc limit 100 Errors or invalid requests still take resources from your systems, lets look at the number of non-successful (non 200) responses per day: select date_parse(logdate, '%d/%b/%Y') as date, count(*) as count FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where response not like '200' group by logdate order by logdate Use this technique for your workload, view the data in your columns and see what makes up the requests to your workload, and what important lines are in your application log files. This should help identify key items within the log file to analyze and measure the true demand on your workload.","title":"1.1 Workload demand components"},{"location":"Cost/CostOpt_Workload/Step2.html#22-workload-cost-components","text":"Select the costusage database: Run the following query to view all the columns: SELECT * FROM \"costusage\".\"costusagefiles_reinventworkshop\" limit 10; View the columns available (there are > 100), most important are typically columns starting with line_item and resource_tags . For a quick listing and description of your costs, the following statement is useful as a starting point: SELECT line_item_line_item_description, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' group by line_item_line_item_description order by cost desc limit 10 Lets add some more columns to help identify what the main components of cost are, run the following statement: SELECT line_item_product_code, line_item_usage_type, line_item_operation, line_item_line_item_description, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' group by line_item_product_code, line_item_usage_type, line_item_operation, line_item_line_item_description order by cost desc limit 10 Using this data we can identify where we should look for savings in your cost optimization cycles.","title":"2.2 Workload cost components"},{"location":"Cost/CostOpt_Workload/Step2.html#23-efficiency","text":"We have looked at our workload demand, and our workload costs. Lets combine the two to create a workload efficiency metric. Run the following query to return the total number of lines in our log file, this is our total demand: SELECT count(*) FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" Copy the results into a spreadsheet application and label it Total workload demand Run the following query to get the total successful responses , and valid successful responses, in the spreadsheet record these also: SELECT count(*) FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where response like '200' and (request like '%index%' or request like '%image_file%') We will now get the successful and valid responses by hour with the query below. Note the use of date_parse to turn the string into an actual date we can work with: SELECT date(date_parse(logdate, '%d/%b/%Y')) as date, hour(date_parse(logtime, '%H:%i:%s')) as hour, count(*) as requests FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where response like '200' and (request like '%index%' or request like '%image_file%') group by date_parse(logdate, '%d/%b/%Y'), hour(date_parse(logtime, '%H:%i:%s')) order by date, hour Click the icon in the top right to automatically open it in a spreadsheet: Copy the results into your original spreadsheet: Now lets get the workload cost that corresponds to the demand, run the following statement, and put it into the spreadsheet next to the demand: SELECT sum(line_item_unblended_cost) FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' Execute the following statement to get the cost by hour, copy the cost column into the spreadsheet: SELECT line_item_usage_start_date as date, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' group by line_item_usage_start_date order by date asc In the spreadsheet, calculate the efficiency by dividing the requests by the cost . This will give you the number of outcomes per dollar spent. Notice the difference when you compare total vs successful vs valid. Also notice any variation of efficiency throughout the day. You can see from the spreadsheet that the workload does: - 6,491.809 total requests per dollar, overall - 2789.539 successful customer requests per dollar, overall - from 945 to 3483 successful customer requests per dollar, throughout the day","title":"2.3 Efficiency"},{"location":"Cost/CostOpt_Workload/Step2.html#2-workload-demand-visualization","text":"We have our workload application logs ready to analyze and visualize. This step will be to visualize the workload demand and understand how it is used.","title":"2. Workload Demand Visualization"},{"location":"Cost/CostOpt_Workload/Step2.html#22-demand-visualization-requests-per-hour","text":"Click in the top left and go to the homepage: Go to QuickSight: Enter in any valid email address and click continue : Click close on any popups Click All dashboards : Click Workload : On sheet 1 in the top image, you have the profile of the valid and successful customer outcomes of your workload: This shows you the demand that is on your workload, it shows you how many requests were made and when these requests were made.","title":"2.2 Demand visualization - Requests per hour"},{"location":"Cost/CostOpt_Workload/Step2.html#23-categorize-requests","text":"Your workload may do different things, there could be different requests that consume different amounts of resources - so you would like to treat them differently. In this section we will look for specific requests and categorize them out. When you viewed the application log files you saw requests: /index.html?name=Isabella,user=sponsored,work=26 You can see there is user=sponsored , for this sample workload we have free , sponsored and paid user types. Go to sheet 2 You now have a visual of your requests by different categories. In our example there is an \"empty\" usertype with a lot of usage, and the others are similar except for the middle of the middle of the day - where paid users are much more than free users: You now have more insight into your workload, as you now know the different types of requests that come into your workload, and how they vary over time.","title":"2.3 Categorize requests"},{"location":"Cost/CostOpt_Workload/Step2.html#24-quantify-requests","text":"In your application log files, you may be able to take a numerical value from inside the log file, or assign a numerical value depending on the log line contents. These can then be used to perform a calculation on, which can indicate the amount of work or resource consumption for the request. When using your logs, you will need to find how different types or categories of requests can be measured in terms of resource consumption. Look to find what is in that request that indicates more resource consumption. In our sample log file it is inside the request string. A sample request string is: /index.php?name=Isabella,user=sponsored,work=26 You can see there is text work=26 which has a number, this corresponds to the amount of resources it consumes, so we will use that in this example. Go to sheet 3 You have a visual of your requests by total work : You now have more insight into your workload, as you can measure overall load in a more meaningful way. Compare the graphs on the first, second and third sheets. You can see that by including additional small amounts of information or focusing on certain parts of information in your analysis you can get a very different outcome.","title":"2.4 Quantify requests"},{"location":"Cost/CostOpt_Workload/Step2.html#25-compare-visualizations","text":"Go back to each of the tabs and focus on the similarities between visualizations. The similarities should indicate if there is a strong correlation between usage and cost for the categories we chose. If the cost follows the demand, then you have a scalable and elastic workload:","title":"2.5 Compare visualizations"},{"location":"Cost/CostOpt_Workload/Step2a.html","text":"Baseline Environment Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Goals Understand and measure your workload demand Understand and measure your workload cost Understand and measure your workload efficiency Table of Contents Workload Demand and Cost Workload Demand Visualization 1 Workload Demand and Cost We will now go deeper into the workload demand and cost, and discover exactly what comprises the demand and cost of the workload. For this we will use SQL queries in Athena against our cost database and a spreadsheet application. NOTE : You may need to change the name of the database and table in the SQL code from webserverlogs.applogfiles_reinventworkshop to your database name and table name if you didnt use the correct folder names, do this for all pasted code. 1.1 Workload demand components Go into the Athena console Select the webserverlogs database, which should have 1 table applogfiles.. Run the following query to return all the columns, with 10 rows of data from the application logs. Note you may need to change the name of the database and table below webserverlogs.applogfiles_reinventworkshop**: SELECT * FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" limit 10; Review the data and look at the columns and sample data available in your log file. We will now look at the most popular requests by size, run the following query. Inside the query we convert the column bytes from a string into a number, which we then divide by 1048576 to get MBytes instead of bytes: select distinct request, verb, response, (sum(cast(bytes as bigint)))/1048576 as Mbytes, count(*) as count FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where bytes not like '-' group by request, verb, response order by Mbytes desc limit 100 Invalid requests still take resources from your systems, lets look at the number of non-successful (non 200) responses per day: select date_parse(logdate, '%d/%b/%Y') as date, count(*) as count FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where response not like '200' group by logdate order by logdate For your workload, review the different types of requests - both successful and not successful, and the available fields of data. This should help identify key items within the log file to analyze and measure the true demand on your workload. 2.2 Workload cost components Select the costusage database, and run the following query to view all the columns: SELECT * FROM \"costusage\".\"costusagefiles_reinventworkshop\" limit 10; View the columns available (there are > 100), most important are typically columns starting with line_item and resource_tags . For a quick listing and description of your costs, the following statement is useful as a starting point: SELECT line_item_line_item_description, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' group by line_item_line_item_description order by cost desc Lets add some more columns to help identify what the main components of cost are, run the following statement: SELECT line_item_product_code, line_item_usage_type, line_item_operation, line_item_line_item_description, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' group by line_item_product_code, line_item_usage_type, line_item_operation, line_item_line_item_description order by cost desc For your workload, review the different columns and results to see what makes up its cost. This will also indicate where you should look for savings in your cost optimization cycles. 2.3 Efficiency We have looked at our workload demand, and our workload costs. Lets combine the two to create a workload efficiency metric. As we saw in the QuickSight graphs, the demand changed over time and the cost changed also. This means our efficiency may change over time also. We will start with a very high level and simple metric, and then drill down a few steps. Go into the Athena console Select the webserverlogs database, which should have 1 table applogfiles.. Run the following query to return the total number of lines in our log file, which is our total demand: SELECT count(*) FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" Copy the results into a spreadsheet application and label it Total workload demand Run the following queries to get the total successful responses, and valid successful responses, record these also: SELECT count(*) FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where response like '200' and (request like '%index%' or request like '%image_file%') We will now get the successful and valid responses by hour with the query below. Note the use of date_parse to turn the string into an actual date we can work with: SELECT date(date_parse(logdate, '%d/%b/%Y')) as date, hour(date_parse(logtime, '%H:%i:%s')) as hour, count(*) as requests FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where response like '200' and (request like '%index%' or request like '%image_file%') group by date_parse(logdate, '%d/%b/%Y'), hour(date_parse(logtime, '%H:%i:%s')) order by date, hour Copy the results into the same spreadsheet. Now lets get the workload cost that corresponds to the demand. Select the costusage database, which should have a table costusagefiles_... Get the total cost for the workload with the following statement, and put it into the spreadsheet next to the demand: SELECT sum(line_item_unblended_cost) FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' Execute the following statement to get the cost by hour and put it into the spreadsheet: SELECT line_item_usage_start_date as date, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' group by line_item_usage_start_date order by date asc In the spreadsheet, calculate the efficiency by dividing the requests by the cost . This will give you the number of outcomes per dollar spent. Notice the difference when you compare total vs successful vs valid. Also notice any variation of efficiency throughout the day. You can see from the spreadsheet that the workload does: - 4,244.23 total requests per dollar, overall - 1,924.57 successful customer requests per dollar, overall - from 491 to 2555 successful customer requests per dollar, throughout the day 2. Workload Demand Visualization We have our workload application logs ready to analyze and visualize. This step will be to visualize the workload demand and understand how it is used. 2.2 Demand visualization - Requests per hour Our application log data is setup as a data source, so lets create a visualization. As our billing data is hourly, we will create an hourly demand graph showing the requests per hour of the workload. Log into QuickSight and open the applogfiles visualization you created in Step1, or create a New analysis from the Application log files data set. In Visual types , select the vertical bar chart, and drag DateTime and request to the field wells as shown: Change the aggregation to hourly : You now have a graph of all the requests over time, by hour. This shows your workload demand Its a webserver on the internet, so there's going to be lots of invalid requests (non-200 status), there could be lots of messages in your log files that will also need to be filtered out. Lets filter them out so that we're only looking at valid requests to our workload. Click on Filter , click Create one for response and use a Filter list to select only 200 . Click Apply : Again, there could be a lot of successful requests & other log messages you want to filter out. So we'll look for a specific string, which in our case is a specific request string, as something that for this workload signifies a successful customer outcome as opposed to all the other log messages. We will create another filter on the request field, both a Filter type of Custom filter , that contains the word index.html or contains the word image_file . This will look for any request that has these words in it: You now have the profile of the valid and successful customer outcomes of your workload: This shows you the demand that is on your workload, it shows you how many requests were made and when these requests were made. 2.3 Categorize requests Your workload may do different things, there could be different requests that consume different amounts of resources - so you would like to treat them differently. In this section we will look for specific requests and categorize them out. When you use your own application log files, find something that can be used to create different categories of requests. Is it in the request string? is it the source or destination of the request? In our sample logfile it is inside the request string. A sample request string is: /index.html?name=Isabella,user=sponsored,work=26 You can see there is user=sponsored , for this sample workload we have free , sponsored and paid user types. Add a new sheet, this will help to contrast the graphs and find the best visualization for your workload. Click the + next to the current sheet: We will create another calculated field, this can be done in the visualization or in the data source (like we did previously). When you do it in the data source you get sample data - so you can see if the formula is correct, and its then available to all visualizations. Click the home icon to go to the QuickSight homepage: Click Manage data in the top right and select your Dataset. Choose the applogfiles data set, and click Edit data set We will create a calculated field UserType with the formula below. It will split the request string by the delimeters and extract the UserType field. Create the custom field and click Create : split(request, ',', 2) Click save at the top Go back into the visualization on sheet2 and hit your browser refresh, the new field will appear: Create a line chart of requests, x-axis DateTime with hourly aggregation , Value request(count) Setup the filters on response include only 200, and request containing index.html and image_file as per the previous step. Now drag UserType to the Color field well: You now have a visual of your requests by different categories. In our example there is an \"empty\" usertype with a lot of usage, and the others are similar except for the middle of the middle of the day - where paid users are much more than free users: You now have more insight into your workload, as you now know the different types of requests that come into your workload, and how they vary over time. 2.4 Quantify requests In your application log files, you may be able to take a numerical value from inside the log file, or assign a numerical value depending on the log line contents. These can then be used to perform a calculation on, which can indicate the amount of work or resource consumption for the request. When using your logs, you will need to find how different types or categories of requests can be measured in terms of resource consumption. Look to find what is in that request that indicates more resource consumption. In our sample log file it is inside the request string. A sample request string is: /index.php?name=Isabella,user=sponsored,work=26 You can see there is text work=26 which has a number, this corresponds to the amount of resources it consumes, so we will use that in this example. We will add a calculated field which contains the number, we can then sum the numbers together across requests to find the total work. You can also use if statements if you want to assign values to text: if a field contains text, then assign a number. Go back to the data source and add a calculated field Work with the formula: parseInt(split(request, '=', 4)) Save the changes to the data source Go back into the visualization, refresh your browser and Add a new sheet Create a line chart with x-axis as DateTime with hourly aggregation , the Sum of Work as the value. Make sure you create the response and request filters: Now add the UserType as the Color : You now have a much clearer picture as to how much resources are being consumed by the different categories of requests to your workload: Compare the graphs on the first, second and third sheets. You can see that by including additional small amounts of information or focusing on certain parts of information in your analysis you can get a very different outcome. 2.5 Add the workload cost visualizations We know the requests that are being made to the workload, we will now add the cost of the workload to the analysis. This will allow us to understand the workload cost given its output. We will take a simple approach and put graphs of cost next to the graphs of the application demand. Go back to the visualizations, select the first sheet , and click the edit icon next to Data set : Click Add data set Select the cost data set you just created, click Select : Change the dataset, click the down arrow and select the CUR data set: Click Add and select Add visual : Create a Vertical Bar Chart with line_item_usage_start_date aggregated by the hour on the x-axis , and the sum of line_item_unblended_cost in the value field. To ensure we only have costs for our workload, we filter on a tag that we've assigned to all our resources. Create a Filter for the graph, create a filter on resource_tags_user_application , a Custom filter that equals ordering You can now see how the cost of the workload in comparison to the workload demand in the chart. We can see some correlation between request count and cost: Create the corresponding graphs on the other two sheets. On each sheet, select the data set , then Add a visual , and then populate the field wells. Make sure you create the filter for resource_tags_user_application . The visualization categorized by UserType , shows correlations depending on the user type: There is also correlation between work and cost: Viewing the images above, you can see the correlation between the usage level, the usage types and cost. There may be a time offset of 1hr depending on how the costs and usage are grouped.","title":"Baseline Environment"},{"location":"Cost/CostOpt_Workload/Step2a.html#baseline-environment","text":"","title":"Baseline Environment"},{"location":"Cost/CostOpt_Workload/Step2a.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/CostOpt_Workload/Step2a.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/CostOpt_Workload/Step2a.html#goals","text":"Understand and measure your workload demand Understand and measure your workload cost Understand and measure your workload efficiency","title":"Goals"},{"location":"Cost/CostOpt_Workload/Step2a.html#table-of-contents","text":"Workload Demand and Cost Workload Demand Visualization","title":"Table of Contents"},{"location":"Cost/CostOpt_Workload/Step2a.html#1-workload-demand-and-cost","text":"We will now go deeper into the workload demand and cost, and discover exactly what comprises the demand and cost of the workload. For this we will use SQL queries in Athena against our cost database and a spreadsheet application. NOTE : You may need to change the name of the database and table in the SQL code from webserverlogs.applogfiles_reinventworkshop to your database name and table name if you didnt use the correct folder names, do this for all pasted code.","title":"1 Workload Demand and Cost"},{"location":"Cost/CostOpt_Workload/Step2a.html#11-workload-demand-components","text":"Go into the Athena console Select the webserverlogs database, which should have 1 table applogfiles.. Run the following query to return all the columns, with 10 rows of data from the application logs. Note you may need to change the name of the database and table below webserverlogs.applogfiles_reinventworkshop**: SELECT * FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" limit 10; Review the data and look at the columns and sample data available in your log file. We will now look at the most popular requests by size, run the following query. Inside the query we convert the column bytes from a string into a number, which we then divide by 1048576 to get MBytes instead of bytes: select distinct request, verb, response, (sum(cast(bytes as bigint)))/1048576 as Mbytes, count(*) as count FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where bytes not like '-' group by request, verb, response order by Mbytes desc limit 100 Invalid requests still take resources from your systems, lets look at the number of non-successful (non 200) responses per day: select date_parse(logdate, '%d/%b/%Y') as date, count(*) as count FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where response not like '200' group by logdate order by logdate For your workload, review the different types of requests - both successful and not successful, and the available fields of data. This should help identify key items within the log file to analyze and measure the true demand on your workload.","title":"1.1 Workload demand components"},{"location":"Cost/CostOpt_Workload/Step2a.html#22-workload-cost-components","text":"Select the costusage database, and run the following query to view all the columns: SELECT * FROM \"costusage\".\"costusagefiles_reinventworkshop\" limit 10; View the columns available (there are > 100), most important are typically columns starting with line_item and resource_tags . For a quick listing and description of your costs, the following statement is useful as a starting point: SELECT line_item_line_item_description, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' group by line_item_line_item_description order by cost desc Lets add some more columns to help identify what the main components of cost are, run the following statement: SELECT line_item_product_code, line_item_usage_type, line_item_operation, line_item_line_item_description, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' group by line_item_product_code, line_item_usage_type, line_item_operation, line_item_line_item_description order by cost desc For your workload, review the different columns and results to see what makes up its cost. This will also indicate where you should look for savings in your cost optimization cycles.","title":"2.2 Workload cost components"},{"location":"Cost/CostOpt_Workload/Step2a.html#23-efficiency","text":"We have looked at our workload demand, and our workload costs. Lets combine the two to create a workload efficiency metric. As we saw in the QuickSight graphs, the demand changed over time and the cost changed also. This means our efficiency may change over time also. We will start with a very high level and simple metric, and then drill down a few steps. Go into the Athena console Select the webserverlogs database, which should have 1 table applogfiles.. Run the following query to return the total number of lines in our log file, which is our total demand: SELECT count(*) FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" Copy the results into a spreadsheet application and label it Total workload demand Run the following queries to get the total successful responses, and valid successful responses, record these also: SELECT count(*) FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where response like '200' and (request like '%index%' or request like '%image_file%') We will now get the successful and valid responses by hour with the query below. Note the use of date_parse to turn the string into an actual date we can work with: SELECT date(date_parse(logdate, '%d/%b/%Y')) as date, hour(date_parse(logtime, '%H:%i:%s')) as hour, count(*) as requests FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where response like '200' and (request like '%index%' or request like '%image_file%') group by date_parse(logdate, '%d/%b/%Y'), hour(date_parse(logtime, '%H:%i:%s')) order by date, hour Copy the results into the same spreadsheet. Now lets get the workload cost that corresponds to the demand. Select the costusage database, which should have a table costusagefiles_... Get the total cost for the workload with the following statement, and put it into the spreadsheet next to the demand: SELECT sum(line_item_unblended_cost) FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' Execute the following statement to get the cost by hour and put it into the spreadsheet: SELECT line_item_usage_start_date as date, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' group by line_item_usage_start_date order by date asc In the spreadsheet, calculate the efficiency by dividing the requests by the cost . This will give you the number of outcomes per dollar spent. Notice the difference when you compare total vs successful vs valid. Also notice any variation of efficiency throughout the day. You can see from the spreadsheet that the workload does: - 4,244.23 total requests per dollar, overall - 1,924.57 successful customer requests per dollar, overall - from 491 to 2555 successful customer requests per dollar, throughout the day","title":"2.3 Efficiency"},{"location":"Cost/CostOpt_Workload/Step2a.html#2-workload-demand-visualization","text":"We have our workload application logs ready to analyze and visualize. This step will be to visualize the workload demand and understand how it is used.","title":"2. Workload Demand Visualization"},{"location":"Cost/CostOpt_Workload/Step2a.html#22-demand-visualization-requests-per-hour","text":"Our application log data is setup as a data source, so lets create a visualization. As our billing data is hourly, we will create an hourly demand graph showing the requests per hour of the workload. Log into QuickSight and open the applogfiles visualization you created in Step1, or create a New analysis from the Application log files data set. In Visual types , select the vertical bar chart, and drag DateTime and request to the field wells as shown: Change the aggregation to hourly : You now have a graph of all the requests over time, by hour. This shows your workload demand Its a webserver on the internet, so there's going to be lots of invalid requests (non-200 status), there could be lots of messages in your log files that will also need to be filtered out. Lets filter them out so that we're only looking at valid requests to our workload. Click on Filter , click Create one for response and use a Filter list to select only 200 . Click Apply : Again, there could be a lot of successful requests & other log messages you want to filter out. So we'll look for a specific string, which in our case is a specific request string, as something that for this workload signifies a successful customer outcome as opposed to all the other log messages. We will create another filter on the request field, both a Filter type of Custom filter , that contains the word index.html or contains the word image_file . This will look for any request that has these words in it: You now have the profile of the valid and successful customer outcomes of your workload: This shows you the demand that is on your workload, it shows you how many requests were made and when these requests were made.","title":"2.2 Demand visualization - Requests per hour"},{"location":"Cost/CostOpt_Workload/Step2a.html#23-categorize-requests","text":"Your workload may do different things, there could be different requests that consume different amounts of resources - so you would like to treat them differently. In this section we will look for specific requests and categorize them out. When you use your own application log files, find something that can be used to create different categories of requests. Is it in the request string? is it the source or destination of the request? In our sample logfile it is inside the request string. A sample request string is: /index.html?name=Isabella,user=sponsored,work=26 You can see there is user=sponsored , for this sample workload we have free , sponsored and paid user types. Add a new sheet, this will help to contrast the graphs and find the best visualization for your workload. Click the + next to the current sheet: We will create another calculated field, this can be done in the visualization or in the data source (like we did previously). When you do it in the data source you get sample data - so you can see if the formula is correct, and its then available to all visualizations. Click the home icon to go to the QuickSight homepage: Click Manage data in the top right and select your Dataset. Choose the applogfiles data set, and click Edit data set We will create a calculated field UserType with the formula below. It will split the request string by the delimeters and extract the UserType field. Create the custom field and click Create : split(request, ',', 2) Click save at the top Go back into the visualization on sheet2 and hit your browser refresh, the new field will appear: Create a line chart of requests, x-axis DateTime with hourly aggregation , Value request(count) Setup the filters on response include only 200, and request containing index.html and image_file as per the previous step. Now drag UserType to the Color field well: You now have a visual of your requests by different categories. In our example there is an \"empty\" usertype with a lot of usage, and the others are similar except for the middle of the middle of the day - where paid users are much more than free users: You now have more insight into your workload, as you now know the different types of requests that come into your workload, and how they vary over time.","title":"2.3 Categorize requests"},{"location":"Cost/CostOpt_Workload/Step2a.html#24-quantify-requests","text":"In your application log files, you may be able to take a numerical value from inside the log file, or assign a numerical value depending on the log line contents. These can then be used to perform a calculation on, which can indicate the amount of work or resource consumption for the request. When using your logs, you will need to find how different types or categories of requests can be measured in terms of resource consumption. Look to find what is in that request that indicates more resource consumption. In our sample log file it is inside the request string. A sample request string is: /index.php?name=Isabella,user=sponsored,work=26 You can see there is text work=26 which has a number, this corresponds to the amount of resources it consumes, so we will use that in this example. We will add a calculated field which contains the number, we can then sum the numbers together across requests to find the total work. You can also use if statements if you want to assign values to text: if a field contains text, then assign a number. Go back to the data source and add a calculated field Work with the formula: parseInt(split(request, '=', 4)) Save the changes to the data source Go back into the visualization, refresh your browser and Add a new sheet Create a line chart with x-axis as DateTime with hourly aggregation , the Sum of Work as the value. Make sure you create the response and request filters: Now add the UserType as the Color : You now have a much clearer picture as to how much resources are being consumed by the different categories of requests to your workload: Compare the graphs on the first, second and third sheets. You can see that by including additional small amounts of information or focusing on certain parts of information in your analysis you can get a very different outcome.","title":"2.4 Quantify requests"},{"location":"Cost/CostOpt_Workload/Step2a.html#25-add-the-workload-cost-visualizations","text":"We know the requests that are being made to the workload, we will now add the cost of the workload to the analysis. This will allow us to understand the workload cost given its output. We will take a simple approach and put graphs of cost next to the graphs of the application demand. Go back to the visualizations, select the first sheet , and click the edit icon next to Data set : Click Add data set Select the cost data set you just created, click Select : Change the dataset, click the down arrow and select the CUR data set: Click Add and select Add visual : Create a Vertical Bar Chart with line_item_usage_start_date aggregated by the hour on the x-axis , and the sum of line_item_unblended_cost in the value field. To ensure we only have costs for our workload, we filter on a tag that we've assigned to all our resources. Create a Filter for the graph, create a filter on resource_tags_user_application , a Custom filter that equals ordering You can now see how the cost of the workload in comparison to the workload demand in the chart. We can see some correlation between request count and cost: Create the corresponding graphs on the other two sheets. On each sheet, select the data set , then Add a visual , and then populate the field wells. Make sure you create the filter for resource_tags_user_application . The visualization categorized by UserType , shows correlations depending on the user type: There is also correlation between work and cost: Viewing the images above, you can see the correlation between the usage level, the usage types and cost. There may be a time offset of 1hr depending on how the costs and usage are grouped.","title":"2.5 Add the workload cost visualizations"},{"location":"Cost/CostOpt_Workload/Step3.html","text":"Licensing Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Goals Understand the licensing costs in your workload Understand the costs of licensing in your workload Table of Contents Understand licensing costs in your workload Create an account structure Simulate the change and validate 1. Understand licensing costs in your workload We will analyze the CUR file for any software that includes licensing. First we will see what columns could give us the information we need. Then we look for the amount we are spending on items containing licenses, and how much we are spending on the actual licenses by comparing to a similar non-licensed option. We take this information and decide if the effort required to make the change will be less than what we save, and if the required functionality is still met. 1.1 Analyze CUR columns The first step is to understand what information we may have that can show us costs associated with licenses. Go to the Athena Console: Select the costusage database, you should see the costusagefiles table We want to see all the columns available, and some sample data, so paste the following command and click Run query : SELECT * FROM \"costusage\".\"costusagefiles_reinventworkshop\" limit 10; You can see each column and 10 rows of data below. There are over 100 columns - take a minute or two and slowly scroll through them and view the data. We want to ensure we are only looking at costs for our workload. There is a column at the far right resource_tags_user_application , this contains the tags we put on our resources. Lets limit our search to rows containing the value of ordering . As all our resources were tagged with this value. Run the query: SELECT * FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' limit 30 Now we will look for EC2 Operating System licenses. Look through the data and see if you can spot a column which indicates the operating system used. There are multiple columns we can use, look at the columns line_item_line_item_description and product_operating_system . Note: columns are typically in close to an alphabetical order. We will focus on line_item_line_item_description , you can see Linux , Windows and RHEL . We will look to get the costs of RHEL licensing and compare it to a similar operating system, AWS Linux. Paste the following query and click Run query : SELECT * FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' limit 30 We will now get the costs for each hour for running RHEL. We dont need 100+ columns, so we'll focus on just the following columns: line_item_usage_start_date , linte_item_line_item_description , line_item_unblended_cost . We will sum the unblended_cost column, and group it by usage_start_date , this will give us cost per hour. The order by line orders the output by date. Paste the following query: SELECT line_item_usage_start_date, line_item_line_item_description, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_usage_start_date, line_item_line_item_description order by line_item_usage_start_date asc You can see that the cost per hour of running the RHEL was 0.8128000000000001 per hour. Lets see what instances we were running, so we can find the costs for a comparable instance. Re-run the command: SELECT line_item_usage_type, line_item_availability_zone, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_usage_type, line_item_availability_zone limit 30 Look at the columns: line_item_usage_type , line_item_availability_zone , you will see the values BoxUsage:t3.medium and us-east-1a/b : We see that we are running t3.medium RHEL instances in us-east-1. Open the pricing pages in a new tab: https://aws.amazon.com/ec2/pricing/on-demand/ Ensure the Linux tab is selected, and the region is US East (N. Virginia) . The pricing for AWS Linux is: $0.0416 per hour . Now click the RHEL tab: The pricing will change, the price for RHEL is: $0.1016 . Lets see exactly how much we were running in total, and the price. In the Athena console paste the following query: SELECT line_item_line_item_description, sum(line_item_usage_amount) usage_hours, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_line_item_description You can see we consumed 176 instance hours for a total cost of $17.8815999... . If we were able to run AWS Linux, it would cost: $7.3216 for 176 hours . Thats a saving of $10.55999 or 59% of our instance EC2 costs. As will all optimization opportunities, weigh up the savings gained from making the change, against the effort required to change. Is 59% of your instance EC2 costs each month worth making the change? How long before you pay off your effort? Is all the required functionality still there? 2. Understand the costs of running licensed software in your workload There can be associated costs with running licensed software. Additional resources may be mandated by the software in addition to a base configuration. We will continue our example by looking at operating systems. We will now see if additional resources are required to run RHEL instead of AWS Linux, and the cost of these additional resources. Go into the EC2 console, and click Launch Instance : Select the base Amazon Linux 2 AMI Select a t3.medium instance size, and click Next: Configure Instance Details : Click Next: Add Storage You can see that the default configuration is for 8Gb GP2 of EBS storage. Click Cancel . From the EC2 Console click Launch Instance , and select the Red Hat Enterprise Linux AMI: Select a t3.medium instance and click Next: Configure Instance Details : Click Next: Add Storage You will see that there is 10Gb GP2 of storage required for RHEL as a default. Click Cancel . You need an additional 2Gb of GP2 storage per instance. Go to the EBS pricing page here: https://aws.amazon.com/ebs/pricing/ The price in US East(N. Virginia) is $0.1 per GB-month . We will now calculate the storage savings across our workload. From the Athena Console paste the following query, which gives us the instance IDs along with how many hours they ran for: SELECT distinct line_item_resource_id, sum(line_item_usage_amount) FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_resource_id You can see that each instance ran for 22hrs, which is what the log file contains. You can confirm the number of hours in your data by running the following query: SELECT distinct line_item_usage_start_date FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' As per the previous query, we are running 8 instances requiring an additional 2Gb GP2. This is 16Gb-mo of storage, which is a cost of $1.60 each month on our workload. To see the impact on our workload, lets look at the total cost of storage in the front tier which contained our RHEL instances. Paste the following query SELECT line_item_usage_start_date, line_item_line_item_description, sum(line_item_unblended_cost) as cost, sum(line_item_usage_amount) as amount FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_usage_type like '%EBS%' and resource_tags_user_tier like 'front' group by line_item_usage_start_date, line_item_line_item_description order by line_item_usage_start_date asc You can see each hour the amount was 0.11111111120000003 at a cost of 0.011111111199999999.. . Storage is calculated by Gb-mo , so to get storage that was provisioned: Hourly amount x 24 x (days in month) 0.11111111120000003 x 24 x 30 = 80Gb Lets get the total storage amount and cost for the period with the following query: SELECT line_item_line_item_description, sum(line_item_unblended_cost) as cost, sum(line_item_usage_amount) as amount FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_usage_type like '%EBS%' and resource_tags_user_tier like 'front' group by line_item_line_item_description You can see there was $0.2628922854999997 of cost and an amount of 2.6289228409999925 in our 22hr of log files. This would be a monthly cost of $8.60 for 80Gb. So we could save $1.72 out of a total cost of $8.60 in a month. That is a 20% reduction in storage costs for the Front Tier . All up we save $10.55 due to OS licensing costs and an additional $1.72 on storage each month if we were able to switch from RHEL to AWS Linux. Again weigh up these costs and the benefits of running the software to decide if it is worth changing. 3. Simulate the change and validate We have performed the change in our environment and created new application logs and new Cost and Usage Reports. We will now analyze these new files to validate our analysis. Download the updated CUR and application log files from here: Step2CUR.gz Step2access_log.gz Delete the current CUR and AppLog files in your S3 Buckets. Upload the new files into the same folders in S3 that the previous application & cost files were uploaded into. You do not have to re run the crawlers, as the structure and format is the same as the previous files. When you run a query, it now uses the new data files automatically. Lets see exactly how much we were running in total, and the price. In the Athena console paste the following query: SELECT line_item_line_item_description, sum(line_item_usage_amount) as usage_hours, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%t3.medium%' and resource_tags_user_tier like 'front' group by line_item_line_item_description You can see we consumed 112 instance hours for a total cost of 4.6591999 . If we scaled this up to 176 hours (176/112 x $4.659), it would be $7.3216 . We have verified we did save the estimated saving of $10.55 or 59% (original cost $17.88). Lets analyze the storage with the following query: SELECT line_item_usage_start_date, line_item_line_item_description, sum(line_item_unblended_cost) as cost, sum(line_item_usage_amount) as amount FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_usage_type like '%EBS%' and resource_tags_user_tier like 'front' group by line_item_usage_start_date, line_item_line_item_description order by line_item_usage_start_date asc You can see there was an hourly amount of 0.0888888888 and an hourly cost of $0.0088888888 , which is exactly 8/10 of the original amount (original amount 0.11111111120000003) and cost of storage (going to 8Gb from 10Gb volumes). In this step we discovered Licensing costs, and associated additional resource costs. We analyzed them to make sure it was worth the effort, made the change (simulated) then verified our change. You have just performed your first Cost Optimization cycle! Always make sure you analyze to make sure the change is worth the effort, and verify after you have performed the change to confirm you realized the savings.","title":"Licensing"},{"location":"Cost/CostOpt_Workload/Step3.html#licensing","text":"","title":"Licensing"},{"location":"Cost/CostOpt_Workload/Step3.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/CostOpt_Workload/Step3.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/CostOpt_Workload/Step3.html#goals","text":"Understand the licensing costs in your workload Understand the costs of licensing in your workload","title":"Goals"},{"location":"Cost/CostOpt_Workload/Step3.html#table-of-contents","text":"Understand licensing costs in your workload Create an account structure Simulate the change and validate","title":"Table of Contents"},{"location":"Cost/CostOpt_Workload/Step3.html#1-understand-licensing-costs-in-your-workload","text":"We will analyze the CUR file for any software that includes licensing. First we will see what columns could give us the information we need. Then we look for the amount we are spending on items containing licenses, and how much we are spending on the actual licenses by comparing to a similar non-licensed option. We take this information and decide if the effort required to make the change will be less than what we save, and if the required functionality is still met.","title":"1. Understand licensing costs in your workload "},{"location":"Cost/CostOpt_Workload/Step3.html#11-analyze-cur-columns","text":"The first step is to understand what information we may have that can show us costs associated with licenses. Go to the Athena Console: Select the costusage database, you should see the costusagefiles table We want to see all the columns available, and some sample data, so paste the following command and click Run query : SELECT * FROM \"costusage\".\"costusagefiles_reinventworkshop\" limit 10; You can see each column and 10 rows of data below. There are over 100 columns - take a minute or two and slowly scroll through them and view the data. We want to ensure we are only looking at costs for our workload. There is a column at the far right resource_tags_user_application , this contains the tags we put on our resources. Lets limit our search to rows containing the value of ordering . As all our resources were tagged with this value. Run the query: SELECT * FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' limit 30 Now we will look for EC2 Operating System licenses. Look through the data and see if you can spot a column which indicates the operating system used. There are multiple columns we can use, look at the columns line_item_line_item_description and product_operating_system . Note: columns are typically in close to an alphabetical order. We will focus on line_item_line_item_description , you can see Linux , Windows and RHEL . We will look to get the costs of RHEL licensing and compare it to a similar operating system, AWS Linux. Paste the following query and click Run query : SELECT * FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' limit 30 We will now get the costs for each hour for running RHEL. We dont need 100+ columns, so we'll focus on just the following columns: line_item_usage_start_date , linte_item_line_item_description , line_item_unblended_cost . We will sum the unblended_cost column, and group it by usage_start_date , this will give us cost per hour. The order by line orders the output by date. Paste the following query: SELECT line_item_usage_start_date, line_item_line_item_description, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_usage_start_date, line_item_line_item_description order by line_item_usage_start_date asc You can see that the cost per hour of running the RHEL was 0.8128000000000001 per hour. Lets see what instances we were running, so we can find the costs for a comparable instance. Re-run the command: SELECT line_item_usage_type, line_item_availability_zone, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_usage_type, line_item_availability_zone limit 30 Look at the columns: line_item_usage_type , line_item_availability_zone , you will see the values BoxUsage:t3.medium and us-east-1a/b : We see that we are running t3.medium RHEL instances in us-east-1. Open the pricing pages in a new tab: https://aws.amazon.com/ec2/pricing/on-demand/ Ensure the Linux tab is selected, and the region is US East (N. Virginia) . The pricing for AWS Linux is: $0.0416 per hour . Now click the RHEL tab: The pricing will change, the price for RHEL is: $0.1016 . Lets see exactly how much we were running in total, and the price. In the Athena console paste the following query: SELECT line_item_line_item_description, sum(line_item_usage_amount) usage_hours, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_line_item_description You can see we consumed 176 instance hours for a total cost of $17.8815999... . If we were able to run AWS Linux, it would cost: $7.3216 for 176 hours . Thats a saving of $10.55999 or 59% of our instance EC2 costs. As will all optimization opportunities, weigh up the savings gained from making the change, against the effort required to change. Is 59% of your instance EC2 costs each month worth making the change? How long before you pay off your effort? Is all the required functionality still there?","title":"1.1 Analyze CUR columns"},{"location":"Cost/CostOpt_Workload/Step3.html#2-understand-the-costs-of-running-licensed-software-in-your-workload","text":"There can be associated costs with running licensed software. Additional resources may be mandated by the software in addition to a base configuration. We will continue our example by looking at operating systems. We will now see if additional resources are required to run RHEL instead of AWS Linux, and the cost of these additional resources. Go into the EC2 console, and click Launch Instance : Select the base Amazon Linux 2 AMI Select a t3.medium instance size, and click Next: Configure Instance Details : Click Next: Add Storage You can see that the default configuration is for 8Gb GP2 of EBS storage. Click Cancel . From the EC2 Console click Launch Instance , and select the Red Hat Enterprise Linux AMI: Select a t3.medium instance and click Next: Configure Instance Details : Click Next: Add Storage You will see that there is 10Gb GP2 of storage required for RHEL as a default. Click Cancel . You need an additional 2Gb of GP2 storage per instance. Go to the EBS pricing page here: https://aws.amazon.com/ebs/pricing/ The price in US East(N. Virginia) is $0.1 per GB-month . We will now calculate the storage savings across our workload. From the Athena Console paste the following query, which gives us the instance IDs along with how many hours they ran for: SELECT distinct line_item_resource_id, sum(line_item_usage_amount) FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' group by line_item_resource_id You can see that each instance ran for 22hrs, which is what the log file contains. You can confirm the number of hours in your data by running the following query: SELECT distinct line_item_usage_start_date FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%RHEL%' As per the previous query, we are running 8 instances requiring an additional 2Gb GP2. This is 16Gb-mo of storage, which is a cost of $1.60 each month on our workload. To see the impact on our workload, lets look at the total cost of storage in the front tier which contained our RHEL instances. Paste the following query SELECT line_item_usage_start_date, line_item_line_item_description, sum(line_item_unblended_cost) as cost, sum(line_item_usage_amount) as amount FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_usage_type like '%EBS%' and resource_tags_user_tier like 'front' group by line_item_usage_start_date, line_item_line_item_description order by line_item_usage_start_date asc You can see each hour the amount was 0.11111111120000003 at a cost of 0.011111111199999999.. . Storage is calculated by Gb-mo , so to get storage that was provisioned: Hourly amount x 24 x (days in month) 0.11111111120000003 x 24 x 30 = 80Gb Lets get the total storage amount and cost for the period with the following query: SELECT line_item_line_item_description, sum(line_item_unblended_cost) as cost, sum(line_item_usage_amount) as amount FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_usage_type like '%EBS%' and resource_tags_user_tier like 'front' group by line_item_line_item_description You can see there was $0.2628922854999997 of cost and an amount of 2.6289228409999925 in our 22hr of log files. This would be a monthly cost of $8.60 for 80Gb. So we could save $1.72 out of a total cost of $8.60 in a month. That is a 20% reduction in storage costs for the Front Tier . All up we save $10.55 due to OS licensing costs and an additional $1.72 on storage each month if we were able to switch from RHEL to AWS Linux. Again weigh up these costs and the benefits of running the software to decide if it is worth changing.","title":"2. Understand the costs of running licensed software in your workload "},{"location":"Cost/CostOpt_Workload/Step3.html#3-simulate-the-change-and-validate","text":"We have performed the change in our environment and created new application logs and new Cost and Usage Reports. We will now analyze these new files to validate our analysis. Download the updated CUR and application log files from here: Step2CUR.gz Step2access_log.gz Delete the current CUR and AppLog files in your S3 Buckets. Upload the new files into the same folders in S3 that the previous application & cost files were uploaded into. You do not have to re run the crawlers, as the structure and format is the same as the previous files. When you run a query, it now uses the new data files automatically. Lets see exactly how much we were running in total, and the price. In the Athena console paste the following query: SELECT line_item_line_item_description, sum(line_item_usage_amount) as usage_hours, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%t3.medium%' and resource_tags_user_tier like 'front' group by line_item_line_item_description You can see we consumed 112 instance hours for a total cost of 4.6591999 . If we scaled this up to 176 hours (176/112 x $4.659), it would be $7.3216 . We have verified we did save the estimated saving of $10.55 or 59% (original cost $17.88). Lets analyze the storage with the following query: SELECT line_item_usage_start_date, line_item_line_item_description, sum(line_item_unblended_cost) as cost, sum(line_item_usage_amount) as amount FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_usage_type like '%EBS%' and resource_tags_user_tier like 'front' group by line_item_usage_start_date, line_item_line_item_description order by line_item_usage_start_date asc You can see there was an hourly amount of 0.0888888888 and an hourly cost of $0.0088888888 , which is exactly 8/10 of the original amount (original amount 0.11111111120000003) and cost of storage (going to 8Gb from 10Gb volumes). In this step we discovered Licensing costs, and associated additional resource costs. We analyzed them to make sure it was worth the effort, made the change (simulated) then verified our change. You have just performed your first Cost Optimization cycle! Always make sure you analyze to make sure the change is worth the effort, and verify after you have performed the change to confirm you realized the savings.","title":"3. Simulate the change and validate"},{"location":"Cost/CostOpt_Workload/Step4.html","text":"Storage Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Goals Understand the cost of storage in your environment Understand where the storage costs are in your environment Table of Contents Understand the cost of storage in your workload Use of storage Simulate the change and validate 1. Understand the cost of storage in your workload Storage costs may make up a small component of the overall cost of a workload, however at an enterprise level with large numbers of workloads or a larger workload, storage costs can become significant. There may not be one single resource that contributes to a large storage cost, it could be a large number of small amounts. So we will develop a technique to find where our storage costs are and how we can minimize waste. We will focus on EBS storage, but this technique can be applied to any storage type, and other services where there are large numbers of small resources. For this exercise we start with the files that were used at the end of Step3, these should already be loaded if you just finished Step3: - [Step2CUR.gz](Code/Step2CUR.gz) - [Step2access_log.gz](Code/Step2AccessLog.gz) 1.1 Discover large volumes The first step is to look for any large volumes, which can singled out for deeper inspection. In the Athena console execute the following query: SELECT line_item_resource_id, sum(line_item_unblended_cost) as cost, sum(line_item_usage_amount) as amount FROM \"costusage\".\"costusagefiles_reinventworkshop\" where line_item_usage_type like '%EBS%' and resource_tags_user_application like 'ordering' group by line_item_resource_id order by cost desc limit 50 It will return a list of volumes with the largest cost at the top of the list, look for any large volumes at the top of the list. If there are any significant costs, you search for the volume by its resource ID, copy the top resource ID, in our example one of them is: vol-0c4ceffc35e2a8d43 You could then see in the console what instance is attached to. We could logon to that instance and ensure there is a high level of utilization of the storage, as it is a large cost. 1.2 Large number of small volumes This technique is useful for scale, such as large organizations that have standard images or templates for resources, as any waste in the template can multiply to a large amount on the bill. We will use a query to get the number of volumes and associated cost, grouped by size. This will return one line per unique volume size, for example: the total cost and number of 10Gb volumes, total cost and number of 15Gb volumes. The query uses 730 as the hours in the month, this will vary depending on the days in the month. 730 is the average monthly hours for a 365 day year. In the Athena console execute the following query: SELECT sum(line_item_unblended_cost) as Cost, line_item_usage_amount*730 as Vol_Size, count(*)/count(distinct line_item_usage_start_date) as Num_Volumes FROM \"costusage\".\"costusagefiles_reinventworkshop\" where line_item_usage_type like '%EBS%' and resource_tags_user_application like 'ordering' group by line_item_usage_amount order by cost desc limit 50 You can see we have 23 x ~30Gb volumes costing $2.283 , and 26 x ~8Gb volumes costing $0.66444 . In the previous step our largest single EBS volume cost was only 1.9166 , even the smallest amount of time spent to optimize $2 of costs would end in wasted time. With the query above, we can see there is a large number of volumes the same size, these may be boot volumes, so there could be potential savings in a large environment if we were to make one change : the boot volume size! 1.3 Wasted space on volumes There are many different ways to manage your fleet of resources, but its important to do it at scale, and be able to pick patterns. In the previous example we highlighted that there may be lots of boot volumes that make up your total storage cost. You need to be able to understand if these volumes are fully utilized before making changes. Inventory management is out of scope for this lab, so we will show how we ran a small Linux script that can be used to display volume utilization for spot checks. We used a return state within the script to trigger Systems Manager to show success or fail depending on if there is wasted storage. We program the script to succeed if there is the specified amount of waste, otherwise it will fail which means there is no waste. We used AWS-RunShellScript from within Systems Manager : The following script checks if the volume utilization is below 40% and the amount of free space is more than 5Gb. You can modify this script to ensure the amount of waste is over a certain $ amount, and there is still enough headroom on the volume: df | grep -vE '^Filesystem|tmpfs|cdrom' | awk '{ print $1 \" \" $5 \" \" $2 \" \" $4}' | while read output; do volume=$(echo $output | awk '{ print $1 }') usepercent=$(echo $output | awk '{ print $2 }' | cut -d'%' -f1 ) free=$(echo $output | awk '{ print $4 }') if [ $usepercent -lt 40 ] && [ $free -gt 5000000 ] then echo \"$HOSTNAME has less than 40% utilization and greater than 5Gb free on $volume\" else exit 1 fi done You can see here we had 4 instances that have a lot of wasted space on their volumes, these are our middle Tier application servers: If there are large numbers of success, and the volume names correspond to boot volumes or other recognizable volumes in your environment, then they could be good candidates for improvement. In our simulated environment, it returned success for all c5 instances in the middle tier. 1.4 Use of storage The other area for saving with storage is to find the best type of storage for your purpose. If you have large amounts of storage with high utilization/low free space, then this could potentially yield large benefits, this will be highly dependent on your workload. We will show how we found what our storage was used for in the test environment. We executed the following disk usage command and view the results: sudo du -h --max-depth=1 | sort -hr You can see there was a lot of storage used in the ftp and var directories, lets look further. Iteratively we executed disk usage statements for the large directories, here we traversed the /var directory to find the largest usage on our front end instance: du -h --max-depth=1 /var | sort -hr du -h --max-depth=1 /var/www | sort -hr du -h --max-depth=1 /var/www/html | sort -hr So we found the directory consuming the storage and can look at the contents. Understand how the storage is being used and look for alternative services. You can then use the AWS storage page to see if there are alternative services or classes of storage that would be most cost effective: https://aws.amazon.com/products/storage/ 2. Simulate the change and validate We saw earlier that our C5 middle tier instances had unused 30Gb volumes that were attached. We will simulate the change, and remove the excess storage. Wait at this step until we tell you to proceed In the Athena console execute the following query: SELECT sum(line_item_unblended_cost) as Cost, line_item_usage_amount*730 as Vol_Size, count(*)/count(distinct line_item_usage_start_date) as Num_Volumes FROM \"costusage\".\"costusagefiles_reinventworkshop\" where line_item_usage_type like '%EBS%' and resource_tags_user_application like 'ordering' group by line_item_usage_amount order by cost desc limit 50 You can see we now have 8 x 30Gb volumes at a cost of $0.7667 and 13 x 8Gb volumes at a cost of $0.33222 . We revmoved the 4 unused 30Gb volumes from the middle tier, and deleted other unused volumes: We have removed $1.84852 of storage cost from our front and middle tiers. This has removed 62% of our storage costs. In this step we analyzed storage costs, looking for small numbers of large savings opportunities, and large numbers of small savings opportunities. Use these methods to find gains across storage and other services where there are large numbers of smaller costs.","title":"Storage"},{"location":"Cost/CostOpt_Workload/Step4.html#storage","text":"","title":"Storage"},{"location":"Cost/CostOpt_Workload/Step4.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/CostOpt_Workload/Step4.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/CostOpt_Workload/Step4.html#goals","text":"Understand the cost of storage in your environment Understand where the storage costs are in your environment","title":"Goals"},{"location":"Cost/CostOpt_Workload/Step4.html#table-of-contents","text":"Understand the cost of storage in your workload Use of storage Simulate the change and validate","title":"Table of Contents"},{"location":"Cost/CostOpt_Workload/Step4.html#1-understand-the-cost-of-storage-in-your-workload","text":"Storage costs may make up a small component of the overall cost of a workload, however at an enterprise level with large numbers of workloads or a larger workload, storage costs can become significant. There may not be one single resource that contributes to a large storage cost, it could be a large number of small amounts. So we will develop a technique to find where our storage costs are and how we can minimize waste. We will focus on EBS storage, but this technique can be applied to any storage type, and other services where there are large numbers of small resources. For this exercise we start with the files that were used at the end of Step3, these should already be loaded if you just finished Step3: - [Step2CUR.gz](Code/Step2CUR.gz) - [Step2access_log.gz](Code/Step2AccessLog.gz)","title":"1. Understand the cost of storage in your workload"},{"location":"Cost/CostOpt_Workload/Step4.html#11-discover-large-volumes","text":"The first step is to look for any large volumes, which can singled out for deeper inspection. In the Athena console execute the following query: SELECT line_item_resource_id, sum(line_item_unblended_cost) as cost, sum(line_item_usage_amount) as amount FROM \"costusage\".\"costusagefiles_reinventworkshop\" where line_item_usage_type like '%EBS%' and resource_tags_user_application like 'ordering' group by line_item_resource_id order by cost desc limit 50 It will return a list of volumes with the largest cost at the top of the list, look for any large volumes at the top of the list. If there are any significant costs, you search for the volume by its resource ID, copy the top resource ID, in our example one of them is: vol-0c4ceffc35e2a8d43 You could then see in the console what instance is attached to. We could logon to that instance and ensure there is a high level of utilization of the storage, as it is a large cost.","title":"1.1 Discover large volumes"},{"location":"Cost/CostOpt_Workload/Step4.html#12-large-number-of-small-volumes","text":"This technique is useful for scale, such as large organizations that have standard images or templates for resources, as any waste in the template can multiply to a large amount on the bill. We will use a query to get the number of volumes and associated cost, grouped by size. This will return one line per unique volume size, for example: the total cost and number of 10Gb volumes, total cost and number of 15Gb volumes. The query uses 730 as the hours in the month, this will vary depending on the days in the month. 730 is the average monthly hours for a 365 day year. In the Athena console execute the following query: SELECT sum(line_item_unblended_cost) as Cost, line_item_usage_amount*730 as Vol_Size, count(*)/count(distinct line_item_usage_start_date) as Num_Volumes FROM \"costusage\".\"costusagefiles_reinventworkshop\" where line_item_usage_type like '%EBS%' and resource_tags_user_application like 'ordering' group by line_item_usage_amount order by cost desc limit 50 You can see we have 23 x ~30Gb volumes costing $2.283 , and 26 x ~8Gb volumes costing $0.66444 . In the previous step our largest single EBS volume cost was only 1.9166 , even the smallest amount of time spent to optimize $2 of costs would end in wasted time. With the query above, we can see there is a large number of volumes the same size, these may be boot volumes, so there could be potential savings in a large environment if we were to make one change : the boot volume size!","title":"1.2 Large number of small volumes"},{"location":"Cost/CostOpt_Workload/Step4.html#13-wasted-space-on-volumes","text":"There are many different ways to manage your fleet of resources, but its important to do it at scale, and be able to pick patterns. In the previous example we highlighted that there may be lots of boot volumes that make up your total storage cost. You need to be able to understand if these volumes are fully utilized before making changes. Inventory management is out of scope for this lab, so we will show how we ran a small Linux script that can be used to display volume utilization for spot checks. We used a return state within the script to trigger Systems Manager to show success or fail depending on if there is wasted storage. We program the script to succeed if there is the specified amount of waste, otherwise it will fail which means there is no waste. We used AWS-RunShellScript from within Systems Manager : The following script checks if the volume utilization is below 40% and the amount of free space is more than 5Gb. You can modify this script to ensure the amount of waste is over a certain $ amount, and there is still enough headroom on the volume: df | grep -vE '^Filesystem|tmpfs|cdrom' | awk '{ print $1 \" \" $5 \" \" $2 \" \" $4}' | while read output; do volume=$(echo $output | awk '{ print $1 }') usepercent=$(echo $output | awk '{ print $2 }' | cut -d'%' -f1 ) free=$(echo $output | awk '{ print $4 }') if [ $usepercent -lt 40 ] && [ $free -gt 5000000 ] then echo \"$HOSTNAME has less than 40% utilization and greater than 5Gb free on $volume\" else exit 1 fi done You can see here we had 4 instances that have a lot of wasted space on their volumes, these are our middle Tier application servers: If there are large numbers of success, and the volume names correspond to boot volumes or other recognizable volumes in your environment, then they could be good candidates for improvement. In our simulated environment, it returned success for all c5 instances in the middle tier.","title":"1.3 Wasted space on volumes"},{"location":"Cost/CostOpt_Workload/Step4.html#14-use-of-storage","text":"The other area for saving with storage is to find the best type of storage for your purpose. If you have large amounts of storage with high utilization/low free space, then this could potentially yield large benefits, this will be highly dependent on your workload. We will show how we found what our storage was used for in the test environment. We executed the following disk usage command and view the results: sudo du -h --max-depth=1 | sort -hr You can see there was a lot of storage used in the ftp and var directories, lets look further. Iteratively we executed disk usage statements for the large directories, here we traversed the /var directory to find the largest usage on our front end instance: du -h --max-depth=1 /var | sort -hr du -h --max-depth=1 /var/www | sort -hr du -h --max-depth=1 /var/www/html | sort -hr So we found the directory consuming the storage and can look at the contents. Understand how the storage is being used and look for alternative services. You can then use the AWS storage page to see if there are alternative services or classes of storage that would be most cost effective: https://aws.amazon.com/products/storage/","title":"1.4 Use of storage"},{"location":"Cost/CostOpt_Workload/Step4.html#2-simulate-the-change-and-validate","text":"We saw earlier that our C5 middle tier instances had unused 30Gb volumes that were attached. We will simulate the change, and remove the excess storage. Wait at this step until we tell you to proceed In the Athena console execute the following query: SELECT sum(line_item_unblended_cost) as Cost, line_item_usage_amount*730 as Vol_Size, count(*)/count(distinct line_item_usage_start_date) as Num_Volumes FROM \"costusage\".\"costusagefiles_reinventworkshop\" where line_item_usage_type like '%EBS%' and resource_tags_user_application like 'ordering' group by line_item_usage_amount order by cost desc limit 50 You can see we now have 8 x 30Gb volumes at a cost of $0.7667 and 13 x 8Gb volumes at a cost of $0.33222 . We revmoved the 4 unused 30Gb volumes from the middle tier, and deleted other unused volumes: We have removed $1.84852 of storage cost from our front and middle tiers. This has removed 62% of our storage costs. In this step we analyzed storage costs, looking for small numbers of large savings opportunities, and large numbers of small savings opportunities. Use these methods to find gains across storage and other services where there are large numbers of smaller costs.","title":"2. Simulate the change and validate"},{"location":"Cost/CostOpt_Workload/Step5.html","text":"Data Transfer Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Goals Understand the cost of data transfer Understand where the data transfer costs are in your environment Table of Contents Understand licensing costs in your workload Create an account structure Simulate the change and validate 1. Understand the cost of data transfer in your workload Depending on the type of workload, data transfer may be a small or significant cost of the overall workload cost. There are also different pricing rates for transferring data (just as there is for computing - i.e. instance types, lambda, containers) depending on the source and destination of the transfer, so we will look at what makes up your data transfer costs. 1.1 Understand data transfer types and total We will look for the word transfer in the line_item_line_item_description column to identify data transfer. We will look at the total amount in your workload and then the different types of data transfer. From the Athena console, and run the following query: select line_item_product_code, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%transfer%' group by line_item_product_code order by cost desc You can see the product code AmazonEC2 , the amount and cost of data transfer. At $13.2286 of a workload total of $44.2776 , it is approximately 30% of our total workload cost . Now lets look deeper into the types of data transfer that is in our workload. We will look at the description, run the following query: select line_item_product_code, line_item_line_item_description, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%transfer%' group by line_item_product_code, line_item_line_item_description order by cost desc We can see that it is regional data transfer between AZs or using Elastic IPs, and also data transfer to Oregon that make up the main cost: We need some more information to find the source of the data transfer, lets look at the usage type column to provide some more information. Run the following query: select line_item_product_code, line_item_usage_type, line_item_line_item_description, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%transfer%' group by line_item_product_code, line_item_usage_type, line_item_line_item_description order by cost desc We can see that it is Regional transfer, but no additional insights can be gained. Lets look at the operation column to provide more insights. Run the following query: select line_item_product_code, line_item_operation, line_item_line_item_description, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%transfer%' group by line_item_product_code, line_item_operation, line_item_line_item_description order by cost desc You can see that the operation column provides more detail. The first two lines PublicIP-In , LoadBalancing-PublicIP-Out are the majority of usage and cost, and are very similar in amounts and cost. There is also 20% of costs in the next two lines InterZone . Lets now find the source and destination of the data transfer, run the following query: select line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, line_item_unblended_rate, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%transfer%' group by line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, line_item_unblended_rate order by cost desc We can see that the main costs are from US East (N. Virginia) to US East (N. Virginia) . The description tells us its between AZs or Elastic IPs or ELB traffic. Since we know the product, location, amount and cost of our data transfer, we will move onto finding the specific resources in our workload transferring the data. 2. Understand where your data transfer is in your workload Now we know the type of data transfer we have in our workload and the costs, we will find the resources that are making the data transfer in order to find a way to reduce the cost if possible. We will take the information from the previous step and find out which tier and resources are the source and destination. 2.1 Workload Tiers We will find which tiers the data transfer is coming from and going too We will narrow our focus to the top costs, run the following query: select line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%regional data transfer%' and (line_item_operation like '%InterZone%' or line_item_operation like '%PublicIP%') group by line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description order by cost desc We can assume the PublicIP transfer in and out is to the Front tier - as its transfer to and from the internet. Lets look at the InterZone transfer and see which of our workload tiers it is in. We will use our tags for this, by selecting the resource_tags_user_tier column. Run the following query: select line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, resource_tags_user_tier, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%regional data transfer%' and line_item_operation like '%InterZone%' group by line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, resource_tags_user_tier order by cost desc You can see there are now 4 lines of results, the top 2 making up the majority of costs and usage. If you read the columns together you can see that it is: Data transfer InterZone-In to middle tier , and data transfer InterZone-Out from front tier . The usage amounts are close to identical, so we have data transfer being transferred from the front to the middle tier. 2.2 Workload Resources Now we know the tiers, lets track down the resources responsible. We will now look at the individual resources that are the source & destination for the traffic. Include the line_item_resource_id column to get the individual resources. Run the following query: select line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, resource_tags_user_tier, line_item_resource_id, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%regional data transfer%' and line_item_operation like '%InterZone%' group by line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, resource_tags_user_tier, line_item_resource_id order by cost desc We can see the top 4 items by amount and cost, are instances in the middle tier, with usage spread across each of them quite evenly. Next there are 8 instances in the front tier which are the source, with the amount of traffic roughly half that of each middle tier instance - as there are 8 front tier instances and 4 middle tier instances. We look at the pricing page here: https://aws.amazon.com/ec2/pricing/on-demand/ select the correct region and scroll down to Data Transfer . There is the heading Data Transfer within the same AWS Region , and it specifies Data transferred \"in\" to and \"out\" from Amazon EC2.... across Availability Zones..... in the same AWS Region is charged at $0.01/GB in each direction. Further down it states: Data transferred between Amazon EC2...instances and Elastic Network Interfaces in the same Availability Zone is free. If we modify our workload to only transfer data between front and middle tiers in the same AZ, we can remove the data transfer cost. In this sample workload the saving is $2.10 per day, which is approximately 5% of our total workload cost. Use your figure to determine if the effort involved will provide a saving over the lifetime of the workload. Ensure you factor in any changes in growth or usage that are predicted. NOTE: Changing the way your application is allowed to route traffic may impact the availability/resiliency of your application. Ensure you maintain the required levels of availability/resiliency for your customers. If data transfer is required between AZs for availability/resiliency, then the cost of data transfer is part of the cost of availability/resiliency, the same way as additional instances and storage is. 3. Simulate the change and validate We will simulate the change of having traffic only go between the tiers within the same AZ. Wait at this step until we tell you to proceed Lets see that we've removed the Inter AZ Data transfer, run the following query: select line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, resource_tags_user_tier, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%regional data transfer%' and line_item_operation like '%InterZone%' group by line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, resource_tags_user_tier order by cost desc Previously we had $1.05 each way between the tiers, we now have virtually zero: Looking at our overall costs, run the following query in Athena: select line_item_product_code, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%transfer%' group by line_item_product_code order by cost desc You can see that is is $11.45 , providing a saving of $1.77 , or 13% of our overall workload cost. We have successfully analyzed our data transfer costs, found out where we can make improvements and implemented the changes to restrict data transfer to within an AZ.","title":"Data Transfer"},{"location":"Cost/CostOpt_Workload/Step5.html#data-transfer","text":"","title":"Data Transfer"},{"location":"Cost/CostOpt_Workload/Step5.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/CostOpt_Workload/Step5.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/CostOpt_Workload/Step5.html#goals","text":"Understand the cost of data transfer Understand where the data transfer costs are in your environment","title":"Goals"},{"location":"Cost/CostOpt_Workload/Step5.html#table-of-contents","text":"Understand licensing costs in your workload Create an account structure Simulate the change and validate","title":"Table of Contents"},{"location":"Cost/CostOpt_Workload/Step5.html#1-understand-the-cost-of-data-transfer-in-your-workload","text":"Depending on the type of workload, data transfer may be a small or significant cost of the overall workload cost. There are also different pricing rates for transferring data (just as there is for computing - i.e. instance types, lambda, containers) depending on the source and destination of the transfer, so we will look at what makes up your data transfer costs.","title":"1. Understand the cost of data transfer in your workload"},{"location":"Cost/CostOpt_Workload/Step5.html#11-understand-data-transfer-types-and-total","text":"We will look for the word transfer in the line_item_line_item_description column to identify data transfer. We will look at the total amount in your workload and then the different types of data transfer. From the Athena console, and run the following query: select line_item_product_code, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%transfer%' group by line_item_product_code order by cost desc You can see the product code AmazonEC2 , the amount and cost of data transfer. At $13.2286 of a workload total of $44.2776 , it is approximately 30% of our total workload cost . Now lets look deeper into the types of data transfer that is in our workload. We will look at the description, run the following query: select line_item_product_code, line_item_line_item_description, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%transfer%' group by line_item_product_code, line_item_line_item_description order by cost desc We can see that it is regional data transfer between AZs or using Elastic IPs, and also data transfer to Oregon that make up the main cost: We need some more information to find the source of the data transfer, lets look at the usage type column to provide some more information. Run the following query: select line_item_product_code, line_item_usage_type, line_item_line_item_description, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%transfer%' group by line_item_product_code, line_item_usage_type, line_item_line_item_description order by cost desc We can see that it is Regional transfer, but no additional insights can be gained. Lets look at the operation column to provide more insights. Run the following query: select line_item_product_code, line_item_operation, line_item_line_item_description, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%transfer%' group by line_item_product_code, line_item_operation, line_item_line_item_description order by cost desc You can see that the operation column provides more detail. The first two lines PublicIP-In , LoadBalancing-PublicIP-Out are the majority of usage and cost, and are very similar in amounts and cost. There is also 20% of costs in the next two lines InterZone . Lets now find the source and destination of the data transfer, run the following query: select line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, line_item_unblended_rate, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%transfer%' group by line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, line_item_unblended_rate order by cost desc We can see that the main costs are from US East (N. Virginia) to US East (N. Virginia) . The description tells us its between AZs or Elastic IPs or ELB traffic. Since we know the product, location, amount and cost of our data transfer, we will move onto finding the specific resources in our workload transferring the data.","title":"1.1 Understand data transfer types and total"},{"location":"Cost/CostOpt_Workload/Step5.html#2-understand-where-your-data-transfer-is-in-your-workload","text":"Now we know the type of data transfer we have in our workload and the costs, we will find the resources that are making the data transfer in order to find a way to reduce the cost if possible. We will take the information from the previous step and find out which tier and resources are the source and destination.","title":"2. Understand where your data transfer is in your workload"},{"location":"Cost/CostOpt_Workload/Step5.html#21-workload-tiers","text":"We will find which tiers the data transfer is coming from and going too We will narrow our focus to the top costs, run the following query: select line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%regional data transfer%' and (line_item_operation like '%InterZone%' or line_item_operation like '%PublicIP%') group by line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description order by cost desc We can assume the PublicIP transfer in and out is to the Front tier - as its transfer to and from the internet. Lets look at the InterZone transfer and see which of our workload tiers it is in. We will use our tags for this, by selecting the resource_tags_user_tier column. Run the following query: select line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, resource_tags_user_tier, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%regional data transfer%' and line_item_operation like '%InterZone%' group by line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, resource_tags_user_tier order by cost desc You can see there are now 4 lines of results, the top 2 making up the majority of costs and usage. If you read the columns together you can see that it is: Data transfer InterZone-In to middle tier , and data transfer InterZone-Out from front tier . The usage amounts are close to identical, so we have data transfer being transferred from the front to the middle tier.","title":"2.1 Workload Tiers"},{"location":"Cost/CostOpt_Workload/Step5.html#22-workload-resources","text":"Now we know the tiers, lets track down the resources responsible. We will now look at the individual resources that are the source & destination for the traffic. Include the line_item_resource_id column to get the individual resources. Run the following query: select line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, resource_tags_user_tier, line_item_resource_id, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%regional data transfer%' and line_item_operation like '%InterZone%' group by line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, resource_tags_user_tier, line_item_resource_id order by cost desc We can see the top 4 items by amount and cost, are instances in the middle tier, with usage spread across each of them quite evenly. Next there are 8 instances in the front tier which are the source, with the amount of traffic roughly half that of each middle tier instance - as there are 8 front tier instances and 4 middle tier instances. We look at the pricing page here: https://aws.amazon.com/ec2/pricing/on-demand/ select the correct region and scroll down to Data Transfer . There is the heading Data Transfer within the same AWS Region , and it specifies Data transferred \"in\" to and \"out\" from Amazon EC2.... across Availability Zones..... in the same AWS Region is charged at $0.01/GB in each direction. Further down it states: Data transferred between Amazon EC2...instances and Elastic Network Interfaces in the same Availability Zone is free. If we modify our workload to only transfer data between front and middle tiers in the same AZ, we can remove the data transfer cost. In this sample workload the saving is $2.10 per day, which is approximately 5% of our total workload cost. Use your figure to determine if the effort involved will provide a saving over the lifetime of the workload. Ensure you factor in any changes in growth or usage that are predicted. NOTE: Changing the way your application is allowed to route traffic may impact the availability/resiliency of your application. Ensure you maintain the required levels of availability/resiliency for your customers. If data transfer is required between AZs for availability/resiliency, then the cost of data transfer is part of the cost of availability/resiliency, the same way as additional instances and storage is.","title":"2.2 Workload Resources"},{"location":"Cost/CostOpt_Workload/Step5.html#3-simulate-the-change-and-validate","text":"We will simulate the change of having traffic only go between the tiers within the same AZ. Wait at this step until we tell you to proceed Lets see that we've removed the Inter AZ Data transfer, run the following query: select line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, resource_tags_user_tier, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%regional data transfer%' and line_item_operation like '%InterZone%' group by line_item_product_code, line_item_operation, product_from_location, product_to_location, line_item_line_item_description, resource_tags_user_tier order by cost desc Previously we had $1.05 each way between the tiers, we now have virtually zero: Looking at our overall costs, run the following query in Athena: select line_item_product_code, sum(line_item_usage_amount) as usage, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' and line_item_line_item_description like '%transfer%' group by line_item_product_code order by cost desc You can see that is is $11.45 , providing a saving of $1.77 , or 13% of our overall workload cost. We have successfully analyzed our data transfer costs, found out where we can make improvements and implemented the changes to restrict data transfer to within an AZ.","title":"3. Simulate the change and validate "},{"location":"Cost/CostOpt_Workload/Step6.html","text":"Pricing Models Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Goals Implement pricing models for relevant components in this workload Table of Contents Implement pricing models For this exercise we start with the files that were used at the end of Step4, these should already be loaded if you just finished Step4: - Step4CUR.gz - Step4access_log.gz 1. Implement Pricing Models Perform the following lab to perform a savings plan analysis on a set of accounts: Pricing Models SPs We will confirm the rates and prices we are paying for our resources before we apply pricing models, and also throughout the operation of our workload. If our workload no longer has pricing model discounts applied, it may mean your pricing model coverage is too low. Lets have a look at our instance costs, go into the Athena console & run the following query: select line_item_product_code as product, line_item_usage_type, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where (resource_tags_user_application like 'ordering' and line_item_usage_type like 'BoxUsage%') or line_item_line_item_type like 'SavingsPlan%' group by line_item_product_code, line_item_usage_type order by cost desc You can see the workload costs for the instances, which is $24.9956 : Now lets look at the rates and costs that make up that figure, run the following query: select line_item_product_code as product, line_item_usage_type, sum(line_item_usage_amount) as usage, line_item_unblended_rate as rate, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where (resource_tags_user_application like 'ordering' and line_item_usage_type like 'BoxUsage%') or line_item_line_item_type like 'SavingsPlan%' group by line_item_product_code, line_item_unblended_rate, line_item_usage_type order by cost desc We can see our main workload resources and rates: Work with your internal teams so they can understand your projected usage and business forecast. The appropriate discount models will then be applied to your workload. 2. Simulate the change and validate We will simulate the change of purchasing pricing models. Wait at this step until we tell you to proceed Lets check our costs for the workload after savings plans have been applied, run the following query: select line_item_product_code as product, line_item_usage_type, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where (resource_tags_user_application like 'ordering' and line_item_usage_type like 'BoxUsage%') or line_item_line_item_type like 'SavingsPlan%' group by line_item_product_code, line_item_usage_type order by cost desc You can see the total costs are now $21.986 : You can see the rates and prices by running the following query, also the savings plan commitments. Note the negation lines: select line_item_product_code as product, line_item_usage_type, sum(line_item_usage_amount) as usage, line_item_unblended_rate as rate, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where (resource_tags_user_application like 'ordering' and line_item_usage_type like 'BoxUsage%') or line_item_line_item_type like 'SavingsPlan%' group by line_item_product_code, line_item_unblended_rate, line_item_usage_type order by cost desc 3. Celebrate We have completed our last Cost Optimization cycle for today. Where did we end up? How did we go? Look at the original graphs of usage and cost: Recall our efficiency numbers: Open up quicksight and refresh your browser, the new data will be automatically loaded: Lets look at our efficiency. Run the following query to return the total number of lines in our log file, which is our total demand: SELECT count(*) FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" Copy the results into a spreadsheet application and label it Total workload demand Run the following queries to get the total successful responses, and valid successful responses, record these also: SELECT count(*) FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where response like '200' and (request like '%index%' or request like '%image_file%') We will now get the successful and valid responses by hour with the query below. Note the use of date_parse to turn the string into an actual date we can work with: SELECT date(date_parse(logdate, '%d/%b/%Y')) as date, hour(date_parse(logtime, '%H:%i:%s')) as hour, count(*) as requests FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where response like '200' and (request like '%index%' or request like '%image_file%') group by date_parse(logdate, '%d/%b/%Y'), hour(date_parse(logtime, '%H:%i:%s')) order by date, hour Copy the results into the same spreadsheet. Now lets get the workload cost that corresponds to the demand. Select the costusage database, which should have a table costusagefiles_... Get the total cost for the workload with the following statement, and put it into the spreadsheet next to the demand: SELECT sum(line_item_unblended_cost) FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' Execute the following statement to get the cost by hour and put it into the spreadsheet: SELECT line_item_usage_start_date as date, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' group by line_item_usage_start_date order by date asc In the spreadsheet, calculate the efficiency by dividing the requests by the cost. This will give you the number of outcomes per dollar spent. Notice the difference when you compare total vs successful vs valid. Also notice any variation of efficiency throughout the day.","title":"Pricing Models"},{"location":"Cost/CostOpt_Workload/Step6.html#pricing-models","text":"","title":"Pricing Models"},{"location":"Cost/CostOpt_Workload/Step6.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/CostOpt_Workload/Step6.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/CostOpt_Workload/Step6.html#goals","text":"Implement pricing models for relevant components in this workload","title":"Goals"},{"location":"Cost/CostOpt_Workload/Step6.html#table-of-contents","text":"Implement pricing models For this exercise we start with the files that were used at the end of Step4, these should already be loaded if you just finished Step4: - Step4CUR.gz - Step4access_log.gz","title":"Table of Contents"},{"location":"Cost/CostOpt_Workload/Step6.html#1-implement-pricing-models","text":"Perform the following lab to perform a savings plan analysis on a set of accounts: Pricing Models SPs We will confirm the rates and prices we are paying for our resources before we apply pricing models, and also throughout the operation of our workload. If our workload no longer has pricing model discounts applied, it may mean your pricing model coverage is too low. Lets have a look at our instance costs, go into the Athena console & run the following query: select line_item_product_code as product, line_item_usage_type, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where (resource_tags_user_application like 'ordering' and line_item_usage_type like 'BoxUsage%') or line_item_line_item_type like 'SavingsPlan%' group by line_item_product_code, line_item_usage_type order by cost desc You can see the workload costs for the instances, which is $24.9956 : Now lets look at the rates and costs that make up that figure, run the following query: select line_item_product_code as product, line_item_usage_type, sum(line_item_usage_amount) as usage, line_item_unblended_rate as rate, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where (resource_tags_user_application like 'ordering' and line_item_usage_type like 'BoxUsage%') or line_item_line_item_type like 'SavingsPlan%' group by line_item_product_code, line_item_unblended_rate, line_item_usage_type order by cost desc We can see our main workload resources and rates: Work with your internal teams so they can understand your projected usage and business forecast. The appropriate discount models will then be applied to your workload.","title":"1. Implement Pricing Models"},{"location":"Cost/CostOpt_Workload/Step6.html#2-simulate-the-change-and-validate","text":"We will simulate the change of purchasing pricing models. Wait at this step until we tell you to proceed Lets check our costs for the workload after savings plans have been applied, run the following query: select line_item_product_code as product, line_item_usage_type, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where (resource_tags_user_application like 'ordering' and line_item_usage_type like 'BoxUsage%') or line_item_line_item_type like 'SavingsPlan%' group by line_item_product_code, line_item_usage_type order by cost desc You can see the total costs are now $21.986 : You can see the rates and prices by running the following query, also the savings plan commitments. Note the negation lines: select line_item_product_code as product, line_item_usage_type, sum(line_item_usage_amount) as usage, line_item_unblended_rate as rate, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where (resource_tags_user_application like 'ordering' and line_item_usage_type like 'BoxUsage%') or line_item_line_item_type like 'SavingsPlan%' group by line_item_product_code, line_item_unblended_rate, line_item_usage_type order by cost desc","title":"2. Simulate the change and validate"},{"location":"Cost/CostOpt_Workload/Step6.html#3-celebrate","text":"We have completed our last Cost Optimization cycle for today. Where did we end up? How did we go? Look at the original graphs of usage and cost: Recall our efficiency numbers: Open up quicksight and refresh your browser, the new data will be automatically loaded: Lets look at our efficiency. Run the following query to return the total number of lines in our log file, which is our total demand: SELECT count(*) FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" Copy the results into a spreadsheet application and label it Total workload demand Run the following queries to get the total successful responses, and valid successful responses, record these also: SELECT count(*) FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where response like '200' and (request like '%index%' or request like '%image_file%') We will now get the successful and valid responses by hour with the query below. Note the use of date_parse to turn the string into an actual date we can work with: SELECT date(date_parse(logdate, '%d/%b/%Y')) as date, hour(date_parse(logtime, '%H:%i:%s')) as hour, count(*) as requests FROM \"webserverlogs\".\"applogfiles_reinventworkshop\" where response like '200' and (request like '%index%' or request like '%image_file%') group by date_parse(logdate, '%d/%b/%Y'), hour(date_parse(logtime, '%H:%i:%s')) order by date, hour Copy the results into the same spreadsheet. Now lets get the workload cost that corresponds to the demand. Select the costusage database, which should have a table costusagefiles_... Get the total cost for the workload with the following statement, and put it into the spreadsheet next to the demand: SELECT sum(line_item_unblended_cost) FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' Execute the following statement to get the cost by hour and put it into the spreadsheet: SELECT line_item_usage_start_date as date, sum(line_item_unblended_cost) as cost FROM \"costusage\".\"costusagefiles_reinventworkshop\" where resource_tags_user_application like 'ordering' group by line_item_usage_start_date order by date asc In the spreadsheet, calculate the efficiency by dividing the requests by the cost. This will give you the number of outcomes per dollar spent. Notice the difference when you compare total vs successful vs valid. Also notice any variation of efficiency throughout the day.","title":"3. Celebrate"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/README.html","text":"Level 100: EC2 Right Sizing - Getting to know CloudWatch and AWS Resource Optimization http://wellarchitectedlabs.com Introduction This hands-on lab will give you an overview on Amazon CloudWatch and AWS Resource Optimization and how to prioritize your EC2 Right Sizing efforts. Goals Learn how to check metrics like CPU, Network and Disk usage on Amazon CloudWatch Enable and use the AWS Resource Optimization and get EC2 Right Sizing recommendations Learn how to filter AWS Resource Optimization report and focus only on the less complex high saving cases Prerequisites Root user access to the master account Enable AWS Resource Optimization at AWS Cost Explorer > Recommendations no additional cost. Permissions required Root user access to the master account NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Best Practice Checklist [ ] Launch Amazon CloudWatch and observe the CPU, Disk and Network consuption of your EC2 instances [ ] Enable and Launch AWS Resource Optimization, view how much your organization can save with Idle and Downsizing EC2 instances [ ] Filter the AWS Resource Optimization recommendations by region, OS and Account [ ] Learn how to prioritize these recommendations focusing on the low complexity high saving cases License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: EC2 Right Sizing - Getting to know CloudWatch and AWS Resource Optimization"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/README.html#level-100-ec2-right-sizing-getting-to-know-cloudwatch-and-aws-resource-optimization","text":"http://wellarchitectedlabs.com","title":"Level 100: EC2 Right Sizing - Getting to know CloudWatch and AWS Resource Optimization"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/README.html#introduction","text":"This hands-on lab will give you an overview on Amazon CloudWatch and AWS Resource Optimization and how to prioritize your EC2 Right Sizing efforts.","title":"Introduction"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/README.html#goals","text":"Learn how to check metrics like CPU, Network and Disk usage on Amazon CloudWatch Enable and use the AWS Resource Optimization and get EC2 Right Sizing recommendations Learn how to filter AWS Resource Optimization report and focus only on the less complex high saving cases","title":"Goals"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/README.html#prerequisites","text":"Root user access to the master account Enable AWS Resource Optimization at AWS Cost Explorer > Recommendations no additional cost.","title":"Prerequisites"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/README.html#permissions-required","text":"Root user access to the master account NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/README.html#best-practice-checklist","text":"[ ] Launch Amazon CloudWatch and observe the CPU, Disk and Network consuption of your EC2 instances [ ] Enable and Launch AWS Resource Optimization, view how much your organization can save with Idle and Downsizing EC2 instances [ ] Filter the AWS Resource Optimization recommendations by region, OS and Account [ ] Learn how to prioritize these recommendations focusing on the low complexity high saving cases","title":"Best Practice Checklist"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/Lab_Guide.html","text":"Level 100: EC2 Right Sizing: Lab Guide Authors Arthur Basbaum, AWS Cloud Economics Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Getting to know Amazon CloudWatch Using Amazon EC2 Resource Optimization Recommendations Download the Amazon EC2 Resource Optimization CSV File and sort it to find quick wins Actionig on the recommendations EC2 Right Sizing Best Practices EC2 Right Sizing Reference Material NOTE: In order to run this lab you will need to have at least one EC2 instance running and have AWS Cost Explorer and Amazon EC2 Resource Optimization enabled. 1. Getting to know Amazon Cloudwatch The first step to perform right sizing is to monitor and analyze your current use of services to gain insight into instance performance and usage patterns. To gather sufficient data, observe performance over at least a two-week period (ideally, over a one-month period) to capture the workload and business peak. The most common metrics that define instance performance are vCPU utilization, memory utilization, network utilization, and disk use. Log into your AWS console, go to the Amazon CloudWatch service page: Select EC2 under the Service Dashboard : Observe the Service Dashboard and all of its different metrics, but focus on CPU Utilization and Network In and Out : Select one of the EC2 resources by clicking on the little color icon to the left of the resource-id name: Deselect the EC2 resource and now modify the time range on the top right, click custom and select the last 2 weeks : Navigate to the CPU Utilization Average widget, click the three dots and launch the View in metrics page. Using the Graphed metrics section try to answer the following questions: a. What is the instance with the highest CPU Average? b. What is the instance with the highest CPU Max? c. What is the instance with the lowest CPU Min? 2. Using Amazon EC2 Resource Optimization Recommendations NOTE : In order to complete this step you need to have Amazon EC2 Resource Optimization enabled, you can do that by going to AWS Cost Explorer, Recommendations (left bar) section. Amazon EC2 Resource Optimization offers right sizing recommendations in AWS Cost Explorer without any additional cost. These recommendations identify likely idle and underutilized instances across your accounts, regions and tags. To generate these recommendations, AWS analyzes your historical EC2 resource usage (using Amazon CloudWatch) and your existing reservation footprint to identify opportunities for cost savings (e.g., by terminating idle instances or downsizing active instances to lower-cost options within the same family/generation). Navigate to the AWS Cost Explorer page Select Recommendations in the left menu bar Click on the View All link associated with the Amazon EC2 Resource Optimization Recommendations section. In case you haven\u2019t enabled the Amazon EC2 Resource Optimization please do so (no additional cost), it may take up to 24 hours in order to generate your first recommendations. Only regular or payer accounts can enable it and by default both linked and payer accounts will be able to access their rightsizing recommendations unless the payer account specifically prohibits it on the settings page (top right). To improve the quality of recommendations, AWS might use other utilization metrics that you might be collecting, such as disk or memory utilization. All resource utilization metrics are anonymized and aggregated before AWS uses them for model training. If you would like to opt out of this experience and request your metrics not be stored and used for model improvement, please submit an AWS support ticket . For more information, see AWS Service Terms . Assuming you had enabled the Amazon EC2 Resource Optimization Recommendations, you will be presented with a screen that provides recommendations (if any exists): Optimization opportunities \u2013 The number of recommendations available based on your resources and usage Estimated monthly savings \u2013 The sum of the projected monthly savings associated with each of the recommendations provided Estimated savings (%) \u2013 The available savings relative to the direct Amazon EC2 costs (On-Demand) associated with the instances in the recommendation list You can also filter your recommendations by the type of action (Idle and Underutilized), Linked Account, Region and Tag. Click on view next to a recommendation, to view the details: How are the potential savings calculated? AWS will first examine the instance running during the last 14 days to identify if it was partially or fully covered by an RI or running On-Demand. Another factor is whether the RI is instance size-flexible. The cost to run the instance is calculated based on the On-Demand hours and the hourly rate for the instance type. For each recommendation, AWS calculates the cost to operate a new instance. AWS assumes that an instance size-flexible Reserved Instance will cover the new instance in the same way as the previous instance. Savings are calculated based on the number of On-Demand running hours and the difference in On-Demand rates. If the Reserved Instance isn't instance size-flexible, the savings calculation is based on whether the instance hours during the last 14 days are operated as On-Demand. AWS will only provide recommendations with estimated savings greater than or equal to $0. Amazon EC2 Resource Optimization recommendations already excludes Spot usage and takes into consideration the existing Reserved Instances and Savings Plan footprint to provide recommendations. There are two types of recommended actions: Terminate if the instance is considered idle ( max CPU utilization is at or below 1% ) or Downsize if the instance is underutilized ( max CPU utilization is between 1% and 40% ). You can enable Amazon CloudWatch to report memory utilization and improve the recommendation accuracy. Please check the 200 level EC2 Right Sizing lab for more information on how to enable memory utilization metrics. Finally, Amazon EC2 Resource Optimization recommendations will only ever recommend up to three, same-family instances as target instances for downsizing. 3. Download the Amazon EC2 Resource Optimization CSV File and sort it to find quick wins Download the Amazon EC2 Resource Optimization report: If you don\u2019t have any Amazon EC2 Resource Optimization recommendation use the file below as a reference. Sample Amazon EC2 Resource Optimization file (.csv) First let\u2019s exclude instances that are too small or were only running for a few hours from the analysis. By doing so, we minimize the time required to perform rightsizing modifications that would otherwise result in minimal savings. NOTE : Depending on how many cost allocation tags you have enabled on your account the columns may differ from the example, that said try to match the formulas using the screenshots below and the default column names. Insert a new column to the right of the Recommended Action column. The first row will be the label for the column \u201cTooSmall\u201d. In each row below the label, paste the following formula: =IF(Q2<25,1,0) Where Column Q = Recommended Instance Type 1 Estimated Savings That formula will flag all EC2 instances with a \u201c1\u201d any instance that will fail to deliver more than $25/month in savings (or $300/year). Feel free to adjust the threshold for your organization own savings expectation. If you prefer to perform the analysis against instance families instead of potential savings you can use the following formula to exclude smaller instances from the recommendations as well. =IF(N2=\"Modify\",IF(SUMPRODUCT(--(NOT(ISERR(SEARCH({\"nano\",\"micro\",\"small\",\"medium\"},D2)))))>0,\"1\",\"0\"),\"0\") Where Column N = Recommended Action and Column D = Instance Type Next, let\u2019s flag EC2 instances that belong to previous generations (C4, M3, etc), if you are investing engineer time on right sizing let's make sure you are also leveraging the newest technology available. Newer EC2 generations have a superior performance increasing the changes of success for the right sizing exercise, they also generally cost less than previous generations providing a higher cost vs benefit. Insert a new column Old Gen to the right of the Instance Type field, and paste the following formula: =IF(SUMPRODUCT(--(NOT(ISERR(SEARCH({\"c4\",\"c3\",\"c1\",\"m4\",\"m3\",\"m2\",\"m1\",\"r3\",\"r4\",\"i2\",\"cr1\",\"hs1\",\"g2\"},D2)))))>0,\"1\",\"0\") Column D = Instance Type Now let\u2019s sort the recommendations by low complexity and higher savings: Minimum Effort: Set the minimum savings required First we want to only focus on savings that are worth our effort, we will define this as $100. Apply a number filter on Recommended Instance Type 1 Estimated Savings that is Greater than 100 Group 1: Idle EC2 resources Filter the data on Recommended Action = \"Terminate\" Sort the data by Recommended Instance Type 1 Estimated Savings = Largest to smallest Start filtering the idle resources or instances where CPU utilization <1%, it is likely these instances were launched and forgotten so the potential savings here may represent the entire On Demand cost. The resulting filtered list should be where you start right sizing discussions with application owners; perform an investigation to understand why these instance were launched and validate their usage with the resource owner. If possible, terminate them. If you are using the Right Sizing CSV file provided in this lab exercise, you will notice that we filtered down from an original 2,534 recommendations to 16 and identified $3,458 per month in potential savings . Group 2: Previous generation instances Filter the data on Recommended Actions = \u201cModify\u201d AND OldGen = \u201c1\u201d AND TooSmall = \u201c0\u201d Filter the data on Recommended Instance Type 1 Projected CPU < 40% Sort the data by Recommended Instance Type 1 Estimated Savings = Largest to smallest This will focus on the underutilized resources (<40% CPU) that belongs to previous generations and can either be downsized within the same family (column P below) or modernized to the newest generation. Moving to a modern generation may require additional testing hours compared to instances identified on Group 1, but depending on the case it can maximize savings and performance. Linux vs new gen Windows vs new gen Linux vs new gen Windows vs new gen c3.large $0.105/hr up 19% $0.188/hr up 5% m3.large $0.133/hr up 27% $0.259/hr up 27% c4.large $0.100/hr up 15% $0.192/hr up 7% m4.large $0.100/hr up 4% $0.192/hr up 2% c5.large $0.985/hr 0% $0.177/hr 0% m5.large $0.096/hr 0% $0.188/hr 0% prices are from US-Virginia (Nov 2019) If you are using the Right Sizing CSV file provided in this lab exercise, you will notice that we filtered down from originally 2,534 recommendations to 22 with $6,362 per month in potential savings . Group 3: Current generation instances Filter the data on Recommended Actions = \u201cModify\u201d AND OldGen = \u201c0\u201d AND TooSmall = \u201c0\u201d The filter on Recommended Instance Type 1 Projected CPU < 40% should still be in place. Sort the data by Recommended Instance Type 1 Estimated Savings = Largest to smallest This will select underutilized resources from the current, most modern generation. We recommend sorting them by potential savings to make sure you are prioritizing the instances that will provide larger savings first. Also, do not forget to check the other recommended instance types (columns U to AD); Amazon EC2 Resource Optimization will recommend up to 3 instances for each resource moving from a more conservative recommendation (the first recommendation) to a more aggressive and higher savings recommendation (second and third recommendations). If you are using the Right Sizing CSV file provided in this lab exercise, you will notice that we filtered down from originally 2,534 recommendations to 22 with $4,879.56 per month in potential savings . 4. Action the recommendations During this lab exercise, we learned how to prioritize the right sizing recommendations with the goal of identifying low complexity and high savings recommendations. We initially started with 2,534 recommendations with a potential saving of $86,627 but we managed to identify the top 60 cases with lowest complexity that together add up to $14,699.56 of the overall potential saving. Group 1 (Idle) and Group 2 (Previous Generation) are the less complex cases where you may want to start the right sizing exercises for your organization. As you gain more confidence and learn how to develop a regular process for right sizing, your organization will be able to rapidly act on Group 3 (Current/modern generation) and other cases. 5. Amazon EC2 Right Sizing Best Practices Start simple: idle resources, non-critical development/QA and previous generation instances will require less testing hours and provide quick wins (The Amazon EC2 Launch time statistics can be used to identify instances that have been running longer than others and is a good statistic to sort your Amazon EC2 instances by). Right Size before performing a migration: If you skip right sizing to save time, your migration speed might increase, but you will end up with higher cloud infrastructure spend for a potentially longer period of time. Instead, leverage the test and QA cycles during a migration exercise to test several instance types and families. Also, take that opportunity to test different sizes and burstable instances like the \u201ct\u201d family. The best right sizing starts on day 1: As you perform right sizing analysis, and ultimately rightsize resources, ensure any learnings are being shared across your organization and influencing the design of new workloads and upcoming migrations. Measure Twice, Cut Once: Test, then test some more: The last thing you want is for a new resource type to be uncapable of handling load, or functioning incorrectly. Test once and perform multiple right sizing: Aggregate instances per autoscaling group and tags to scale right sizing activities. Combine Reserved Instance or Savings Plans strategy with Right Sizing to maximize savings: For Standard RIs and EC2 Instance SP: Perform your pricing model purchases after rightsizing and for Convertible RIs, exchange them after rightsizing. Compute Savings plan will automatically adjust the commitment for the new environment. Ignore burstable instance families (T types): These families are designed to typically run at low CPU percentages for significant periods of time and shouldn\u2019t be part of the instance types being analyzed for rightsizing. 7. Tear down No tear down is required for this lab. 8. Rate this lab","title":"Level 100: EC2 Right Sizing: Lab Guide"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/Lab_Guide.html#level-100-ec2-right-sizing-lab-guide","text":"","title":"Level 100: EC2 Right Sizing: Lab Guide"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/Lab_Guide.html#authors","text":"Arthur Basbaum, AWS Cloud Economics","title":"Authors"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/Lab_Guide.html#table-of-contents","text":"Getting to know Amazon CloudWatch Using Amazon EC2 Resource Optimization Recommendations Download the Amazon EC2 Resource Optimization CSV File and sort it to find quick wins Actionig on the recommendations EC2 Right Sizing Best Practices EC2 Right Sizing Reference Material NOTE: In order to run this lab you will need to have at least one EC2 instance running and have AWS Cost Explorer and Amazon EC2 Resource Optimization enabled.","title":"Table of Contents"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/Lab_Guide.html#1-getting-to-know-amazon-cloudwatch","text":"The first step to perform right sizing is to monitor and analyze your current use of services to gain insight into instance performance and usage patterns. To gather sufficient data, observe performance over at least a two-week period (ideally, over a one-month period) to capture the workload and business peak. The most common metrics that define instance performance are vCPU utilization, memory utilization, network utilization, and disk use. Log into your AWS console, go to the Amazon CloudWatch service page: Select EC2 under the Service Dashboard : Observe the Service Dashboard and all of its different metrics, but focus on CPU Utilization and Network In and Out : Select one of the EC2 resources by clicking on the little color icon to the left of the resource-id name: Deselect the EC2 resource and now modify the time range on the top right, click custom and select the last 2 weeks : Navigate to the CPU Utilization Average widget, click the three dots and launch the View in metrics page. Using the Graphed metrics section try to answer the following questions: a. What is the instance with the highest CPU Average? b. What is the instance with the highest CPU Max? c. What is the instance with the lowest CPU Min?","title":"1. Getting to know Amazon Cloudwatch"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/Lab_Guide.html#2-using-amazon-ec2-resource-optimization-recommendations","text":"NOTE : In order to complete this step you need to have Amazon EC2 Resource Optimization enabled, you can do that by going to AWS Cost Explorer, Recommendations (left bar) section. Amazon EC2 Resource Optimization offers right sizing recommendations in AWS Cost Explorer without any additional cost. These recommendations identify likely idle and underutilized instances across your accounts, regions and tags. To generate these recommendations, AWS analyzes your historical EC2 resource usage (using Amazon CloudWatch) and your existing reservation footprint to identify opportunities for cost savings (e.g., by terminating idle instances or downsizing active instances to lower-cost options within the same family/generation). Navigate to the AWS Cost Explorer page Select Recommendations in the left menu bar Click on the View All link associated with the Amazon EC2 Resource Optimization Recommendations section. In case you haven\u2019t enabled the Amazon EC2 Resource Optimization please do so (no additional cost), it may take up to 24 hours in order to generate your first recommendations. Only regular or payer accounts can enable it and by default both linked and payer accounts will be able to access their rightsizing recommendations unless the payer account specifically prohibits it on the settings page (top right). To improve the quality of recommendations, AWS might use other utilization metrics that you might be collecting, such as disk or memory utilization. All resource utilization metrics are anonymized and aggregated before AWS uses them for model training. If you would like to opt out of this experience and request your metrics not be stored and used for model improvement, please submit an AWS support ticket . For more information, see AWS Service Terms . Assuming you had enabled the Amazon EC2 Resource Optimization Recommendations, you will be presented with a screen that provides recommendations (if any exists): Optimization opportunities \u2013 The number of recommendations available based on your resources and usage Estimated monthly savings \u2013 The sum of the projected monthly savings associated with each of the recommendations provided Estimated savings (%) \u2013 The available savings relative to the direct Amazon EC2 costs (On-Demand) associated with the instances in the recommendation list You can also filter your recommendations by the type of action (Idle and Underutilized), Linked Account, Region and Tag. Click on view next to a recommendation, to view the details: How are the potential savings calculated? AWS will first examine the instance running during the last 14 days to identify if it was partially or fully covered by an RI or running On-Demand. Another factor is whether the RI is instance size-flexible. The cost to run the instance is calculated based on the On-Demand hours and the hourly rate for the instance type. For each recommendation, AWS calculates the cost to operate a new instance. AWS assumes that an instance size-flexible Reserved Instance will cover the new instance in the same way as the previous instance. Savings are calculated based on the number of On-Demand running hours and the difference in On-Demand rates. If the Reserved Instance isn't instance size-flexible, the savings calculation is based on whether the instance hours during the last 14 days are operated as On-Demand. AWS will only provide recommendations with estimated savings greater than or equal to $0. Amazon EC2 Resource Optimization recommendations already excludes Spot usage and takes into consideration the existing Reserved Instances and Savings Plan footprint to provide recommendations. There are two types of recommended actions: Terminate if the instance is considered idle ( max CPU utilization is at or below 1% ) or Downsize if the instance is underutilized ( max CPU utilization is between 1% and 40% ). You can enable Amazon CloudWatch to report memory utilization and improve the recommendation accuracy. Please check the 200 level EC2 Right Sizing lab for more information on how to enable memory utilization metrics. Finally, Amazon EC2 Resource Optimization recommendations will only ever recommend up to three, same-family instances as target instances for downsizing.","title":"2. Using Amazon EC2 Resource Optimization Recommendations"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/Lab_Guide.html#3-download-the-amazon-ec2-resource-optimization-csv-file-and-sort-it-to-find-quick-wins","text":"Download the Amazon EC2 Resource Optimization report: If you don\u2019t have any Amazon EC2 Resource Optimization recommendation use the file below as a reference. Sample Amazon EC2 Resource Optimization file (.csv) First let\u2019s exclude instances that are too small or were only running for a few hours from the analysis. By doing so, we minimize the time required to perform rightsizing modifications that would otherwise result in minimal savings. NOTE : Depending on how many cost allocation tags you have enabled on your account the columns may differ from the example, that said try to match the formulas using the screenshots below and the default column names. Insert a new column to the right of the Recommended Action column. The first row will be the label for the column \u201cTooSmall\u201d. In each row below the label, paste the following formula: =IF(Q2<25,1,0) Where Column Q = Recommended Instance Type 1 Estimated Savings That formula will flag all EC2 instances with a \u201c1\u201d any instance that will fail to deliver more than $25/month in savings (or $300/year). Feel free to adjust the threshold for your organization own savings expectation. If you prefer to perform the analysis against instance families instead of potential savings you can use the following formula to exclude smaller instances from the recommendations as well. =IF(N2=\"Modify\",IF(SUMPRODUCT(--(NOT(ISERR(SEARCH({\"nano\",\"micro\",\"small\",\"medium\"},D2)))))>0,\"1\",\"0\"),\"0\") Where Column N = Recommended Action and Column D = Instance Type Next, let\u2019s flag EC2 instances that belong to previous generations (C4, M3, etc), if you are investing engineer time on right sizing let's make sure you are also leveraging the newest technology available. Newer EC2 generations have a superior performance increasing the changes of success for the right sizing exercise, they also generally cost less than previous generations providing a higher cost vs benefit. Insert a new column Old Gen to the right of the Instance Type field, and paste the following formula: =IF(SUMPRODUCT(--(NOT(ISERR(SEARCH({\"c4\",\"c3\",\"c1\",\"m4\",\"m3\",\"m2\",\"m1\",\"r3\",\"r4\",\"i2\",\"cr1\",\"hs1\",\"g2\"},D2)))))>0,\"1\",\"0\") Column D = Instance Type Now let\u2019s sort the recommendations by low complexity and higher savings: Minimum Effort: Set the minimum savings required First we want to only focus on savings that are worth our effort, we will define this as $100. Apply a number filter on Recommended Instance Type 1 Estimated Savings that is Greater than 100 Group 1: Idle EC2 resources Filter the data on Recommended Action = \"Terminate\" Sort the data by Recommended Instance Type 1 Estimated Savings = Largest to smallest Start filtering the idle resources or instances where CPU utilization <1%, it is likely these instances were launched and forgotten so the potential savings here may represent the entire On Demand cost. The resulting filtered list should be where you start right sizing discussions with application owners; perform an investigation to understand why these instance were launched and validate their usage with the resource owner. If possible, terminate them. If you are using the Right Sizing CSV file provided in this lab exercise, you will notice that we filtered down from an original 2,534 recommendations to 16 and identified $3,458 per month in potential savings . Group 2: Previous generation instances Filter the data on Recommended Actions = \u201cModify\u201d AND OldGen = \u201c1\u201d AND TooSmall = \u201c0\u201d Filter the data on Recommended Instance Type 1 Projected CPU < 40% Sort the data by Recommended Instance Type 1 Estimated Savings = Largest to smallest This will focus on the underutilized resources (<40% CPU) that belongs to previous generations and can either be downsized within the same family (column P below) or modernized to the newest generation. Moving to a modern generation may require additional testing hours compared to instances identified on Group 1, but depending on the case it can maximize savings and performance. Linux vs new gen Windows vs new gen Linux vs new gen Windows vs new gen c3.large $0.105/hr up 19% $0.188/hr up 5% m3.large $0.133/hr up 27% $0.259/hr up 27% c4.large $0.100/hr up 15% $0.192/hr up 7% m4.large $0.100/hr up 4% $0.192/hr up 2% c5.large $0.985/hr 0% $0.177/hr 0% m5.large $0.096/hr 0% $0.188/hr 0% prices are from US-Virginia (Nov 2019) If you are using the Right Sizing CSV file provided in this lab exercise, you will notice that we filtered down from originally 2,534 recommendations to 22 with $6,362 per month in potential savings . Group 3: Current generation instances Filter the data on Recommended Actions = \u201cModify\u201d AND OldGen = \u201c0\u201d AND TooSmall = \u201c0\u201d The filter on Recommended Instance Type 1 Projected CPU < 40% should still be in place. Sort the data by Recommended Instance Type 1 Estimated Savings = Largest to smallest This will select underutilized resources from the current, most modern generation. We recommend sorting them by potential savings to make sure you are prioritizing the instances that will provide larger savings first. Also, do not forget to check the other recommended instance types (columns U to AD); Amazon EC2 Resource Optimization will recommend up to 3 instances for each resource moving from a more conservative recommendation (the first recommendation) to a more aggressive and higher savings recommendation (second and third recommendations). If you are using the Right Sizing CSV file provided in this lab exercise, you will notice that we filtered down from originally 2,534 recommendations to 22 with $4,879.56 per month in potential savings .","title":"3. Download the Amazon EC2 Resource Optimization CSV File and sort it to find quick wins "},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/Lab_Guide.html#4-action-the-recommendations","text":"During this lab exercise, we learned how to prioritize the right sizing recommendations with the goal of identifying low complexity and high savings recommendations. We initially started with 2,534 recommendations with a potential saving of $86,627 but we managed to identify the top 60 cases with lowest complexity that together add up to $14,699.56 of the overall potential saving. Group 1 (Idle) and Group 2 (Previous Generation) are the less complex cases where you may want to start the right sizing exercises for your organization. As you gain more confidence and learn how to develop a regular process for right sizing, your organization will be able to rapidly act on Group 3 (Current/modern generation) and other cases.","title":"4. Action the recommendations"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/Lab_Guide.html#5-amazon-ec2-right-sizing-best-practices","text":"Start simple: idle resources, non-critical development/QA and previous generation instances will require less testing hours and provide quick wins (The Amazon EC2 Launch time statistics can be used to identify instances that have been running longer than others and is a good statistic to sort your Amazon EC2 instances by). Right Size before performing a migration: If you skip right sizing to save time, your migration speed might increase, but you will end up with higher cloud infrastructure spend for a potentially longer period of time. Instead, leverage the test and QA cycles during a migration exercise to test several instance types and families. Also, take that opportunity to test different sizes and burstable instances like the \u201ct\u201d family. The best right sizing starts on day 1: As you perform right sizing analysis, and ultimately rightsize resources, ensure any learnings are being shared across your organization and influencing the design of new workloads and upcoming migrations. Measure Twice, Cut Once: Test, then test some more: The last thing you want is for a new resource type to be uncapable of handling load, or functioning incorrectly. Test once and perform multiple right sizing: Aggregate instances per autoscaling group and tags to scale right sizing activities. Combine Reserved Instance or Savings Plans strategy with Right Sizing to maximize savings: For Standard RIs and EC2 Instance SP: Perform your pricing model purchases after rightsizing and for Convertible RIs, exchange them after rightsizing. Compute Savings plan will automatically adjust the commitment for the new environment. Ignore burstable instance families (T types): These families are designed to typically run at low CPU percentages for significant periods of time and shouldn\u2019t be part of the instance types being analyzed for rightsizing.","title":"5. Amazon EC2 Right Sizing Best Practices"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/Lab_Guide.html#7-tear-down","text":"No tear down is required for this lab.","title":"7. Tear down"},{"location":"Cost/Cost_Effective_Resources/100_AWS_Resource_Optimization/Lab_Guide.html#8-rate-this-lab","text":"","title":"8. Rate this lab"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/README.html","text":"Level 200: EC2 Right Sizing - Collecting Memory utilization from EC2 Instances http://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to install the CloudWatch agent to collect memory utilization (% GB consumption) and analyze how that new datapoint can help during EC2 right sizing exercises with the AWS Resource Optimization tool. Goals Learn how to check metrics like CPU, Network and Disk usage on Amazon CloudWatch Learn how to install and collect Memory data through a custom metric at Amazon CloudWatch Enable AWS Resource Optimization and observe how the recommendations are impacted by this new datapoint (Memory) Prerequisites Root user access to the master account Enable AWS Resource Optimization at AWS Cost Explorer > Recommendations no additional cost. Permissions required Root user access to the master account NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Best Practice Checklist [ ] Launch Amazon CloudWatch and observe the average CPU, Disk and Network consuption of your EC2 instances [ ] Manually install CloudWatch Agent on an EC2 instance to track memory utilization [ ] Observe the impact on the AWS Resource Optimization when you have additional datapoints (Memory utilization) License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: EC2 Right Sizing - Collecting Memory utilization from EC2 Instances"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/README.html#level-200-ec2-right-sizing-collecting-memory-utilization-from-ec2-instances","text":"http://wellarchitectedlabs.com","title":"Level 200: EC2 Right Sizing - Collecting Memory utilization from EC2 Instances"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/README.html#introduction","text":"This hands-on lab will guide you through the steps to install the CloudWatch agent to collect memory utilization (% GB consumption) and analyze how that new datapoint can help during EC2 right sizing exercises with the AWS Resource Optimization tool.","title":"Introduction"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/README.html#goals","text":"Learn how to check metrics like CPU, Network and Disk usage on Amazon CloudWatch Learn how to install and collect Memory data through a custom metric at Amazon CloudWatch Enable AWS Resource Optimization and observe how the recommendations are impacted by this new datapoint (Memory)","title":"Goals"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/README.html#prerequisites","text":"Root user access to the master account Enable AWS Resource Optimization at AWS Cost Explorer > Recommendations no additional cost.","title":"Prerequisites"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/README.html#permissions-required","text":"Root user access to the master account NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/README.html#best-practice-checklist","text":"[ ] Launch Amazon CloudWatch and observe the average CPU, Disk and Network consuption of your EC2 instances [ ] Manually install CloudWatch Agent on an EC2 instance to track memory utilization [ ] Observe the impact on the AWS Resource Optimization when you have additional datapoints (Memory utilization)","title":"Best Practice Checklist"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/Lab_Guide.html","text":"Level 200: EC2 Right Sizing: Lab Guide Authors Jeff Kassel, AWS Technical Account Manager Arthur Basbaum, AWS Cloud Economics Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Getting to know Amazon CloudWatch Create an IAM Role to use with Amazon CloudWatch Agent Attach CloudWatch IAM role to selected EC2 Instances Cloudwatch Agent Manual Install Updated Amazon EC2 Resource Optimization recommendations EC2 Right Sizing Best Practices EC2 Right Sizing Reference Material NOTE: In order to run this lab you will need to have at least one EC2 instance running and have AWS Cost Explorer and Amazon EC2 Resource Optimization enabled. During this lab we will create a custom metric at Amazon CloudWatch and install an agent in one EC2 instance to start collecting Memory utilization and improve the recommendation accuracy of the Amazon EC2 Resource Optimization report. Be aware that custom metrics are not part of the Amazon CloudWatch free tier usage so additional costs will incur at your bill . For more information read the Amazon CloudWatch pricing page. All custom metrics charges are prorated by the hour and metered only when you send metrics to Amazon CloudWatch. Each custom metrics costs $0.30 per metric/month for the first 10,000 metrics and can go down to $0.02 per metric/month at the lowest priced tier ( US Virginia prices from November 2019 ). 1. Getting to know Amazon Cloudwatch The first step to perform right sizing is to monitor and analyze your current use of services to gain insight into instance performance and usage patterns. To gather sufficient data, observe performance over at least a two-week period (ideally, over a one-month period) to capture the workload and business peak. The most common metrics that define instance performance are vCPU utilization, memory utilization, network utilization, and disk use. Log into your AWS console, go to the Amazon CloudWatch service page: Select EC2 under the Service Dashboard : Observe the Service Dashboard and all of its different metrics, but focus on CPU Utilization and Network In and Out : Select one of the EC2 resources by clicking on the little color icon to the left of the resource-id name: Deselect the EC2 resource and now modify the time range on the top right, click custom and select the last 2 weeks : Navigate to the CPU Utilization Average widget and launch the View Metrics detailed page. Using the Graphed metrics session try to answer the following questions: a) What is the instance with the lowest CPU Average? b) What is the instance with the lowest CPU Max? c) What is the instance with the lowest CPU Min? 2. Create an IAM Role to use with Amazon CloudWatch Agent Access to AWS resources requires permissions. You will now create an IAM role to grant permissions that the agent needs to write metrics to CloudWatch. Amazon created two new default policies called CloudWatchAgentServerPolicy and CloudWatchAgentAdminPolicy only for that purpose. To create the IAM role first you will need to sign in to the AWS Management Console and open the IAM console In the navigation pane on the left, choose Roles and then Create role . Under Choose the service that will use this role , choose EC2 Allows EC2 instances to call AWS services on your behalf. Choose Next: Permissions . In the list of policies, select the check box next to CloudWatchAgentServerPolicy . If necessary, use the search box to find the policy, click Next: Tags : Add tags (optional) for this policy, click Next: Review . Confirm that CloudWatchAgentServerPolicy appears next to Policies . In Role name, enter a name for the role, such as CloudWatchAgentServerRole . Optionally give it a description. Then click Create role . The role is now created. 3. Attach CloudWatch IAM role to selected EC2 Instances We are now going to attach the IAM Role created on the previous step in one of our EC2 Instances, to do that let's go to the Amazon EC2 Dashboard . On the left bar, click on Instances . Click on Launch Instance and select Linux 2 AMI (HVM) and t2.micro (free tier eligible) on the following screens. Click Review and launch : Click Launch Select Proceed without a key pair Complete this step by launching this testing t2.micro EC2 instance. In order to install the CloudWatch agent We will need to access this instance using the browser-based SSH native AWS connection tool so make sure that the port 22 is not blocked in the Security Group attached to this instance. After the instance is launched and runnning, select the instance you want to start collecting Memory data by going to Actions on the top bar, select Instance Settings>>Attach/Replace IAM Role . Look for the created IAM role CloudWatchAgentServerRole under the IAM role box, select it and apply. Confirm that the CloudWatchAgentServerRole was sucessfully attached to your EC2 instance Validade if the IAM role CloudWatchAgentServerRole is attached to the desired instance 4. Cloudwatch Agent Manual Install We are now going to manually install the CloudWatch agent to start collecting memory data, to start let's go back to the Amazon EC2 Dashboard . On the left bar, click on Instances and select the EC2 Instance with the CloudWatchAgentServerRole IAM role. Connect into the EC2 Instance using the browser-based SSH connection tool . Download the Amazon Cloudwatch agent package, the instructions below are for Amazon Linux, for other OS please check here wget https://s3.amazonaws.com/amazoncloudwatch-agent/linux/amd64/latest/AmazonCloudWatchAgent.zip Unzip and Install the package unzip AmazonCloudWatchAgent.zip sudo ./install.sh Configure the AmazonCloudWatchAgent profile Before running the CloudWatch agent on any servers, you must create a CloudWatch agent configuration file, which is a JSON file that specifies the metrics and logs that the agent is to collect, including custom metrics. You can create it by using the wizard or by writting it yourself from scratch. Any time you change the agent configuration file, you must then restart the agent to have the changes take effect. The wizard can autodetect the credentials and AWS Region to use if you have the AWS credentials and configuration files in place. For more information about these files, see Configuration and Credential Files in the AWS Systems Manager User Guide and the AWS documentation page . For now, let's start the CloudWatch agent configuration file wizard executing the command below at the selected EC2 instance. sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard For this lab we want to keep the following structure: CloudWatch Agent Configutation File Wizard Parameter On which OS are you planning to use the agent? 1. Linux Are you using EC2 or On-Premises hosts? 1. EC2 Which user are you planning to run the agent? 2. cwagent Do you want to turn on StatsD daemon? 2. No Do you want to monitor metrics from CollectD? 2. No Do you want to monitor any host metrics? 1. Yes Do you want to monitor cpu metrics per core? 2. No Do you want to add ec2 dimensions? 1. Yes Would you like to collect your metrics at high resolution? 4. 60s Which default metrics config do you want? 1. Basic Are you satisfied with the above config? 1. Yes Do you have any existing CloudWatch Log Agent? 2. No Do you want to monitor any log files? 2. No Do you want to store the config in the SSM parameter store? 2. No The CloudWatch Agent config file should look like the following: { \"agent\": { \"metrics_collection_interval\": 60, \"run_as_user\": \"cwagent\" }, \"metrics\": { \"append_dimensions\": { \"AutoScalingGroupName\": \"${aws:AutoScalingGroupName}\", \"ImageId\": \"${aws:ImageId}\", \"InstanceId\": \"${aws:InstanceId}\", \"InstanceType\": \"${aws:InstanceType}\" }, \"metrics_collected\": { \"disk\": { \"measurement\": [ \"used_percent\" ], \"metrics_collection_interval\": 60, \"resources\": [ \"*\" ] }, \"mem\": { \"measurement\": [ \"mem_used_percent\" ], \"metrics_collection_interval\": 60 } } } } Start the CloudWatch Agent sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json -s It may take up to 5 minutes for the metrics to become available, go back to the Amazon CloudWatch console page, under the Metrics session to validate that you are getting Memory information. Click CWAgent : Click ImageID,InstanceID,InstanceType : Select the Instance from the list below: You have now completed the CloudWatch agent installation and will be able to monitor on Amazon CloudWatch the memory utilization of that instance. [BONUS] The next step is not mandatory to complete this lab. If you have a lot of instances manually installing the CloudWatch agent in each of them is not a scalable option, instead consider using a pre-configured AWS CloudFormation template to automatically install the CloudWatch agent by default on all your stack. As an example on how to do that check the following steps: Right-click and save link as: here to download the AWS Cloudformation template Go to the AWS CloudFormation console Click to Create Stack and select Upload a template file and point to the downloaded file Enter a Stack Name and select a KeyName Enter the tag Key: Event | Value: myStackforWACostLab Click Next and Create stack 5. Updated Amazon EC2 Resource Optimization recommendations NOTE : In order to complete this step you need to have Amazon EC2 Resource Optimization enabled, you can do that going to the AWS Cost Explorer, Recommendations (left bar) section. Important: If you have just installed the CloudWatch agent at your instances it may take a couple of days for Amazon EC2 Resource Optimization to start to provide updated recommendations, so don't worry if you don't see the memory data during the first checks. Now that we have Memory data as a custom metric in CloudWatch let's check how that affects the Amazon EC2 Resource Optimization recommendations. Amazon EC2 Resource Optimization offers right sizing recommendations in the AWS Cost Explorer without any additional cost. These recommendations identify likely idle and underutilized instances across your accounts, regions and tags. To generate these recommendations, AWS analyzes your historical EC2 resource usage (last 14 days using Amazon CloudWatch) and your existing reservation footprint to identify opportunities for cost savings. There are two types of recommended actions: Terminate if the instance is considered idle ( max CPU utilization is at or below 1% ) or Downsize if the instance is underutilized ( max CPU utilization is between 1% and 40% ). By default Amazon EC2 Resource Optimization doesn't need memory datapoint to provide recommendations, but if that information is available it will take that into consideration updating the Downsize recommendation for instances that now have max CPU and MEM utilization between 1% and 40% over the past 14 days. Let's validate that with the following steps. Navigate to the AWS Cost Explorer page Select Recommendations in the left menu bar Click on the View All link associated with the Amazon EC2 Resource Optimization Recommendations section. In case you haven\u2019t enabled the Amazon EC2 Resource Optimization please do so (no additional cost), it may take up to 24 hours in order to generate your first recommendations. Only regular or payer accounts can enable it and by default both linked and payer accounts will be able to access their rightsizing recommendations unless the payer account specifically prohibits it on the settings page (top right). Assuming you had enabled the Amazon EC2 Resource Optimization Recommendations, you will be presented with a screen that provides recommendations (if any exists). Click to view the Resource Optimization recommendations. Optimization opportunities \u2013 The number of recommendations available based on your resources and usage Estimated monthly savings \u2013 The sum of the projected monthly savings associated with each of the recommendations provided Estimated savings (%) \u2013 The available savings relative to the direct Amazon EC2 costs (On-Demand) associated with the instances in the recommendation list You can also filter your recommendations by the type of action (Idle and Underutilized), Linked Account, Region and Tag. Understanding the Amazon EC2 Resource Optimization recommendations. In the example below we have a recommendation to downsize the t2.micro (1vCPU for a 2h 24m burst and 1GB RAM) to a t2.nano (1vCPU for a 1h 12m burst and 0.5 GB RAM) and save $12 USD per year. Over the past 14 days the maximum CPU utilization for this instance was only 9% and this instance was running for 86 hours and all of these were On Demand hours. Observe that there is no memory information available so Amazon EC2 Resource Optimization will ignore that datapoint and recommend to downsize to a t2.nano that have half of the memory available of a t2.micro. That can be risky and waste engineer time when testing if the proposed right sizing option is valid or not. That said you can improve the accuracy of this recommendation with the CloudWatch agent we just installed. In this other example we see a recommendation to downsize a r5.8xlarge (32 vCPU and 256GB RAM) to a r5.4xlarge (16 vCPU and 128GB RAM) and save $2,412 USD per year. On this case we have both CPU and Memory information available: the maximum CPU utilization was 21% and Memory was only 5%. That makes the case for downsize much stronger and the recommendation will even try estimate the CPU and Memory utilization for the new instance size. Keep in mind that this is just a simple estimation based on the past utilization data from CloudWatch, before executing the modification all the required load tests must be performed to avoid any impacts on your workload. As explained above the Amazon EC2 Resource Optimization logic will recommend to downsize any instances where the maximum CPU utilization was between 1% to 40% over the past 14 days. If you do have memory information available the Amazon EC2 Resource Optimization will now consider to downsize instances that have both CPU and Memory maximum utilization between 1% and 40%. Idle recommendations are not impacted if memory data is available, so any EC2 instance that during the past 14 days never passed 1% of CPU utlization will be automatically flagged as idle. 6. Amazon EC2 Right Sizing Best Practices Start simple: idle resources, non-critical development/QA and previous generation instances will require less testing hours and provide quick wins (The Amazon EC2 Launch time statistics can be used to identify instances that have been running longer than others and is a good statistic to sort your Amazon EC2 instances by). Right Size before performing a migration: If you skip right sizing to save time, your migration speed might increase, but you will end up with higher cloud infrastructure spend for a potentially longer period of time. Instead, leverage the test and QA cycles during a migration exercise to test several instance types and families. Also, take that opportunity to test different sizes and burstable instances like the \u201ct\u201d family. The best right sizing starts on day 1: As you perform right sizing analysis, and ultimately rightsize resources, ensure any learnings are being shared across your organization and influencing the design of new workloads. Measure Twice, Cut Once: Test, then test some more: The last thing you want is for a new resource type to be uncapable of handling load, or functioning incorrectly. Test once and perform multiple right sizing: Aggregate instances per autoscaling group and tags to scale right sizing activities. Combine Reserved Instance or Savings Plans strategy with Right Sizing to maximize savings: For Standard RIs and EC2 Instance SP: Perform your pricing model purchases after rightsizing and for Convertible RIs, exchange them after rightsizing. Compute Savings plan will automatically adjust the commitment for the new environment. Ignore burstable instance families (T types): These families are designed to typically run at low CPU percentages for significant periods of time and shouldn\u2019t be part of the instance types being analyzed for rightsizing. 7. Tear down - Terminate the EC2 Instance - Delete the IAM role **CloudWatchAgentServerRole** 8. Rate this lab","title":"Level 200: EC2 Right Sizing: Lab Guide"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/Lab_Guide.html#level-200-ec2-right-sizing-lab-guide","text":"","title":"Level 200: EC2 Right Sizing: Lab Guide"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/Lab_Guide.html#authors","text":"Jeff Kassel, AWS Technical Account Manager Arthur Basbaum, AWS Cloud Economics","title":"Authors"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/Lab_Guide.html#table-of-contents","text":"Getting to know Amazon CloudWatch Create an IAM Role to use with Amazon CloudWatch Agent Attach CloudWatch IAM role to selected EC2 Instances Cloudwatch Agent Manual Install Updated Amazon EC2 Resource Optimization recommendations EC2 Right Sizing Best Practices EC2 Right Sizing Reference Material NOTE: In order to run this lab you will need to have at least one EC2 instance running and have AWS Cost Explorer and Amazon EC2 Resource Optimization enabled. During this lab we will create a custom metric at Amazon CloudWatch and install an agent in one EC2 instance to start collecting Memory utilization and improve the recommendation accuracy of the Amazon EC2 Resource Optimization report. Be aware that custom metrics are not part of the Amazon CloudWatch free tier usage so additional costs will incur at your bill . For more information read the Amazon CloudWatch pricing page. All custom metrics charges are prorated by the hour and metered only when you send metrics to Amazon CloudWatch. Each custom metrics costs $0.30 per metric/month for the first 10,000 metrics and can go down to $0.02 per metric/month at the lowest priced tier ( US Virginia prices from November 2019 ).","title":"Table of Contents"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/Lab_Guide.html#1-getting-to-know-amazon-cloudwatch","text":"The first step to perform right sizing is to monitor and analyze your current use of services to gain insight into instance performance and usage patterns. To gather sufficient data, observe performance over at least a two-week period (ideally, over a one-month period) to capture the workload and business peak. The most common metrics that define instance performance are vCPU utilization, memory utilization, network utilization, and disk use. Log into your AWS console, go to the Amazon CloudWatch service page: Select EC2 under the Service Dashboard : Observe the Service Dashboard and all of its different metrics, but focus on CPU Utilization and Network In and Out : Select one of the EC2 resources by clicking on the little color icon to the left of the resource-id name: Deselect the EC2 resource and now modify the time range on the top right, click custom and select the last 2 weeks : Navigate to the CPU Utilization Average widget and launch the View Metrics detailed page. Using the Graphed metrics session try to answer the following questions: a) What is the instance with the lowest CPU Average? b) What is the instance with the lowest CPU Max? c) What is the instance with the lowest CPU Min?","title":"1. Getting to know Amazon Cloudwatch"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/Lab_Guide.html#2-create-an-iam-role-to-use-with-amazon-cloudwatch-agent","text":"Access to AWS resources requires permissions. You will now create an IAM role to grant permissions that the agent needs to write metrics to CloudWatch. Amazon created two new default policies called CloudWatchAgentServerPolicy and CloudWatchAgentAdminPolicy only for that purpose. To create the IAM role first you will need to sign in to the AWS Management Console and open the IAM console In the navigation pane on the left, choose Roles and then Create role . Under Choose the service that will use this role , choose EC2 Allows EC2 instances to call AWS services on your behalf. Choose Next: Permissions . In the list of policies, select the check box next to CloudWatchAgentServerPolicy . If necessary, use the search box to find the policy, click Next: Tags : Add tags (optional) for this policy, click Next: Review . Confirm that CloudWatchAgentServerPolicy appears next to Policies . In Role name, enter a name for the role, such as CloudWatchAgentServerRole . Optionally give it a description. Then click Create role . The role is now created.","title":"2. Create an IAM Role to use with Amazon CloudWatch Agent"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/Lab_Guide.html#3-attach-cloudwatch-iam-role-to-selected-ec2-instances","text":"We are now going to attach the IAM Role created on the previous step in one of our EC2 Instances, to do that let's go to the Amazon EC2 Dashboard . On the left bar, click on Instances . Click on Launch Instance and select Linux 2 AMI (HVM) and t2.micro (free tier eligible) on the following screens. Click Review and launch : Click Launch Select Proceed without a key pair Complete this step by launching this testing t2.micro EC2 instance. In order to install the CloudWatch agent We will need to access this instance using the browser-based SSH native AWS connection tool so make sure that the port 22 is not blocked in the Security Group attached to this instance. After the instance is launched and runnning, select the instance you want to start collecting Memory data by going to Actions on the top bar, select Instance Settings>>Attach/Replace IAM Role . Look for the created IAM role CloudWatchAgentServerRole under the IAM role box, select it and apply. Confirm that the CloudWatchAgentServerRole was sucessfully attached to your EC2 instance Validade if the IAM role CloudWatchAgentServerRole is attached to the desired instance","title":"3. Attach CloudWatch IAM role to selected EC2 Instances"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/Lab_Guide.html#4-cloudwatch-agent-manual-install","text":"We are now going to manually install the CloudWatch agent to start collecting memory data, to start let's go back to the Amazon EC2 Dashboard . On the left bar, click on Instances and select the EC2 Instance with the CloudWatchAgentServerRole IAM role. Connect into the EC2 Instance using the browser-based SSH connection tool . Download the Amazon Cloudwatch agent package, the instructions below are for Amazon Linux, for other OS please check here wget https://s3.amazonaws.com/amazoncloudwatch-agent/linux/amd64/latest/AmazonCloudWatchAgent.zip Unzip and Install the package unzip AmazonCloudWatchAgent.zip sudo ./install.sh Configure the AmazonCloudWatchAgent profile Before running the CloudWatch agent on any servers, you must create a CloudWatch agent configuration file, which is a JSON file that specifies the metrics and logs that the agent is to collect, including custom metrics. You can create it by using the wizard or by writting it yourself from scratch. Any time you change the agent configuration file, you must then restart the agent to have the changes take effect. The wizard can autodetect the credentials and AWS Region to use if you have the AWS credentials and configuration files in place. For more information about these files, see Configuration and Credential Files in the AWS Systems Manager User Guide and the AWS documentation page . For now, let's start the CloudWatch agent configuration file wizard executing the command below at the selected EC2 instance. sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard For this lab we want to keep the following structure: CloudWatch Agent Configutation File Wizard Parameter On which OS are you planning to use the agent? 1. Linux Are you using EC2 or On-Premises hosts? 1. EC2 Which user are you planning to run the agent? 2. cwagent Do you want to turn on StatsD daemon? 2. No Do you want to monitor metrics from CollectD? 2. No Do you want to monitor any host metrics? 1. Yes Do you want to monitor cpu metrics per core? 2. No Do you want to add ec2 dimensions? 1. Yes Would you like to collect your metrics at high resolution? 4. 60s Which default metrics config do you want? 1. Basic Are you satisfied with the above config? 1. Yes Do you have any existing CloudWatch Log Agent? 2. No Do you want to monitor any log files? 2. No Do you want to store the config in the SSM parameter store? 2. No The CloudWatch Agent config file should look like the following: { \"agent\": { \"metrics_collection_interval\": 60, \"run_as_user\": \"cwagent\" }, \"metrics\": { \"append_dimensions\": { \"AutoScalingGroupName\": \"${aws:AutoScalingGroupName}\", \"ImageId\": \"${aws:ImageId}\", \"InstanceId\": \"${aws:InstanceId}\", \"InstanceType\": \"${aws:InstanceType}\" }, \"metrics_collected\": { \"disk\": { \"measurement\": [ \"used_percent\" ], \"metrics_collection_interval\": 60, \"resources\": [ \"*\" ] }, \"mem\": { \"measurement\": [ \"mem_used_percent\" ], \"metrics_collection_interval\": 60 } } } } Start the CloudWatch Agent sudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json -s It may take up to 5 minutes for the metrics to become available, go back to the Amazon CloudWatch console page, under the Metrics session to validate that you are getting Memory information. Click CWAgent : Click ImageID,InstanceID,InstanceType : Select the Instance from the list below: You have now completed the CloudWatch agent installation and will be able to monitor on Amazon CloudWatch the memory utilization of that instance. [BONUS] The next step is not mandatory to complete this lab. If you have a lot of instances manually installing the CloudWatch agent in each of them is not a scalable option, instead consider using a pre-configured AWS CloudFormation template to automatically install the CloudWatch agent by default on all your stack. As an example on how to do that check the following steps: Right-click and save link as: here to download the AWS Cloudformation template Go to the AWS CloudFormation console Click to Create Stack and select Upload a template file and point to the downloaded file Enter a Stack Name and select a KeyName Enter the tag Key: Event | Value: myStackforWACostLab Click Next and Create stack","title":"4. Cloudwatch Agent Manual Install"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/Lab_Guide.html#5-updated-amazon-ec2-resource-optimization-recommendations","text":"NOTE : In order to complete this step you need to have Amazon EC2 Resource Optimization enabled, you can do that going to the AWS Cost Explorer, Recommendations (left bar) section. Important: If you have just installed the CloudWatch agent at your instances it may take a couple of days for Amazon EC2 Resource Optimization to start to provide updated recommendations, so don't worry if you don't see the memory data during the first checks. Now that we have Memory data as a custom metric in CloudWatch let's check how that affects the Amazon EC2 Resource Optimization recommendations. Amazon EC2 Resource Optimization offers right sizing recommendations in the AWS Cost Explorer without any additional cost. These recommendations identify likely idle and underutilized instances across your accounts, regions and tags. To generate these recommendations, AWS analyzes your historical EC2 resource usage (last 14 days using Amazon CloudWatch) and your existing reservation footprint to identify opportunities for cost savings. There are two types of recommended actions: Terminate if the instance is considered idle ( max CPU utilization is at or below 1% ) or Downsize if the instance is underutilized ( max CPU utilization is between 1% and 40% ). By default Amazon EC2 Resource Optimization doesn't need memory datapoint to provide recommendations, but if that information is available it will take that into consideration updating the Downsize recommendation for instances that now have max CPU and MEM utilization between 1% and 40% over the past 14 days. Let's validate that with the following steps. Navigate to the AWS Cost Explorer page Select Recommendations in the left menu bar Click on the View All link associated with the Amazon EC2 Resource Optimization Recommendations section. In case you haven\u2019t enabled the Amazon EC2 Resource Optimization please do so (no additional cost), it may take up to 24 hours in order to generate your first recommendations. Only regular or payer accounts can enable it and by default both linked and payer accounts will be able to access their rightsizing recommendations unless the payer account specifically prohibits it on the settings page (top right). Assuming you had enabled the Amazon EC2 Resource Optimization Recommendations, you will be presented with a screen that provides recommendations (if any exists). Click to view the Resource Optimization recommendations. Optimization opportunities \u2013 The number of recommendations available based on your resources and usage Estimated monthly savings \u2013 The sum of the projected monthly savings associated with each of the recommendations provided Estimated savings (%) \u2013 The available savings relative to the direct Amazon EC2 costs (On-Demand) associated with the instances in the recommendation list You can also filter your recommendations by the type of action (Idle and Underutilized), Linked Account, Region and Tag. Understanding the Amazon EC2 Resource Optimization recommendations. In the example below we have a recommendation to downsize the t2.micro (1vCPU for a 2h 24m burst and 1GB RAM) to a t2.nano (1vCPU for a 1h 12m burst and 0.5 GB RAM) and save $12 USD per year. Over the past 14 days the maximum CPU utilization for this instance was only 9% and this instance was running for 86 hours and all of these were On Demand hours. Observe that there is no memory information available so Amazon EC2 Resource Optimization will ignore that datapoint and recommend to downsize to a t2.nano that have half of the memory available of a t2.micro. That can be risky and waste engineer time when testing if the proposed right sizing option is valid or not. That said you can improve the accuracy of this recommendation with the CloudWatch agent we just installed. In this other example we see a recommendation to downsize a r5.8xlarge (32 vCPU and 256GB RAM) to a r5.4xlarge (16 vCPU and 128GB RAM) and save $2,412 USD per year. On this case we have both CPU and Memory information available: the maximum CPU utilization was 21% and Memory was only 5%. That makes the case for downsize much stronger and the recommendation will even try estimate the CPU and Memory utilization for the new instance size. Keep in mind that this is just a simple estimation based on the past utilization data from CloudWatch, before executing the modification all the required load tests must be performed to avoid any impacts on your workload. As explained above the Amazon EC2 Resource Optimization logic will recommend to downsize any instances where the maximum CPU utilization was between 1% to 40% over the past 14 days. If you do have memory information available the Amazon EC2 Resource Optimization will now consider to downsize instances that have both CPU and Memory maximum utilization between 1% and 40%. Idle recommendations are not impacted if memory data is available, so any EC2 instance that during the past 14 days never passed 1% of CPU utlization will be automatically flagged as idle.","title":"5. Updated Amazon EC2 Resource Optimization recommendations"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/Lab_Guide.html#6-amazon-ec2-right-sizing-best-practices","text":"Start simple: idle resources, non-critical development/QA and previous generation instances will require less testing hours and provide quick wins (The Amazon EC2 Launch time statistics can be used to identify instances that have been running longer than others and is a good statistic to sort your Amazon EC2 instances by). Right Size before performing a migration: If you skip right sizing to save time, your migration speed might increase, but you will end up with higher cloud infrastructure spend for a potentially longer period of time. Instead, leverage the test and QA cycles during a migration exercise to test several instance types and families. Also, take that opportunity to test different sizes and burstable instances like the \u201ct\u201d family. The best right sizing starts on day 1: As you perform right sizing analysis, and ultimately rightsize resources, ensure any learnings are being shared across your organization and influencing the design of new workloads. Measure Twice, Cut Once: Test, then test some more: The last thing you want is for a new resource type to be uncapable of handling load, or functioning incorrectly. Test once and perform multiple right sizing: Aggregate instances per autoscaling group and tags to scale right sizing activities. Combine Reserved Instance or Savings Plans strategy with Right Sizing to maximize savings: For Standard RIs and EC2 Instance SP: Perform your pricing model purchases after rightsizing and for Convertible RIs, exchange them after rightsizing. Compute Savings plan will automatically adjust the commitment for the new environment. Ignore burstable instance families (T types): These families are designed to typically run at low CPU percentages for significant periods of time and shouldn\u2019t be part of the instance types being analyzed for rightsizing.","title":"6. Amazon EC2 Right Sizing Best Practices"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/Lab_Guide.html#7-tear-down","text":"- Terminate the EC2 Instance - Delete the IAM role **CloudWatchAgentServerRole**","title":"7. Tear down"},{"location":"Cost/Cost_Effective_Resources/200_AWS_Resource_Optimization/Lab_Guide.html#8-rate-this-lab","text":"","title":"8. Rate this lab"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/README.html","text":"Level 200: Pricing Model Analysis http://wellarchitectedlabs.com Your browser doesnt support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video. Introduction This hands-on lab will guide you through setting up pricing and usage data sources, then creating a visualization to view your costs over time for EC2 in Savings Plans rates, allowing you to make low risk, high return purchases for pricing models. The data sources will also allow you to do custom allocation of discounts for your organization (a separate lab). You will create two pricing data sources, by using Lambda to download the AWS price list files (On Demand EC2 pricing, and Savings Plans rates from all regions) and extract the pricing components required. You can configure CloudWatch Events to peridoically run these functions to ensure you have the most up to date pricing and the latest instances in your data. The pricing files are then combined with your Cost and Usage Report (CUR), to provide an hourly usage report with multiple pricing dimensions, this allows you query and analyze your usage by on demand rates, savings plan rates, or the difference between the two (discount level). Finally you create a visualization with calculations in QuickSight which allows you to view your usage patterns, and also perform analysis to understand the commitment levels that are right for your business. NOTE : this lab demonstrates EC2 savings plans only, but can be extended to cover other services such as Fargate or Lambda. Goals Setup the pricing and usage data sources Create the visualization for recommendations and analysis Prerequisites An AWS Account An Amazon QuickSight Account A Cost and Usage Report (CUR) Amazon Athena and QuickSight have been setup Completed the Cost and Usage Analysis lab Completed the Cost and Usage Visualization lab Basic knowledge of AWS Lambda, Amazon Athena and Amazon Quicksight Permissions required Create a Lambda function, trigger it via CloudWatch Access to create an S3 bucket Access to your CUR files Access to setup a data source and create a visualization in QuickSight Costs TBA Time to complete The lab should take approximately 50-60 minutes to complete License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/README.html#level-200-pricing-model-analysis","text":"http://wellarchitectedlabs.com Your browser doesnt support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video.","title":"Level 200: Pricing Model Analysis"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/README.html#introduction","text":"This hands-on lab will guide you through setting up pricing and usage data sources, then creating a visualization to view your costs over time for EC2 in Savings Plans rates, allowing you to make low risk, high return purchases for pricing models. The data sources will also allow you to do custom allocation of discounts for your organization (a separate lab). You will create two pricing data sources, by using Lambda to download the AWS price list files (On Demand EC2 pricing, and Savings Plans rates from all regions) and extract the pricing components required. You can configure CloudWatch Events to peridoically run these functions to ensure you have the most up to date pricing and the latest instances in your data. The pricing files are then combined with your Cost and Usage Report (CUR), to provide an hourly usage report with multiple pricing dimensions, this allows you query and analyze your usage by on demand rates, savings plan rates, or the difference between the two (discount level). Finally you create a visualization with calculations in QuickSight which allows you to view your usage patterns, and also perform analysis to understand the commitment levels that are right for your business. NOTE : this lab demonstrates EC2 savings plans only, but can be extended to cover other services such as Fargate or Lambda.","title":"Introduction"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/README.html#goals","text":"Setup the pricing and usage data sources Create the visualization for recommendations and analysis","title":"Goals"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/README.html#prerequisites","text":"An AWS Account An Amazon QuickSight Account A Cost and Usage Report (CUR) Amazon Athena and QuickSight have been setup Completed the Cost and Usage Analysis lab Completed the Cost and Usage Visualization lab Basic knowledge of AWS Lambda, Amazon Athena and Amazon Quicksight","title":"Prerequisites"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/README.html#permissions-required","text":"Create a Lambda function, trigger it via CloudWatch Access to create an S3 bucket Access to your CUR files Access to setup a data source and create a visualization in QuickSight","title":"Permissions required"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/README.html#costs","text":"TBA","title":"Costs"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/README.html#time-to-complete","text":"The lab should take approximately 50-60 minutes to complete","title":"Time to complete"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/Lab_Guide.html","text":"Level 200: Pricing Model Analysis Authors Nathan Besh, Cost Lead, Well-Architected (AWS) Nataliya Godunok, Technial Account Manager (AWS) Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Create Pricing Data Sources Create the Usage Data Source Setup QuickSight Dashboard Create the Recommendation Dashboard Format the Recommendation Dashboard Teardown Rate this Lab 1. Create Pricing Data Sources Create S3 bucket with folders, lambda functions, CloudWatch event, table in Athena 1.1. Create S3 Bucket and Folders Create a single S3 bucket that contains two folders - od_pricedata and sp_pricedata , these will contain the on-demand pricing data and the Savings Plans pricing data. Log into the console as an IAM user with the required permissions, go to the S3 service page: Click Create bucket : Enter a Bucket name (we have used sptool-pricingfiles, you will need to use a unique bucket name) and click Create bucket : Click on the (bucket name) : Click Create folder : Enter a folder name of od_pricedata , click Save : Click Create folder : Enter a folder name of sp_pricedata , click Save : You have now setup the S3 bucket with the two folders that will contain the OnDemand and Savings Plans pricing data. 1.2. Setup On-Demand Pricing Lambda Function Create the On-Demand Lambda function to get the pricing information, and extract the required parts from it. Go to the Lambda service page: Click Create function : Enter the following details: Select: Author from scratch Function name: SPTool_ODPricing_Download Runtime: Python (Latest) Execution Role: Create a new role Role name: SPTool_Lambda Click Create function Copy and paste the following code into the Function code section: Click here to see the function code # Lambda Function Code - SPTool_OD_pricing_Download # Function to download OnDemand pricing, get out the required lines & upload it to S3 as a zipped file # It will find 'OnDemand' and 'Compute Instance', and write to a file # Written by natbesh@amazon.com # Please reachout to costoptimization@amazon.com if there's any comments or suggestions import boto3 import gzip import urllib3 def lambda_handler(event, context): # Create the connection http = urllib3.PoolManager() try: # Get the EC2 OnDemand pricing file, its huge >1GB r = http.request('GET', 'https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/AmazonEC2/current/index.csv') # Put the response data into a variable & split it into an array line by line plaintext_content = r.data plaintext_lines = plaintext_content.splitlines() # Varaible to hold the OnDemand pricing data pricing_output = \"\" # Go through each of the pricing lines to find the ones we need for line in plaintext_lines: # If the line contains 'OnDemand' or 'Compute Instance' then add it to the output string if ((str(line).find('OnDemand') != -1) and (str(line).find('RunInstances') != -1)): pricing_output += str(line.decode(\"utf-8\")) pricing_output += \"\\n\" # Add the output to a local temporary file & zip it with gzip.open('/tmp/od_pricedata.txt.gz', 'wb') as f: f.write(pricing_output.encode()) # Upload the zipped file to S3 s3 = boto3.resource('s3') # Specify the local file, the bucket, and the folder and object name - you MUST have a folder and object name s3.meta.client.upload_file('/tmp/od_pricedata.txt.gz', 'bucket_name', 'od_pricedata/od_pricedata.txt.gz') # Die if you cant get the pricing file except Exception as e: print(e) raise e # Return happy return { 'statusCode': 200 } Edit the pasted code, replacing bucket_name with the name of your bucket: Edit Basic settings below: Memory: 2688MB Timeout: 5min Click save Scroll to the top and click Test Enter an Event name of Test , click Create : Click Save : Click on Permissions : Right-click(open link in new tab) View the SPTool_Lambda role to open the role in the IAM Service page: Click on Attach policies : Click on Create policy : Create a policy to allow object write to your S3 bucket created, and click Review policy : Name the policy s3_pricing_lambda , optionally and add a description, click Create policy : Click on Roles : Click on the role SPTool_Lambda : Click Attach policies : Click Filter Policies and select Customer managed : Select the S3_pricing_lambda policy and click Attach policy : Go back to the Lambda service page and click Test : The function will run, it will take a minute or two given the size of the pricing files and processing required, then return success. Click Details and verify there is headroom in the configured resources and duration to allow any increases in pricing file size over time: Go to your S3 bucket and into the od_pricedata folder and you should see a gz file of non-zero size is in it: 1.3. Setup Savings Plan Pricing Lambda Function Create the Savings Plan Lambda function to get the pricing information, and extract the required parts from it. Go to the Lambda service page: Click Create function : Enter the following details: Select: Author from scratch Function name: SPTool_SPPricing_Download Runtime: Python (Latest) Execution Role: Use an existing role Role name: SPTool_Lambda Click Create function Copy and paste the following code into the Function code section: Click here to see the function code # Lambda Function Code - SPTool_SP_pricing_Download # Function to download SavingsPlans pricing, get out the required lines & upload it to S3 as a zipped file # It will get each regions pricing file in CSV, find 'Usage' and '1yr', and write to a file # Written by natbesh@amazon.com # Please reachout to costoptimization@amazon.com if there's any comments or suggestions import boto3 import gzip import urllib3 import json def lambda_handler(event, context): # Create the connection http = urllib3.PoolManager() try: # Get the SavingsPlans pricing index file, so you can get all the region files, which have the pricing in them r = http.request('GET', 'https://pricing.us-east-1.amazonaws.com/savingsPlan/v1.0/aws/AWSComputeSavingsPlan/current/region_index.json') # Load the json file into a variable, and parse it sp_regions = r.data sp_regions_json = (json.loads(sp_regions)) # Variable to hold all of the pricing data, its large at over 150MB sp_pricing_data = \"\" # Cycle through each regions pricing file, to get the data we need for region in sp_regions_json['regions']: # Get the CSV URL url = \"https://pricing.us-east-1.amazonaws.com\" + region['versionUrl'] url = url.replace('.json', '.csv') # Create a connection & get the regions pricing data CSV file http = urllib3.PoolManager() r = http.request('GET', url) spregion_content = r.data # Split the lines into an array spregion_lines = spregion_content.splitlines() # Go through each of the pricing lines for line in spregion_lines: # If the line has 'Usage' then grab it for pricing data, exclude all others if (str(line).find('Usage') != -1): sp_pricing_data += str(line.decode(\"utf-8\")) sp_pricing_data += \"\\n\" # Compress the text into a local temporary file with gzip.open('/tmp/sp_pricedata.txt.gz', 'wb') as f: f.write(sp_pricing_data.encode()) # Upload the file to S3 s3 = boto3.resource('s3') # Specify the local file, the bucket, and the folder and object name - you MUST have a folder and object name s3.meta.client.upload_file('/tmp/sp_pricedata.txt.gz', 'bucket_name', 'sp_pricedata/sp_pricedata.txt.gz') # Die if you cant get the file except Exception as e: print(e) raise e # Return happy return { 'statusCode': 200 } Edit the pasted code, replacing bucket_name with the name of your bucket: Edit Basic settings below: Memory: 2688MB Timeout: 5min Click save Scroll to the top and click Test Enter an Event name of Test , click Create : Click Save : Click Test : The function will run, it will take a minute or two given the size of the pricing files and processing required, then return success. Click Details and verify there is headroom in the configured resources and duration to allow any increases in pricing file size over time: Go to your S3 bucket and into the sp_pricedata folder and you should see a gz file of non-zero size is in it: 1.4 CloudWatch Events Setup We will setup a CloudWatch Event to periodically run the Lambda functions, this will update the pricing and include any newly released instances. Go to the CloudWatch service page: Click on Events , then click Rules : Click Create rule For the Event Source select Schedule and set the required period, we have selected 5 days , click Add target : Add the SPTool_ODPricing_Download Lambda function, and click Add target : Add the SPTool_SPPricing_Download Lambda function, and click Configure details : Add the name SPTool-pricing , optionally add a description and click Create rule : You have now successfully configured a CloudWatch event, it will run the two Lambda functions and update the pricing information every 5 days. 1.5 Prepare the Pricing Data Source We will prepare a pricing data source which we will use to join with the CUR. In this example we will take 1 year No Upfront Savings Plans rates and join them to On-Demand pricing. You can modify this part to select 3 year or Partial or All-Upfront rates. Go to the Glue Service page: Click Crawlers from the left menu: Click Add crawler : Enter a crawler name of OD_Pricing and click Next : Ensure Data stores is the source type, click Next : Click the folder icon to list the S3 folders in your account: Expand the bucket which contains your pricing folders, and select the folder name od_pricedata , click Select : Click Next : Click Next : Create an IAM role with a name of SPToolPricing , click Next : Leave the frequency as Run on demand , and click Next : Click on Add database : Enter a database name of pricing , and click Create : Click Next : Click Finish : Select the crawler OD_Pricing and click Run crawler : Once its run, you should see tables created: Repeat Steps 3 through to 17 with the following details: Crawler name: SP_Pricing Include path: s3://(pricing bucket)/sp_pricedata (replace pricing bucket) IAM Role: Choose an existing IAM role and AWSGlueServiceRole-SPToolPricing Database: pricing Open the IAM Console in a new tab, click Policies : Click on the AWSGlueServiceRole-SPToolPricing role: Type in SPTool and click on the policy name AWSGlueServiceRole-SPTool : Click Edit policy : Click JSON : Edit the Resource line by removing the OD_Pricing folder to leave the bucket: Click Review policy : Click Save changes : Go back to the Glue console, select the SP_Pricing crawler, click Run crawler : Click on Databases : Click on Pricing : Click Tables in pricing : Click od_pricedata : 32.Click Edit schema : Click double next to col9 : Select string and click Update : Click Save : 2. Create the Usage Data Source We will combine the pricing information with our Cost and Usage Report (CUR). This will give us a usage data source which contains a summary of your usage at an hourly level, with multiple pricing dimensions. Go to the Athena service page: Run the following query to create a single pricing data source, combining the OD and SP pricing: Click here to see the Athena SQL code CREATE VIEW pricing.pricing AS SELECT sp.location AS Region, sp.discountedoperation AS OS, REPLACE(od.col18, '\"') AS InstanceType, REPLACE(od.col35, '\"') AS Tenancy, REPLACE(od.col9, '\"') AS ODRate, sp.discountedrate AS SPRate FROM pricing.sp_pricedata sp JOIN pricing.od_pricedata od ON ((sp.discountedusagetype = REPLACE(od.col46, '\"')) AND (sp.discountedoperation = REPLACE(od.col47, '\"'))) WHERE od.col9 IS NOT NULL AND sp.location NOT LIKE 'Any' AND sp.purchaseoption LIKE 'No Upfront' AND sp.leasecontractlength = 1 Next we'll join the CUR file with that pricing source as a view. Edit the following query, replace cur.curfile with your existing database name and tablename of your CUR, then run the rollowing query: Click here to see the Athena SQL code CREATE VIEW cur.SP_Usage AS SELECT cur.line_item_usage_account_id, cur.line_item_usage_start_date, to_unixtime(cur.line_item_usage_start_date) AS EpochTime, cur.product_instance_type, cur.product_location, cur.product_operating_system, cur.product_tenancy, SUM(cur.line_item_unblended_cost) AS ODPrice, SUM(cur.line_item_unblended_cost*(cast(pr.SPRate AS double)/cast(pr.ODRate AS double))) SPPrice, abs(SUM(cast(pr.SPRate AS double)) - SUM (cast(pr.ODRate AS double))) / SUM(cast(pr.ODRate AS double))*100 AS DiscountRate, SUM(cur.line_item_usage_amount) AS InstanceCount FROM cur.curfile cur JOIN pricing.pricing pr ON (cur.product_location = pr.Region) AND (cur.line_item_operation = pr.OS) AND (cur.product_instance_type = pr.InstanceType) AND (cur.product_tenancy = pr.Tenancy) WHERE cur.line_item_product_code LIKE '%EC2%' AND cur.product_instance_type NOT LIKE '' AND cur.product_operating_system NOT LIKE 'NA' AND cur.line_item_unblended_cost > 0 GROUP BY cur.line_item_usage_account_id, cur.line_item_usage_start_date, cur.product_instance_type, cur.product_location, cur.product_operating_system, cur.product_tenancy ORDER BY cur.line_item_usage_start_date ASC, DiscountRate DESC Verify the data source is setup by editing the following query, replace cur. with the name of the database and run the following query: SELECT * FROM cur.sp_usage limit 10; You now have your usage data source setup with your pricing dimensions. You can modify the queries above to add or remove any columns you want in the view, which can later be used to visualize the data, for example tags. 3. Setup QuickSight Dashboard We will now setup the QuickSight dashboard to visualize your usage by cost, and setup the analysis to provide Savings Plan recommendations. Go to the QuickSight service: Click on your username in the top right: Click Manage QuickSight : Click Security & permissions , then click Add or remove : Click Details next to Amazon S3: Click Select S3 buckets : Select your pricing bucket and click Finish : Scroll down and click Update : Click on QuickSight to go to the home page: Click on Manage data : Click on New data set : Click Athena : Enter a Data source name of SP_Usage and click Create data source : Select the cur database, and the sp_usage table, click Select : Ensure SPICE is selected, click Visualize : Click on QuickSight to go to the home page: Click on Manage data : Select the sp_usage Dataset: Click Schedule refresh : Click Create : Enter a schedule, it needs to be refreshed daily, and click Create : Click Cancel to exit: Click the x in the top corner: 4. Create the Recommendation Dashboard Go to the QuickSight service homepage: Go to the sp_usage analysis : Create a line chart, add line_item_usage_start_date to the X axis , aggregate day . Add spprice to the Value and set the aggregate to min . Drag the product_instance_type to Colour field well. Change the title to Usage in Savings Plan Rates : Click Parameters , and click Create one : Parameter name OperatingSystem , Data type String , click Set a dynamic default : Select your dataset, and select product_operating_system for the columns, click Apply: Click Create : Click Control : Enter OperatingSystem as the display name, style Single select drop down , values Link to a data set field , dataset your data set , column product_operating_system , click Add : Using the process above, Add the parameter Region : Name: Region Data type: String Values: Single value Dyanmic default Dataset: your dataset, product_location, product_location Add as: Control Control Display Name: Region Style: Single select drop down Values: link to data set field Data set: your data set Column: product_location Using the process above, Add the parameter Tenancy : Name: Tenancy Data type: String Values: Single value Dyanmic default Dataset: your dataset, product_tenancy, product_tenancy Add as: Control Control Display Name: Tenancy Style: Single select drop down Values: link to data set field Data set: your data set Column: product_tenancy Create an InstanceType parameter, datatype String , Single value , Static default value of . (a full stop): Click Control , Display name InstanceType , style Text box , click Add : Click Filter and click Create one , select product_instance_type : Edit the filter, Filter type: All visuals Custom filter Contains Use Parameters InstanceType click Apply : Create a Parameter DaysToDisplay : Name: DaysToDisplay Data type: Integer Values: Single value Static default value: 90 Click Create : Click Control : Enter a Display name DaysToDisplay , Style Text box and click Add : Click on Filter , click + , and select line_item_usage_start_date : Click on the new filter: Select a filter type of: All visuals Relative dates Days Last N days select Use parameters , and accept to change the scope of the filter select the parameter DaysToDisplay click Apply : Create a filter for product_operating_system : All visuals Type: Custom filter equals Use parameters, change the scope of this filter: yes Parameter: OperatingSystem Create a filter for product_location : All visuals Type: Custom filter equals Use parameters, change the scope of this filter: yes Parameter: Region Create a filter for product_tenancy : All visuals Type: Custom filter equals Use parameters, change the scope of this filter: yes Parameter: Tenancy Click on Visualize , click Add , select Add calculated field : Field name HoursDisplayed , add the formula below and click Create : distinct_count({line_item_usage_start_date}) Create a calculated field HoursRun , formula: HoursDisplayed / (${DaysToDisplay} * 24) Create a calculated field PayOffMonth , formula: ifelse(((((sum(spprice) / HoursDisplayed) * 730 * 12) / ((sum(odprice) / (${DaysToDisplay} * 24)) * 730))) < 12,((((sum(spprice) / HoursDisplayed) * 730 * 12) / ((sum(odprice) / (${DaysToDisplay} * 24)) * 730))),12) Create a calculated field SavingsPlanReco , formula: ifelse(PayOffMonth < 12,percentile(spprice,10),0.00) Create a calculated field StartSPPrice , formula: lag(min(spprice),[{line_item_usage_start_date} ASC],${DaysToDisplay} - 2,[{product_instance_type}]) Create a calculated field Trend , formula: (min(spprice) - {StartSPPrice}) / min(spprice) Create a calculated field First3QtrAvg , formula: windowAvg(avg(spprice),[{line_item_usage_start_date} ASC],${DaysToDisplay},${DaysToDisplay} / 4,[{product_instance_type}]) Create a calculated field LastQtrAvg , formula: windowAvg(avg(spprice),[{line_item_usage_start_date} ASC],${DaysToDisplay} / 4,1,[{product_instance_type}]) Create a calculated field TrendAvg , formula: (LastQtrAvg- First3QtrAvg) / First3QtrAvg Add a Visual, click Add , select Add visual : Select a Table visualization, Group by product_instance_type , Add the values : SavingsPlanReco PayOffMonth discountrate, aggregate: average HoursRun, show as percent Label it Recommendations Add a Table visual, Group By: product_instance_type and line_item_usage_start_date aggreate: day , Add the values : instancecount aggregate: average Trend TrendAvg (show as percent) Label it Trends , Add a filter to this visual only : Filter on: StartSPPrice Type: Custom filter Operation: Greater than Value: -1 Nulls: Exclude nulls Decrease the width of the date column as much as possible, its not needed 5. Format the Recommendation Dashboard We will format the recommendation dashboard, this will improve its appearance, and also includes some business rules. Click on Themes , then click on Midnight : Select the Recommendations table, click the three dots , click Conditional formatting : Column: PayOffMonth , Add background color : Enter the formatting: Condition: Greater than Value: 9 Color: red Click Apply , click Close : Using the same process, add formatting for the column discountrate : Type: Background color Condition: Less than Value: 10 NOTE adjust this for your business rules, speak with your finance teams Color: red Click Apply , click Close Under the discountrate formatting, Click Add text color : Condition: Greater than Value: 20 Color: Green Click Apply , click Close Using the same process, add formatting for the column HoursRun : Type: Add text color Condition: Less than Value: 0.6 Color: Red Click Add condition Condition#2: Less than Value: 0.85 Color: Orange Click Apply , click Close Add formatting for the column SavingsPlanReco : Type: Add background color Format field based on: PayOffMonth Condition: Greater than Value: 9 Color: Red Click Apply , click Close Click Add text color Format field based on: discountrate Aggregation: Average Condition: greater than Value: 20 Color: Green Click Apply , click Close Click SavingsPlanReco , Sort by Descending : Select the Trends table, select conditional formatting, Column instancecount : Type: Add background color Condition: Less than Value: 5, speak with your team to set this at the appropriate level Color: red Click Add condition Condition #2: Less than Value: 10 Color: Orange Click Apply , click Close Using the process above on the Trends table, Select the Trend column: Type: Add background color Condition: Less than Value 0 Color: Red Click Apply , click Close Click Add text color Condition: Greater than Value: 0 Color: Green Click Apply , Click Close Add the same formatting to the TrendAvg column as the Trend column. Congratulations - you now have an analytics dashboard for Savings Plan recommendations! 6. Teardown Savings Plan analysis is a critical requirement of cost optimization, so there is no tear down for this lab. The following resources were created in this lab: S3 Bucket: (custom name) Lambda Functions: SPTool_ODPricing_Download and SPTool_SPPricing_Download IAM Role: SPTool_Lambda IAM Policy: s3_pricing_lambda CloudWatch Event, Rule: SPTool-Pricing Glue Crawlers: OD_Pricing and SP_Pricing IAM Role: AWSGlueServiceRole-SPToolPricing Glue Database: Pricing Athena Views: pricing.pricing and cur.SP_USage QuickSight Permissions: your pricing S3 bucket QuickSight Dataset: SP_Usage QuickSight Analysis: sp_usage analysis 7. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/Lab_Guide.html#level-200-pricing-model-analysis","text":"","title":"Level 200: Pricing Model Analysis"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead, Well-Architected (AWS) Nataliya Godunok, Technial Account Manager (AWS)","title":"Authors"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/Lab_Guide.html#table-of-contents","text":"Create Pricing Data Sources Create the Usage Data Source Setup QuickSight Dashboard Create the Recommendation Dashboard Format the Recommendation Dashboard Teardown Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/Lab_Guide.html#1-create-pricing-data-sources","text":"Create S3 bucket with folders, lambda functions, CloudWatch event, table in Athena","title":"1. Create Pricing Data Sources"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/Lab_Guide.html#11-create-s3-bucket-and-folders","text":"Create a single S3 bucket that contains two folders - od_pricedata and sp_pricedata , these will contain the on-demand pricing data and the Savings Plans pricing data. Log into the console as an IAM user with the required permissions, go to the S3 service page: Click Create bucket : Enter a Bucket name (we have used sptool-pricingfiles, you will need to use a unique bucket name) and click Create bucket : Click on the (bucket name) : Click Create folder : Enter a folder name of od_pricedata , click Save : Click Create folder : Enter a folder name of sp_pricedata , click Save : You have now setup the S3 bucket with the two folders that will contain the OnDemand and Savings Plans pricing data.","title":"1.1. Create S3 Bucket and Folders"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/Lab_Guide.html#12-setup-on-demand-pricing-lambda-function","text":"Create the On-Demand Lambda function to get the pricing information, and extract the required parts from it. Go to the Lambda service page: Click Create function : Enter the following details: Select: Author from scratch Function name: SPTool_ODPricing_Download Runtime: Python (Latest) Execution Role: Create a new role Role name: SPTool_Lambda Click Create function Copy and paste the following code into the Function code section: Click here to see the function code # Lambda Function Code - SPTool_OD_pricing_Download # Function to download OnDemand pricing, get out the required lines & upload it to S3 as a zipped file # It will find 'OnDemand' and 'Compute Instance', and write to a file # Written by natbesh@amazon.com # Please reachout to costoptimization@amazon.com if there's any comments or suggestions import boto3 import gzip import urllib3 def lambda_handler(event, context): # Create the connection http = urllib3.PoolManager() try: # Get the EC2 OnDemand pricing file, its huge >1GB r = http.request('GET', 'https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/AmazonEC2/current/index.csv') # Put the response data into a variable & split it into an array line by line plaintext_content = r.data plaintext_lines = plaintext_content.splitlines() # Varaible to hold the OnDemand pricing data pricing_output = \"\" # Go through each of the pricing lines to find the ones we need for line in plaintext_lines: # If the line contains 'OnDemand' or 'Compute Instance' then add it to the output string if ((str(line).find('OnDemand') != -1) and (str(line).find('RunInstances') != -1)): pricing_output += str(line.decode(\"utf-8\")) pricing_output += \"\\n\" # Add the output to a local temporary file & zip it with gzip.open('/tmp/od_pricedata.txt.gz', 'wb') as f: f.write(pricing_output.encode()) # Upload the zipped file to S3 s3 = boto3.resource('s3') # Specify the local file, the bucket, and the folder and object name - you MUST have a folder and object name s3.meta.client.upload_file('/tmp/od_pricedata.txt.gz', 'bucket_name', 'od_pricedata/od_pricedata.txt.gz') # Die if you cant get the pricing file except Exception as e: print(e) raise e # Return happy return { 'statusCode': 200 } Edit the pasted code, replacing bucket_name with the name of your bucket: Edit Basic settings below: Memory: 2688MB Timeout: 5min Click save Scroll to the top and click Test Enter an Event name of Test , click Create : Click Save : Click on Permissions : Right-click(open link in new tab) View the SPTool_Lambda role to open the role in the IAM Service page: Click on Attach policies : Click on Create policy : Create a policy to allow object write to your S3 bucket created, and click Review policy : Name the policy s3_pricing_lambda , optionally and add a description, click Create policy : Click on Roles : Click on the role SPTool_Lambda : Click Attach policies : Click Filter Policies and select Customer managed : Select the S3_pricing_lambda policy and click Attach policy : Go back to the Lambda service page and click Test : The function will run, it will take a minute or two given the size of the pricing files and processing required, then return success. Click Details and verify there is headroom in the configured resources and duration to allow any increases in pricing file size over time: Go to your S3 bucket and into the od_pricedata folder and you should see a gz file of non-zero size is in it:","title":"1.2. Setup On-Demand Pricing Lambda Function"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/Lab_Guide.html#13-setup-savings-plan-pricing-lambda-function","text":"Create the Savings Plan Lambda function to get the pricing information, and extract the required parts from it. Go to the Lambda service page: Click Create function : Enter the following details: Select: Author from scratch Function name: SPTool_SPPricing_Download Runtime: Python (Latest) Execution Role: Use an existing role Role name: SPTool_Lambda Click Create function Copy and paste the following code into the Function code section: Click here to see the function code # Lambda Function Code - SPTool_SP_pricing_Download # Function to download SavingsPlans pricing, get out the required lines & upload it to S3 as a zipped file # It will get each regions pricing file in CSV, find 'Usage' and '1yr', and write to a file # Written by natbesh@amazon.com # Please reachout to costoptimization@amazon.com if there's any comments or suggestions import boto3 import gzip import urllib3 import json def lambda_handler(event, context): # Create the connection http = urllib3.PoolManager() try: # Get the SavingsPlans pricing index file, so you can get all the region files, which have the pricing in them r = http.request('GET', 'https://pricing.us-east-1.amazonaws.com/savingsPlan/v1.0/aws/AWSComputeSavingsPlan/current/region_index.json') # Load the json file into a variable, and parse it sp_regions = r.data sp_regions_json = (json.loads(sp_regions)) # Variable to hold all of the pricing data, its large at over 150MB sp_pricing_data = \"\" # Cycle through each regions pricing file, to get the data we need for region in sp_regions_json['regions']: # Get the CSV URL url = \"https://pricing.us-east-1.amazonaws.com\" + region['versionUrl'] url = url.replace('.json', '.csv') # Create a connection & get the regions pricing data CSV file http = urllib3.PoolManager() r = http.request('GET', url) spregion_content = r.data # Split the lines into an array spregion_lines = spregion_content.splitlines() # Go through each of the pricing lines for line in spregion_lines: # If the line has 'Usage' then grab it for pricing data, exclude all others if (str(line).find('Usage') != -1): sp_pricing_data += str(line.decode(\"utf-8\")) sp_pricing_data += \"\\n\" # Compress the text into a local temporary file with gzip.open('/tmp/sp_pricedata.txt.gz', 'wb') as f: f.write(sp_pricing_data.encode()) # Upload the file to S3 s3 = boto3.resource('s3') # Specify the local file, the bucket, and the folder and object name - you MUST have a folder and object name s3.meta.client.upload_file('/tmp/sp_pricedata.txt.gz', 'bucket_name', 'sp_pricedata/sp_pricedata.txt.gz') # Die if you cant get the file except Exception as e: print(e) raise e # Return happy return { 'statusCode': 200 } Edit the pasted code, replacing bucket_name with the name of your bucket: Edit Basic settings below: Memory: 2688MB Timeout: 5min Click save Scroll to the top and click Test Enter an Event name of Test , click Create : Click Save : Click Test : The function will run, it will take a minute or two given the size of the pricing files and processing required, then return success. Click Details and verify there is headroom in the configured resources and duration to allow any increases in pricing file size over time: Go to your S3 bucket and into the sp_pricedata folder and you should see a gz file of non-zero size is in it:","title":"1.3. Setup Savings Plan Pricing Lambda Function"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/Lab_Guide.html#14-cloudwatch-events-setup","text":"We will setup a CloudWatch Event to periodically run the Lambda functions, this will update the pricing and include any newly released instances. Go to the CloudWatch service page: Click on Events , then click Rules : Click Create rule For the Event Source select Schedule and set the required period, we have selected 5 days , click Add target : Add the SPTool_ODPricing_Download Lambda function, and click Add target : Add the SPTool_SPPricing_Download Lambda function, and click Configure details : Add the name SPTool-pricing , optionally add a description and click Create rule : You have now successfully configured a CloudWatch event, it will run the two Lambda functions and update the pricing information every 5 days.","title":"1.4 CloudWatch Events Setup"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/Lab_Guide.html#15-prepare-the-pricing-data-source","text":"We will prepare a pricing data source which we will use to join with the CUR. In this example we will take 1 year No Upfront Savings Plans rates and join them to On-Demand pricing. You can modify this part to select 3 year or Partial or All-Upfront rates. Go to the Glue Service page: Click Crawlers from the left menu: Click Add crawler : Enter a crawler name of OD_Pricing and click Next : Ensure Data stores is the source type, click Next : Click the folder icon to list the S3 folders in your account: Expand the bucket which contains your pricing folders, and select the folder name od_pricedata , click Select : Click Next : Click Next : Create an IAM role with a name of SPToolPricing , click Next : Leave the frequency as Run on demand , and click Next : Click on Add database : Enter a database name of pricing , and click Create : Click Next : Click Finish : Select the crawler OD_Pricing and click Run crawler : Once its run, you should see tables created: Repeat Steps 3 through to 17 with the following details: Crawler name: SP_Pricing Include path: s3://(pricing bucket)/sp_pricedata (replace pricing bucket) IAM Role: Choose an existing IAM role and AWSGlueServiceRole-SPToolPricing Database: pricing Open the IAM Console in a new tab, click Policies : Click on the AWSGlueServiceRole-SPToolPricing role: Type in SPTool and click on the policy name AWSGlueServiceRole-SPTool : Click Edit policy : Click JSON : Edit the Resource line by removing the OD_Pricing folder to leave the bucket: Click Review policy : Click Save changes : Go back to the Glue console, select the SP_Pricing crawler, click Run crawler : Click on Databases : Click on Pricing : Click Tables in pricing : Click od_pricedata : 32.Click Edit schema : Click double next to col9 : Select string and click Update : Click Save :","title":"1.5 Prepare the Pricing Data Source"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/Lab_Guide.html#2-create-the-usage-data-source","text":"We will combine the pricing information with our Cost and Usage Report (CUR). This will give us a usage data source which contains a summary of your usage at an hourly level, with multiple pricing dimensions. Go to the Athena service page: Run the following query to create a single pricing data source, combining the OD and SP pricing: Click here to see the Athena SQL code CREATE VIEW pricing.pricing AS SELECT sp.location AS Region, sp.discountedoperation AS OS, REPLACE(od.col18, '\"') AS InstanceType, REPLACE(od.col35, '\"') AS Tenancy, REPLACE(od.col9, '\"') AS ODRate, sp.discountedrate AS SPRate FROM pricing.sp_pricedata sp JOIN pricing.od_pricedata od ON ((sp.discountedusagetype = REPLACE(od.col46, '\"')) AND (sp.discountedoperation = REPLACE(od.col47, '\"'))) WHERE od.col9 IS NOT NULL AND sp.location NOT LIKE 'Any' AND sp.purchaseoption LIKE 'No Upfront' AND sp.leasecontractlength = 1 Next we'll join the CUR file with that pricing source as a view. Edit the following query, replace cur.curfile with your existing database name and tablename of your CUR, then run the rollowing query: Click here to see the Athena SQL code CREATE VIEW cur.SP_Usage AS SELECT cur.line_item_usage_account_id, cur.line_item_usage_start_date, to_unixtime(cur.line_item_usage_start_date) AS EpochTime, cur.product_instance_type, cur.product_location, cur.product_operating_system, cur.product_tenancy, SUM(cur.line_item_unblended_cost) AS ODPrice, SUM(cur.line_item_unblended_cost*(cast(pr.SPRate AS double)/cast(pr.ODRate AS double))) SPPrice, abs(SUM(cast(pr.SPRate AS double)) - SUM (cast(pr.ODRate AS double))) / SUM(cast(pr.ODRate AS double))*100 AS DiscountRate, SUM(cur.line_item_usage_amount) AS InstanceCount FROM cur.curfile cur JOIN pricing.pricing pr ON (cur.product_location = pr.Region) AND (cur.line_item_operation = pr.OS) AND (cur.product_instance_type = pr.InstanceType) AND (cur.product_tenancy = pr.Tenancy) WHERE cur.line_item_product_code LIKE '%EC2%' AND cur.product_instance_type NOT LIKE '' AND cur.product_operating_system NOT LIKE 'NA' AND cur.line_item_unblended_cost > 0 GROUP BY cur.line_item_usage_account_id, cur.line_item_usage_start_date, cur.product_instance_type, cur.product_location, cur.product_operating_system, cur.product_tenancy ORDER BY cur.line_item_usage_start_date ASC, DiscountRate DESC Verify the data source is setup by editing the following query, replace cur. with the name of the database and run the following query: SELECT * FROM cur.sp_usage limit 10; You now have your usage data source setup with your pricing dimensions. You can modify the queries above to add or remove any columns you want in the view, which can later be used to visualize the data, for example tags.","title":"2. Create the Usage Data Source"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/Lab_Guide.html#3-setup-quicksight-dashboard","text":"We will now setup the QuickSight dashboard to visualize your usage by cost, and setup the analysis to provide Savings Plan recommendations. Go to the QuickSight service: Click on your username in the top right: Click Manage QuickSight : Click Security & permissions , then click Add or remove : Click Details next to Amazon S3: Click Select S3 buckets : Select your pricing bucket and click Finish : Scroll down and click Update : Click on QuickSight to go to the home page: Click on Manage data : Click on New data set : Click Athena : Enter a Data source name of SP_Usage and click Create data source : Select the cur database, and the sp_usage table, click Select : Ensure SPICE is selected, click Visualize : Click on QuickSight to go to the home page: Click on Manage data : Select the sp_usage Dataset: Click Schedule refresh : Click Create : Enter a schedule, it needs to be refreshed daily, and click Create : Click Cancel to exit: Click the x in the top corner:","title":"3. Setup QuickSight Dashboard"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/Lab_Guide.html#4-create-the-recommendation-dashboard","text":"Go to the QuickSight service homepage: Go to the sp_usage analysis : Create a line chart, add line_item_usage_start_date to the X axis , aggregate day . Add spprice to the Value and set the aggregate to min . Drag the product_instance_type to Colour field well. Change the title to Usage in Savings Plan Rates : Click Parameters , and click Create one : Parameter name OperatingSystem , Data type String , click Set a dynamic default : Select your dataset, and select product_operating_system for the columns, click Apply: Click Create : Click Control : Enter OperatingSystem as the display name, style Single select drop down , values Link to a data set field , dataset your data set , column product_operating_system , click Add : Using the process above, Add the parameter Region : Name: Region Data type: String Values: Single value Dyanmic default Dataset: your dataset, product_location, product_location Add as: Control Control Display Name: Region Style: Single select drop down Values: link to data set field Data set: your data set Column: product_location Using the process above, Add the parameter Tenancy : Name: Tenancy Data type: String Values: Single value Dyanmic default Dataset: your dataset, product_tenancy, product_tenancy Add as: Control Control Display Name: Tenancy Style: Single select drop down Values: link to data set field Data set: your data set Column: product_tenancy Create an InstanceType parameter, datatype String , Single value , Static default value of . (a full stop): Click Control , Display name InstanceType , style Text box , click Add : Click Filter and click Create one , select product_instance_type : Edit the filter, Filter type: All visuals Custom filter Contains Use Parameters InstanceType click Apply : Create a Parameter DaysToDisplay : Name: DaysToDisplay Data type: Integer Values: Single value Static default value: 90 Click Create : Click Control : Enter a Display name DaysToDisplay , Style Text box and click Add : Click on Filter , click + , and select line_item_usage_start_date : Click on the new filter: Select a filter type of: All visuals Relative dates Days Last N days select Use parameters , and accept to change the scope of the filter select the parameter DaysToDisplay click Apply : Create a filter for product_operating_system : All visuals Type: Custom filter equals Use parameters, change the scope of this filter: yes Parameter: OperatingSystem Create a filter for product_location : All visuals Type: Custom filter equals Use parameters, change the scope of this filter: yes Parameter: Region Create a filter for product_tenancy : All visuals Type: Custom filter equals Use parameters, change the scope of this filter: yes Parameter: Tenancy Click on Visualize , click Add , select Add calculated field : Field name HoursDisplayed , add the formula below and click Create : distinct_count({line_item_usage_start_date}) Create a calculated field HoursRun , formula: HoursDisplayed / (${DaysToDisplay} * 24) Create a calculated field PayOffMonth , formula: ifelse(((((sum(spprice) / HoursDisplayed) * 730 * 12) / ((sum(odprice) / (${DaysToDisplay} * 24)) * 730))) < 12,((((sum(spprice) / HoursDisplayed) * 730 * 12) / ((sum(odprice) / (${DaysToDisplay} * 24)) * 730))),12) Create a calculated field SavingsPlanReco , formula: ifelse(PayOffMonth < 12,percentile(spprice,10),0.00) Create a calculated field StartSPPrice , formula: lag(min(spprice),[{line_item_usage_start_date} ASC],${DaysToDisplay} - 2,[{product_instance_type}]) Create a calculated field Trend , formula: (min(spprice) - {StartSPPrice}) / min(spprice) Create a calculated field First3QtrAvg , formula: windowAvg(avg(spprice),[{line_item_usage_start_date} ASC],${DaysToDisplay},${DaysToDisplay} / 4,[{product_instance_type}]) Create a calculated field LastQtrAvg , formula: windowAvg(avg(spprice),[{line_item_usage_start_date} ASC],${DaysToDisplay} / 4,1,[{product_instance_type}]) Create a calculated field TrendAvg , formula: (LastQtrAvg- First3QtrAvg) / First3QtrAvg Add a Visual, click Add , select Add visual : Select a Table visualization, Group by product_instance_type , Add the values : SavingsPlanReco PayOffMonth discountrate, aggregate: average HoursRun, show as percent Label it Recommendations Add a Table visual, Group By: product_instance_type and line_item_usage_start_date aggreate: day , Add the values : instancecount aggregate: average Trend TrendAvg (show as percent) Label it Trends , Add a filter to this visual only : Filter on: StartSPPrice Type: Custom filter Operation: Greater than Value: -1 Nulls: Exclude nulls Decrease the width of the date column as much as possible, its not needed","title":"4. Create the Recommendation Dashboard"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/Lab_Guide.html#5-format-the-recommendation-dashboard","text":"We will format the recommendation dashboard, this will improve its appearance, and also includes some business rules. Click on Themes , then click on Midnight : Select the Recommendations table, click the three dots , click Conditional formatting : Column: PayOffMonth , Add background color : Enter the formatting: Condition: Greater than Value: 9 Color: red Click Apply , click Close : Using the same process, add formatting for the column discountrate : Type: Background color Condition: Less than Value: 10 NOTE adjust this for your business rules, speak with your finance teams Color: red Click Apply , click Close Under the discountrate formatting, Click Add text color : Condition: Greater than Value: 20 Color: Green Click Apply , click Close Using the same process, add formatting for the column HoursRun : Type: Add text color Condition: Less than Value: 0.6 Color: Red Click Add condition Condition#2: Less than Value: 0.85 Color: Orange Click Apply , click Close Add formatting for the column SavingsPlanReco : Type: Add background color Format field based on: PayOffMonth Condition: Greater than Value: 9 Color: Red Click Apply , click Close Click Add text color Format field based on: discountrate Aggregation: Average Condition: greater than Value: 20 Color: Green Click Apply , click Close Click SavingsPlanReco , Sort by Descending : Select the Trends table, select conditional formatting, Column instancecount : Type: Add background color Condition: Less than Value: 5, speak with your team to set this at the appropriate level Color: red Click Add condition Condition #2: Less than Value: 10 Color: Orange Click Apply , click Close Using the process above on the Trends table, Select the Trend column: Type: Add background color Condition: Less than Value 0 Color: Red Click Apply , click Close Click Add text color Condition: Greater than Value: 0 Color: Green Click Apply , Click Close Add the same formatting to the TrendAvg column as the Trend column. Congratulations - you now have an analytics dashboard for Savings Plan recommendations!","title":"5. Format the Recommendation Dashboard"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/Lab_Guide.html#6-teardown","text":"Savings Plan analysis is a critical requirement of cost optimization, so there is no tear down for this lab. The following resources were created in this lab: S3 Bucket: (custom name) Lambda Functions: SPTool_ODPricing_Download and SPTool_SPPricing_Download IAM Role: SPTool_Lambda IAM Policy: s3_pricing_lambda CloudWatch Event, Rule: SPTool-Pricing Glue Crawlers: OD_Pricing and SP_Pricing IAM Role: AWSGlueServiceRole-SPToolPricing Glue Database: Pricing Athena Views: pricing.pricing and cur.SP_USage QuickSight Permissions: your pricing S3 bucket QuickSight Dataset: SP_Usage QuickSight Analysis: sp_usage analysis","title":"6. Teardown"},{"location":"Cost/Cost_Effective_Resources/200_Pricing_Model_Analysis/Lab_Guide.html#7-rate-this-lab","text":"","title":"7. Rate this lab"},{"location":"Cost/Cost_Fundamentals/README.html","text":"Cost Optimization Fundamentals http://wellarchitectedlabs.com About cost optimization fundamentals The first step in your Cost Optimization journey is to setup your account correctly, and get to know the tools and data available for Cost Optimization. These are a collection of labs that are accessible to anyone that will be working with the cloud, including non-technical roles. Step 1 - Account Setup This first step will help you to you build a basic account structure, and make sure your account is configured correctly. This will ensure you are collecting data for cost optimization, and this data is accessible to the right people withing your organization. This is a 100 level lab which requires root access. It must be completed for each AWS account in your organization. Step 2 - Cost and Usage Governance - Notifications Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend. Step 3 - Pricing Models By using the right pricing model for your workload resources, you pay the lowest price for that resource. 100 Level Lab : This lab will introduce you to working with Savings Plans (SP's), utilizing AWS Cost Explorer to make low risk, high return SP purchases for your business. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business. Step 4 - Monitor Usage and Cost - Analysis Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts. Step 5 - Monitor Usage and Cost - Visualization Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights. Step 6 - Govern Usage and Cost - Controls Implementing usage controls will ensure excess usage and accompanying costs does not occur. 200 Level Lab : This lab will extend the permissions of the Cost Optimization team, then utilize Identity and Access Management (IAM) policies to control and restrict usage. Step 7 - Monitor Usage and Cost - Advanced Analysis Advanced analysis using your Cost and Usage Report (CUR) will allow you to answer the most challenging questions on your usage and cost. It is the most detailed source of information on your cost and usage available. 200 Level Lab : This lab will utilize Amazon Athena to provide an interface to query the CUR, provide you the most common customer queries, and help you to build your own queries. Step 8 - Monitor Usage and Cost - Advanced Visualization Utilizing the CUR data source in the previous step, you can provide more detailed and custom visualizations and dashboards. 200 Level Lab : This Lab extends the previous step, utilizing Amazon Quicksight to visualize the CUR data source.","title":"Cost Optimization Fundamentals"},{"location":"Cost/Cost_Fundamentals/README.html#cost-optimization-fundamentals","text":"http://wellarchitectedlabs.com","title":"Cost Optimization Fundamentals"},{"location":"Cost/Cost_Fundamentals/README.html#about-cost-optimization-fundamentals","text":"The first step in your Cost Optimization journey is to setup your account correctly, and get to know the tools and data available for Cost Optimization. These are a collection of labs that are accessible to anyone that will be working with the cloud, including non-technical roles.","title":"About cost optimization fundamentals"},{"location":"Cost/Cost_Fundamentals/README.html#step-1-account-setup","text":"This first step will help you to you build a basic account structure, and make sure your account is configured correctly. This will ensure you are collecting data for cost optimization, and this data is accessible to the right people withing your organization. This is a 100 level lab which requires root access. It must be completed for each AWS account in your organization.","title":"Step 1 - Account Setup"},{"location":"Cost/Cost_Fundamentals/README.html#step-2-cost-and-usage-governance-notifications","text":"Configuring notifications allows you to receive an email when usage or cost is above a defined amount. 100 Level Lab : This lab will show you how to implement AWS Budgets to provide notifications on usage and spend.","title":"Step 2 - Cost and Usage Governance - Notifications"},{"location":"Cost/Cost_Fundamentals/README.html#step-3-pricing-models","text":"By using the right pricing model for your workload resources, you pay the lowest price for that resource. 100 Level Lab : This lab will introduce you to working with Savings Plans (SP's), utilizing AWS Cost Explorer to make low risk, high return SP purchases for your business. 200 Level Lab : This lab will introduce you to working with Reserved Instances (RI's), utilizing AWS Cost Explorer to make low risk, high return RI purchases for your business.","title":"Step 3 - Pricing Models"},{"location":"Cost/Cost_Fundamentals/README.html#step-4-monitor-usage-and-cost-analysis","text":"Cost and Usage Analysis will enable you to understand how you consumed the cloud, and what your costs are for that consumption. 100 Level Lab : This lab introduces you to the billing console, allowing you to view your current and past bills, and also inspect your usage across services and accounts.","title":"Step 4 - Monitor Usage and Cost - Analysis"},{"location":"Cost/Cost_Fundamentals/README.html#step-5-monitor-usage-and-cost-visualization","text":"Visualizing cost and usage highlights trends and allows you to gain further insights. 100 Level Lab : This lab will introduce AWS Cost Explorer, and demonstrate how to use its features to provide insights.","title":"Step 5 - Monitor Usage and Cost - Visualization"},{"location":"Cost/Cost_Fundamentals/README.html#step-6-govern-usage-and-cost-controls","text":"Implementing usage controls will ensure excess usage and accompanying costs does not occur. 200 Level Lab : This lab will extend the permissions of the Cost Optimization team, then utilize Identity and Access Management (IAM) policies to control and restrict usage.","title":"Step 6 - Govern Usage and Cost - Controls"},{"location":"Cost/Cost_Fundamentals/README.html#step-7-monitor-usage-and-cost-advanced-analysis","text":"Advanced analysis using your Cost and Usage Report (CUR) will allow you to answer the most challenging questions on your usage and cost. It is the most detailed source of information on your cost and usage available. 200 Level Lab : This lab will utilize Amazon Athena to provide an interface to query the CUR, provide you the most common customer queries, and help you to build your own queries.","title":"Step 7 - Monitor Usage and Cost - Advanced Analysis"},{"location":"Cost/Cost_Fundamentals/README.html#step-8-monitor-usage-and-cost-advanced-visualization","text":"Utilizing the CUR data source in the previous step, you can provide more detailed and custom visualizations and dashboards. 200 Level Lab : This Lab extends the previous step, utilizing Amazon Quicksight to visualize the CUR data source.","title":"Step 8 - Monitor Usage and Cost - Advanced Visualization"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html","text":"Level 100: AWS Account Setup https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to create and setup an initial account structure, and enable access to billing reports. This will ensure that you can complete the Well-Architected Cost workshops, and enable you to optimize your workloads inline with the Well-Architected Framework. Goals Implement an account structure Configure billing services Prerequisites Multiple AWS accounts (at least two) Root user access to the master account Permissions required Root user access to the master account ./Code/master_policy IAM policy required for Master account user ./Code/member_policy IAM policy required for Member account user ./Code/IAM_policy IAM policy required to create the cost optimization team NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Costs https://aws.amazon.com/aws-cost-management/pricing/ Variable costs will be incurred Cost Explorer: $0.01 per 1,000 usage records S3: Storage of CUR file, refer to S3 pricing https://aws.amazon.com/s3/pricing/ Time to complete The lab should take approximately 15 minutes to complete Best Practice Checklist [ ] Create a basic account structure, with a master (payer) account and at least 1 member (linked) account [ ] Configure account parameters [ ] Configure IAM access to billing information [ ] Configure a Cost and Usage Report (CUR) [ ] Enable AWS Cost Explorer [ ] Enable AWS-Generated Cost Allocation Tags [ ] Create a cost optimization team, to manage cost optimization across your organization License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#level-100-aws-account-setup","text":"https://wellarchitectedlabs.com","title":"Level 100: AWS Account Setup"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#introduction","text":"This hands-on lab will guide you through the steps to create and setup an initial account structure, and enable access to billing reports. This will ensure that you can complete the Well-Architected Cost workshops, and enable you to optimize your workloads inline with the Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#goals","text":"Implement an account structure Configure billing services","title":"Goals"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#prerequisites","text":"Multiple AWS accounts (at least two) Root user access to the master account","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#permissions-required","text":"Root user access to the master account ./Code/master_policy IAM policy required for Master account user ./Code/member_policy IAM policy required for Member account user ./Code/IAM_policy IAM policy required to create the cost optimization team NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#costs","text":"https://aws.amazon.com/aws-cost-management/pricing/ Variable costs will be incurred Cost Explorer: $0.01 per 1,000 usage records S3: Storage of CUR file, refer to S3 pricing https://aws.amazon.com/s3/pricing/","title":"Costs"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#time-to-complete","text":"The lab should take approximately 15 minutes to complete","title":"Time to complete"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#best-practice-checklist","text":"[ ] Create a basic account structure, with a master (payer) account and at least 1 member (linked) account [ ] Configure account parameters [ ] Configure IAM access to billing information [ ] Configure a Cost and Usage Report (CUR) [ ] Enable AWS Cost Explorer [ ] Enable AWS-Generated Cost Allocation Tags [ ] Create a cost optimization team, to manage cost optimization across your organization","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html","text":"Level 100: AWS Account Setup: Lab Guide Authors Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Configure IAM access Create an account structure Configure account settings Configure Cost and Usage reports Enable AWS Cost Explorer Enable AWS-Generated Cost Allocation Tags Create a cost optimization team Tear down Rate this Lab 1. Configure IAM access to your billing NOTE : You will need to sign into the account with root account credentials to perform this action. You need to enter in the account email and password for root access. You need to enable IAM access to your billing so the correct IAM users can access the information. This allows other users (non-root) to access billing information in the master account. It is also required if you wish for member accounts to see their usage and billing information. This step will not provide access to the information, that is configured through IAM policies. Log in to your Master account as the root user, Click on the account name in the top right, and click on My Account from the menu: Scroll down to IAM User and Role Access to Billing Information , and click Edit : Select Activate IAM Access and click on Update : Confirm that IAM user/role access to billing information is activated : You will now be able to provide access to non-root users to billing information via IAM policies. NOTE: Logout as the root user before continuing. 2. Create an account structure NOTE : Do NOT do this step if you already have an organization and consolidated billing setup. You will create an AWS Organization, and join one or more accounts to the master account. An organization will allow you to centrally manage multiple AWS accounts efficiently and consistently. It is recommended to have a master account that is primarily used for billing and does not contain any resources, all resources and workloads will reside in the member accounts. You will need organizations:CreateOrganization access, and 2 or more AWS accounts. When you create a new master account, it will contain all billing information for member accounts, member accounts will no longer have any billing information, including historical billing information. Ensure you backup or export any reports or data. 2.1 Create an AWS Organization You will create an AWS Organization with the master account. Login to the AWS console as an IAM user with the required permissions, start typing AWS Organizations into the Find Services box and click on AWS Organizations : Click on Create organization : To create a fully featured organization, Click on Create organization You will receive a verification email, click on Verify your email address to verify your account: You will then see a verification message in the console for your organization: You now have an organization that you can join other accounts to. 2.2 Join member accounts You will now join other accounts to your organization. From the AWS Organizations console click on Add account : Click on Invite account : Enter in the Email or account ID , enter in any relevant Notes and click Invite : You will then have an open request: Log in to your member account , and go to AWS Organizations : You will see an invitation in the menu, click on Invitations : Verify the details in the request (they are blacked out here), and click on Accept : Verify the Organization ID (blacked out here), and click Confirm : You are shown that the account is now part of your organization: The member account will receive an email showing success: The master account will also receive email notification of success: Repeat the steps above (exercise 1.2) for each additional account in your organization. 3. Configure billing account settings It is important to ensure your account contacts are up to date and correct. This allows AWS to be able to contact the correct people in your organization if required. It is recommended to use a mailing list or shared email that is accessible by multiple team members for redudancy. Ensure the email accounts are actively monitored. Log in to your Master account as an IAM user with the required permissions, Click on the account name in the top right, and click on My Account from the menu: Scroll down to Alternate Contacts and click on Edit : Enter information into each of the fields for Billing , Operations and Security , and click Update : 4. Configure Cost and Usage Reports Cost and Usage Reports provide the most detailed information on your usage and bills. They can be configured to deliver 1 line per resource, for every hour of the day. They must be configured to enable you to access and analyze your usage and billing information. This will allow you to make modifications to your usage, and make your applications more efficient. 4.1 Configure a Cost and Usage Report If you configure multiple Cost and Usage Reports (CURs), then it is recommended to have 1 CUR per bucket. If you must have multiple CURs in a single bucket, ensure you use a different report path prefix so it is clear they are different reports. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost & Usage Reports from the left menu: Click on Create report : Enter a Report name (it can be any name), ensure you have selected Include resource IDs and Data refresh settings , then click on Next : Click on Configure : Enter a unique bucket name, and ensure the region is correct, click Next : Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct , then click Save : Ensure your bucket is a Valid Bucket (if not, verify the bucket policy). Enter a Report path prefix (it can be any word) without any '/' characters, ensure the Time Granularity is Hourly , Report Versioning is set to Overwrite existing report , under Enable report data integration for select Amazon Athena , and click Next : Review the configuration, scroll to the bottom and click on Review and Complete : You have successfully configured a Cost and Usage Report to be delivered. It may take up to 24hrs for the first report to be delivered. 4.2 Enable monthly billing report The monthly billing report contains estimated AWS charges for the month. It contains line items for each unique combination of AWS product, usage type, and operation that the account uses. NOTE : Billing files will only be delivered from the current month onwards. It will not generate previous months billing files. Go to the billing console: Click on Billing preferences from the left menu: Scroll down, and click on Receive Billing Reports , then click on Configure : From the left dropdown, select your S3 billing bucket configured above: Click on Next : Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct , then click Save : Ensure only Monthly report is selected, and uncheck all other boxes. Click on Save preferences : 5. Enable AWS Cost Explorer AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. You must enable it before you can use it within your accounts. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Explorer from the left menu: Click on Enable Cost Explorer : You will receive notification that Cost Explorer has been enabled, and data will be populated: Go into Cost Explorer : Click Settings in the top right: Select Hourly and Resource Level Data , and click Save : NOTE : This will incur costs depending on the number of EC2 resources you are running. 6. Enable AWS-Generated Cost Allocation Tags Enabling AWS-Generated Cost Allocation Tags, generates a cost allocation tag containing resource creator information that is automatically applied to resources that are created within your account. This allows you to view and allocate costs based on who created a resource. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Allocation Tags from the left menu: Click on Activate to enable the tags: You will see that it is activated: 7. Create a cost optimization team We are going to create a cost optimization team within your master/payer account - which is where the billing information is. Within your organization there needs to be a team of people that are focused around costs and usage. This exercise will create the users and the group, then assign all the access they need. This team will then be able to manage the organizations cost and usage, and start to implement optimization mechanisms. NOTE : Review the IAM policy below with your security team, the permissions below are required for completion of the Fundamentals series of labs. Verify if they need to be changed for your organization. Log into the console as an IAM user with the required permissions, as per: - ./Code/IAM_policy IAM policy required for this lab 7.1 Create an IAM policy for the team This provides access to allow the cost optimization team to perform their work, namely the Labs in the 100 level fundamental series. This is the minimum access the team requires. 1 - Log in and go to the IAM Service page: 2 - Select Policies from the left menu: 3 - Select Create Policy : 4 - Select the JSON tab: 5 - Edit the policy below, replacing the billing bucket with what you previously configured. Then Copy & paste the following policy into the field: NOTE : Ensure you copy the entire policy, everything including the first '{' and last '}' { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:ListBucket\" ], \"Resource\": [ \"arn:aws:s3:::-billing bucket-\", \"arn:aws:s3:::-billing bucket-/*\" ] }, { \"Sid\": \"VisualEditor1\", \"Effect\": \"Allow\", \"Action\": [ \"iam:GetPolicyVersion\", \"quicksight:CreateAdmin\", \"iam:DeletePolicy\", \"iam:CreateRole\", \"iam:AttachRolePolicy\", \"aws-portal:ViewUsage\", \"iam:GetGroup\", \"aws-portal:ModifyBilling\", \"iam:DetachRolePolicy\", \"iam:ListAttachedRolePolicies\", \"ds:UnauthorizeApplication\", \"aws-portal:ViewBilling\", \"iam:DetachGroupPolicy\", \"iam:ListAttachedGroupPolicies\", \"iam:CreatePolicyVersion\", \"ds:CheckAlias\", \"quicksight:Subscribe\", \"ds:DeleteDirectory\", \"iam:ListPolicies\", \"iam:GetRole\", \"ds:CreateIdentityPoolDirectory\", \"ds:DescribeTrusts\", \"iam:GetPolicy\", \"iam:ListGroupPolicies\", \"aws-portal:ViewAccount\", \"iam:ListEntitiesForPolicy\", \"iam:AttachUserPolicy\", \"iam:ListRoles\", \"iam:DeleteRole\", \"budgets:*\", \"iam:CreatePolicy\", \"quicksight:CreateUser\", \"s3:ListAllMyBuckets\", \"iam:ListPolicyVersions\", \"iam:AttachGroupPolicy\", \"quicksight:Unsubscribe\", \"iam:ListAccountAliases\", \"ds:DescribeDirectories\", \"iam:ListGroups\", \"iam:GetGroupPolicy\", \"ds:CreateAlias\", \"ds:AuthorizeApplication\", \"iam:DeletePolicyVersion\" ], \"Resource\": \"*\" } ] } 6 - Click Review policy : 7 - Enter a Name and Description for the policy and click Create policy : You have successfully created the cost optimization teams policy. 7.2 Create an IAM Group This group will bring together IAM users and apply the required policies. 1 - While in the IAM console, select Groups from the left menu: 2 - Click on Create New Group : 3 - Enter a Group Name and click Next Step : 4 - Click Policy Type and select Customer Managed : 5 - Select the CostOptimization_Summit policy (created previously): 6 - Click Create Group : You have now successfully created the cost optimization group, and attached the required policies. 7.3 Create an IAM User For this lab we will create a user and join them to the group above. 1 - In the IAM console, select Users from the left menu: 2 - Click Add user : 3 - Enter a User name , select AWS Management Console access , choose Custom Password , type a suitable password, deselect Require password reset , and click Next: Permissions : 4 - Select the CostOptimization group (created previously), and click Next: Tags : 5 - Click Next Review : 6 - Click Create user : 7 - Copy the link provided, and logout by clicking on your username in the top right, and selecting Sign Out :: 8 - Log back in as the username you just created, with the link you copied for the remainder of the Lab. You have successfully created a user, placed them in the cost optimization group and have applied policies. You can continue to expand this group by adding additional users from your organization. 8. Tear down This exercise covered fundamental steps that are recommended for all AWS accounts to enable Cost Optimization. There is no tear down for exercises in this lab. Ensure you remove the IAM policies from the users/groups if they were used. 9. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#level-100-aws-account-setup-lab-guide","text":"","title":"Level 100: AWS Account Setup: Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect","title":"Authors"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#table-of-contents","text":"Configure IAM access Create an account structure Configure account settings Configure Cost and Usage reports Enable AWS Cost Explorer Enable AWS-Generated Cost Allocation Tags Create a cost optimization team Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#1-configure-iam-access-to-your-billing","text":"NOTE : You will need to sign into the account with root account credentials to perform this action. You need to enter in the account email and password for root access. You need to enable IAM access to your billing so the correct IAM users can access the information. This allows other users (non-root) to access billing information in the master account. It is also required if you wish for member accounts to see their usage and billing information. This step will not provide access to the information, that is configured through IAM policies. Log in to your Master account as the root user, Click on the account name in the top right, and click on My Account from the menu: Scroll down to IAM User and Role Access to Billing Information , and click Edit : Select Activate IAM Access and click on Update : Confirm that IAM user/role access to billing information is activated : You will now be able to provide access to non-root users to billing information via IAM policies. NOTE: Logout as the root user before continuing.","title":"1. Configure IAM access to your billing"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#2-create-an-account-structure","text":"NOTE : Do NOT do this step if you already have an organization and consolidated billing setup. You will create an AWS Organization, and join one or more accounts to the master account. An organization will allow you to centrally manage multiple AWS accounts efficiently and consistently. It is recommended to have a master account that is primarily used for billing and does not contain any resources, all resources and workloads will reside in the member accounts. You will need organizations:CreateOrganization access, and 2 or more AWS accounts. When you create a new master account, it will contain all billing information for member accounts, member accounts will no longer have any billing information, including historical billing information. Ensure you backup or export any reports or data.","title":"2. Create an account structure"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#21-create-an-aws-organization","text":"You will create an AWS Organization with the master account. Login to the AWS console as an IAM user with the required permissions, start typing AWS Organizations into the Find Services box and click on AWS Organizations : Click on Create organization : To create a fully featured organization, Click on Create organization You will receive a verification email, click on Verify your email address to verify your account: You will then see a verification message in the console for your organization: You now have an organization that you can join other accounts to.","title":"2.1 Create an AWS Organization"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#22-join-member-accounts","text":"You will now join other accounts to your organization. From the AWS Organizations console click on Add account : Click on Invite account : Enter in the Email or account ID , enter in any relevant Notes and click Invite : You will then have an open request: Log in to your member account , and go to AWS Organizations : You will see an invitation in the menu, click on Invitations : Verify the details in the request (they are blacked out here), and click on Accept : Verify the Organization ID (blacked out here), and click Confirm : You are shown that the account is now part of your organization: The member account will receive an email showing success: The master account will also receive email notification of success: Repeat the steps above (exercise 1.2) for each additional account in your organization.","title":"2.2 Join member accounts"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#3-configure-billing-account-settings","text":"It is important to ensure your account contacts are up to date and correct. This allows AWS to be able to contact the correct people in your organization if required. It is recommended to use a mailing list or shared email that is accessible by multiple team members for redudancy. Ensure the email accounts are actively monitored. Log in to your Master account as an IAM user with the required permissions, Click on the account name in the top right, and click on My Account from the menu: Scroll down to Alternate Contacts and click on Edit : Enter information into each of the fields for Billing , Operations and Security , and click Update :","title":"3. Configure billing account settings"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#4-configure-cost-and-usage-reports","text":"Cost and Usage Reports provide the most detailed information on your usage and bills. They can be configured to deliver 1 line per resource, for every hour of the day. They must be configured to enable you to access and analyze your usage and billing information. This will allow you to make modifications to your usage, and make your applications more efficient.","title":"4. Configure Cost and Usage Reports"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#41-configure-a-cost-and-usage-report","text":"If you configure multiple Cost and Usage Reports (CURs), then it is recommended to have 1 CUR per bucket. If you must have multiple CURs in a single bucket, ensure you use a different report path prefix so it is clear they are different reports. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost & Usage Reports from the left menu: Click on Create report : Enter a Report name (it can be any name), ensure you have selected Include resource IDs and Data refresh settings , then click on Next : Click on Configure : Enter a unique bucket name, and ensure the region is correct, click Next : Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct , then click Save : Ensure your bucket is a Valid Bucket (if not, verify the bucket policy). Enter a Report path prefix (it can be any word) without any '/' characters, ensure the Time Granularity is Hourly , Report Versioning is set to Overwrite existing report , under Enable report data integration for select Amazon Athena , and click Next : Review the configuration, scroll to the bottom and click on Review and Complete : You have successfully configured a Cost and Usage Report to be delivered. It may take up to 24hrs for the first report to be delivered.","title":"4.1 Configure a Cost and Usage Report"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#42-enable-monthly-billing-report","text":"The monthly billing report contains estimated AWS charges for the month. It contains line items for each unique combination of AWS product, usage type, and operation that the account uses. NOTE : Billing files will only be delivered from the current month onwards. It will not generate previous months billing files. Go to the billing console: Click on Billing preferences from the left menu: Scroll down, and click on Receive Billing Reports , then click on Configure : From the left dropdown, select your S3 billing bucket configured above: Click on Next : Read and verify the policy, this will allow AWS to deliver billing reports to the bucket. Click on I have confirmed that this policy is correct , then click Save : Ensure only Monthly report is selected, and uncheck all other boxes. Click on Save preferences :","title":"4.2 Enable monthly billing report"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#5-enable-aws-cost-explorer","text":"AWS Cost Explorer has an easy-to-use interface that lets you visualize, understand, and manage your AWS costs and usage over time. You must enable it before you can use it within your accounts. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Explorer from the left menu: Click on Enable Cost Explorer : You will receive notification that Cost Explorer has been enabled, and data will be populated: Go into Cost Explorer : Click Settings in the top right: Select Hourly and Resource Level Data , and click Save : NOTE : This will incur costs depending on the number of EC2 resources you are running.","title":"5. Enable AWS Cost Explorer"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#6-enable-aws-generated-cost-allocation-tags","text":"Enabling AWS-Generated Cost Allocation Tags, generates a cost allocation tag containing resource creator information that is automatically applied to resources that are created within your account. This allows you to view and allocate costs based on who created a resource. Log in to your Master account as an IAM user with the required permissions, and go to the Billing console: Select Cost Allocation Tags from the left menu: Click on Activate to enable the tags: You will see that it is activated:","title":"6. Enable AWS-Generated Cost Allocation Tags"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#7-create-a-cost-optimization-team","text":"We are going to create a cost optimization team within your master/payer account - which is where the billing information is. Within your organization there needs to be a team of people that are focused around costs and usage. This exercise will create the users and the group, then assign all the access they need. This team will then be able to manage the organizations cost and usage, and start to implement optimization mechanisms. NOTE : Review the IAM policy below with your security team, the permissions below are required for completion of the Fundamentals series of labs. Verify if they need to be changed for your organization. Log into the console as an IAM user with the required permissions, as per: - ./Code/IAM_policy IAM policy required for this lab","title":"7. Create a cost optimization team"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#71-create-an-iam-policy-for-the-team","text":"This provides access to allow the cost optimization team to perform their work, namely the Labs in the 100 level fundamental series. This is the minimum access the team requires. 1 - Log in and go to the IAM Service page: 2 - Select Policies from the left menu: 3 - Select Create Policy : 4 - Select the JSON tab: 5 - Edit the policy below, replacing the billing bucket with what you previously configured. Then Copy & paste the following policy into the field: NOTE : Ensure you copy the entire policy, everything including the first '{' and last '}' { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:ListBucket\" ], \"Resource\": [ \"arn:aws:s3:::-billing bucket-\", \"arn:aws:s3:::-billing bucket-/*\" ] }, { \"Sid\": \"VisualEditor1\", \"Effect\": \"Allow\", \"Action\": [ \"iam:GetPolicyVersion\", \"quicksight:CreateAdmin\", \"iam:DeletePolicy\", \"iam:CreateRole\", \"iam:AttachRolePolicy\", \"aws-portal:ViewUsage\", \"iam:GetGroup\", \"aws-portal:ModifyBilling\", \"iam:DetachRolePolicy\", \"iam:ListAttachedRolePolicies\", \"ds:UnauthorizeApplication\", \"aws-portal:ViewBilling\", \"iam:DetachGroupPolicy\", \"iam:ListAttachedGroupPolicies\", \"iam:CreatePolicyVersion\", \"ds:CheckAlias\", \"quicksight:Subscribe\", \"ds:DeleteDirectory\", \"iam:ListPolicies\", \"iam:GetRole\", \"ds:CreateIdentityPoolDirectory\", \"ds:DescribeTrusts\", \"iam:GetPolicy\", \"iam:ListGroupPolicies\", \"aws-portal:ViewAccount\", \"iam:ListEntitiesForPolicy\", \"iam:AttachUserPolicy\", \"iam:ListRoles\", \"iam:DeleteRole\", \"budgets:*\", \"iam:CreatePolicy\", \"quicksight:CreateUser\", \"s3:ListAllMyBuckets\", \"iam:ListPolicyVersions\", \"iam:AttachGroupPolicy\", \"quicksight:Unsubscribe\", \"iam:ListAccountAliases\", \"ds:DescribeDirectories\", \"iam:ListGroups\", \"iam:GetGroupPolicy\", \"ds:CreateAlias\", \"ds:AuthorizeApplication\", \"iam:DeletePolicyVersion\" ], \"Resource\": \"*\" } ] } 6 - Click Review policy : 7 - Enter a Name and Description for the policy and click Create policy : You have successfully created the cost optimization teams policy.","title":"7.1 Create an IAM policy for the team"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#72-create-an-iam-group","text":"This group will bring together IAM users and apply the required policies. 1 - While in the IAM console, select Groups from the left menu: 2 - Click on Create New Group : 3 - Enter a Group Name and click Next Step : 4 - Click Policy Type and select Customer Managed : 5 - Select the CostOptimization_Summit policy (created previously): 6 - Click Create Group : You have now successfully created the cost optimization group, and attached the required policies.","title":"7.2 Create an IAM Group"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#73-create-an-iam-user","text":"For this lab we will create a user and join them to the group above. 1 - In the IAM console, select Users from the left menu: 2 - Click Add user : 3 - Enter a User name , select AWS Management Console access , choose Custom Password , type a suitable password, deselect Require password reset , and click Next: Permissions : 4 - Select the CostOptimization group (created previously), and click Next: Tags : 5 - Click Next Review : 6 - Click Create user : 7 - Copy the link provided, and logout by clicking on your username in the top right, and selecting Sign Out :: 8 - Log back in as the username you just created, with the link you copied for the remainder of the Lab. You have successfully created a user, placed them in the cost optimization group and have applied policies. You can continue to expand this group by adding additional users from your organization.","title":"7.3 Create an IAM User"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#8-tear-down","text":"This exercise covered fundamental steps that are recommended for all AWS accounts to enable Cost Optimization. There is no tear down for exercises in this lab. Ensure you remove the IAM policies from the users/groups if they were used.","title":"8. Tear down"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Lab_Guide.html#9-rate-this-lab","text":"","title":"9. Rate this lab"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Code/IAM_policy.html","text":"{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"iam:ListPolicies\", \"iam:GetPolicyVersion\", \"iam:CreateGroup\", \"iam:GetPolicy\", \"iam:DeletePolicy\", \"iam:DetachGroupPolicy\", \"iam:ListGroupPolicies\", \"iam:AttachUserPolicy\", \"iam:CreateUser\", \"iam:GetGroup\", \"iam:CreatePolicy\", \"iam:CreateLoginProfile\", \"iam:AddUserToGroup\", \"iam:ListPolicyVersions\", \"iam:AttachGroupPolicy\", \"iam:ListUsers\", \"iam:ListAttachedGroupPolicies\", \"iam:ListGroups\", \"iam:GetGroupPolicy\", \"iam:CreatePolicyVersion\", \"iam:DeletePolicyVersion\", \"iam:GetLoginProfile\" ], \"Resource\": \"*\" } ] }","title":"IAM policy"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Code/master_policy.html","text":"{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"organizations:InviteAccountToOrganization\", \"organizations:ListRoots\", \"aws-portal:ModifyAccount\", \"s3:ListBucketVersions\", \"organizations:DescribeAccount\", \"s3:CreateBucket\", \"s3:ListBucket\", \"s3:GetBucketPolicy\", \"organizations:ListChildren\", \"aws-portal:ModifyBilling\", \"organizations:ListCreateAccountStatus\", \"organizations:DescribeOrganization\", \"organizations:EnableAllFeatures\", \"aws-portal:ViewBilling\", \"organizations:DescribeHandshake\", \"s3:PutBucketVersioning\", \"organizations:DescribeCreateAccountStatus\", \"organizations:CreateOrganization\", \"s3:GetBucketPolicyStatus\", \"s3:GetBucketPublicAccessBlock\", \"s3:PutBucketPublicAccessBlock\", \"aws-portal:ViewAccount\", \"s3:GetBucketVersioning\", \"organizations:ListAWSServiceAccessForOrganization\", \"s3:DeleteBucketPolicy\", \"organizations:ListHandshakesForOrganization\", \"organizations:ListAccounts\", \"iam:CreateServiceLinkedRole\", \"s3:ListAllMyBuckets\", \"s3:PutBucketPolicy\", \"s3:GetBucketLocation\" ], \"Resource\": \"*\" } ] }","title":"Master policy"},{"location":"Cost/Cost_Fundamentals/100_1_AWS_Account_Setup/Code/member_policy.html","text":"{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"iam:CreateServiceLinkedRole\", \"organizations:AcceptHandshake\", \"organizations:DescribeHandshake\" ], \"Resource\": [ \"arn:aws:iam::*:role/*\", \"arn:aws:organizations::*:handshake/o-*/*/h-*\" ] }, { \"Sid\": \"VisualEditor1\", \"Effect\": \"Allow\", \"Action\": \"organizations:DescribeAccount\", \"Resource\": \"arn:aws:organizations::*:account/o-*/*\" }, { \"Sid\": \"VisualEditor2\", \"Effect\": \"Allow\", \"Action\": [ \"organizations:ListHandshakesForAccount\", \"organizations:DescribeOrganization\", \"organizations:DescribeCreateAccountStatus\" ], \"Resource\": \"*\" } ] }","title":"Member policy"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html","text":"Level 100: Cost and Usage Governance https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements. Goals Implement AWS Budgets to notify on usage and spend Create an AWS Budget report to notify users every week on budget status Prerequisites An AWS Account AWS Account Setup has been completed Permissions required Access to the Cost Optimization team created in AWS Account Setup Costs https://aws.amazon.com/aws-cost-management/pricing/ Less than $1 per month if the tear down is not performed Time to complete The lab should take approximately 15 minutes to complete Best Practice Checklist [ ] Create an AWS budget to notify on forecasted account cost [ ] Create an AWS budget to notify on actual cost of EC2 [ ] Create an AWS budget to notify on Savings Plan (SP) Coverage [ ] Create a weekly AWS budget report License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#level-100-cost-and-usage-governance","text":"https://wellarchitectedlabs.com","title":"Level 100: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#introduction","text":"This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#goals","text":"Implement AWS Budgets to notify on usage and spend Create an AWS Budget report to notify users every week on budget status","title":"Goals"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#prerequisites","text":"An AWS Account AWS Account Setup has been completed","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#permissions-required","text":"Access to the Cost Optimization team created in AWS Account Setup","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#costs","text":"https://aws.amazon.com/aws-cost-management/pricing/ Less than $1 per month if the tear down is not performed","title":"Costs"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#time-to-complete","text":"The lab should take approximately 15 minutes to complete","title":"Time to complete"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#best-practice-checklist","text":"[ ] Create an AWS budget to notify on forecasted account cost [ ] Create an AWS budget to notify on actual cost of EC2 [ ] Create an AWS budget to notify on Savings Plan (SP) Coverage [ ] Create a weekly AWS budget report","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html","text":"Level 100: Cost and Usage Governance Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Create an AWS Budget - monthly forecast Create an AWS Budget - EC2 actual Create an AWS Budget - SP Coverage Create an AWS Budget Report Tear down Rate this Lab 1. Create and implement an AWS Budget for monthly forecasted cost Budgets allow you to manage cost and usage by providing notifications when cost or usage are outside of configured amounts. They cannot be used to restrict actions, only notify on usage after it has occurred. Budgets and notifications are updated when your billing data is updated, which is at least once per day. NOTE : You may not receive an alarm for a forecasted budget if your account is new. Forecasting requires existing usage within the account. Create a monthly cost budget for your account We will create a monthly cost budget which will notify if the forecasted amount exceeds the budget. Log into the console as an IAM user with the required permissions, go to the Billing console : Select Budgets from the left menu: Click on Create a budget : Ensure Cost Budget is selected, and click Set your budget > : Create a cost budget, enter the following details: Name : CostBudget1 Period : Monthly Budget effective dates : Recurring Budget Start Month : (select current month) Budget amount : Fixed Budgeted amount : $1 (enter an amount a lot LESS than last months cost), Other fields: leave as defaults: Scroll down and click Configure alerts > : Select: Send alert based on : Forecasted Costs Alert threshold : 100% of budgeted amount Email contacts : (your email address) Click on Confirm budget > : Review the configuration, and click Create : You should see the current forecast will exceed the budget (it should be red, you may need to refresh your browser): 10: You will receive an email similar to this within a few minutes: You have created a forecasted budget, when your forecasted costs for the entire account are predicted to exceed the forecast, you will receive a notification. You can also create an actual budget, for when your current costs actually exceed a defined amount. 2. Create and implement an AWS Budget for EC2 actual cost We will create a monthly EC2 actual cost budget, which will notify if the actual costs of EC2 instances exceeds the specified amount. Click Create budget : Select Cost budget , and click Set your budget > : Create a cost budget, enter the following details: Name : EC2_actual Period : Monthly Budget effective dates : Recurring Budget Start Month : (current month) Budget amount : Fixed Budgeted amount : $1 (enter an amount a lot LESS than last months cost), Other fields: leave a defaults Under FILTERING click on Service: Type Elastic in the search field, then select the checkbox next to EC2-Instances(Elastic Compute Cloud - Compute) and Click Apply filters : De-select Upfront reservation fees , and click Configure alerts > : Select: Send alert based on : Actual Costs Alert threshold : 100% of budgeted amount Email contacts : (your email address) Click on Confirm budget > : Review the configuration, and click Create : You can see the current amount exceeds the budget (it is red, you may need to refresh your browser): You will receive an email similar to the previous budget within a few minutes. You have created an actual cost budget for EC2 usage. You can extend this budget by adding specific filters such as linked accounts, tags or instance types. You can also create budgets for services other than EC2. 3. Create and implement an AWS Budget for EC2 Savings Plan coverage We will create a monthly savings plan coverage budget which will notify if the coverage of Savings Plan for EC2 is below the specified amount. From the AWS Budgets dashboard in the console, click Create budget : Select Savings Plans budget , and click Set your budget > : Create a cost budget, enter the following details: Name : SP_Coverage Period : Monthly Savings Plans budget type : Savings Plans Coverage Coverage threshold : 90% Leave all other fields as defaults NOTE : NEVER create a utilization budget, unless you are doing it for a single and specific discount rate by using filters. For example you want to track the utilization of m5.large Linux discount. A utilization budget across different discounts will most likely lead to confusion and unnecessary work. Scroll down and click Configure alerts > : Enter an address for Email contacts and click Confirm budget > : Review the configuration, and click Create in the lower right: You have created an Savings Plans Coverage budget. High coverage is critical for cost optimization, as it ensures you are paying the lowest price for your resources. You will receive an email similar to this within a few minutes: 4. Create and implement an AWS Budget Report AWS Budgets Reports allow you to create and send daily, weekly, or monthly reports to monitor the performance of your AWS Budgets. From the Budgets dashboard, Click on Budgets Reports : Click Create budget report : Create a report with the following details: Report name : WeeklyBudgets Select all budgets Click Configure delivery settings > : Configure the delivery settings: Report frequency : Weekly Day of week : Monday Email recipients : Click Confirm budget report > : Review the configuration, click Create : Your budget report should now be complete: You should receive an email similar to the one below: 5. Tear down Delete a budget report We will delete the bugdet report we created in section 4. From the Budgets Reports dashboard, click on the three dots next to the Weekly Budgets budget report, and click Delete : You can see there are no budget reports: Delete a budget We will delete all three budgets that were configured in sections 1,2 and 3. From the Budgets dashboard, click on the budget name CostBudget1 : Click on the 3 dot menu in the top right, select Delete : Click on the other budget name EC2_actual : Click on the 3 dot menu in the top right, select Delete : Click on the other budget name SP_Coverage : Click on the 3 dot menu in the top right, select Delete : All budgets should be deleted that were created in this workshop: 5. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#level-100-cost-and-usage-governance","text":"","title":"Level 100: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#table-of-contents","text":"Create an AWS Budget - monthly forecast Create an AWS Budget - EC2 actual Create an AWS Budget - SP Coverage Create an AWS Budget Report Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#1-create-and-implement-an-aws-budget-for-monthly-forecasted-cost","text":"Budgets allow you to manage cost and usage by providing notifications when cost or usage are outside of configured amounts. They cannot be used to restrict actions, only notify on usage after it has occurred. Budgets and notifications are updated when your billing data is updated, which is at least once per day. NOTE : You may not receive an alarm for a forecasted budget if your account is new. Forecasting requires existing usage within the account.","title":"1. Create and implement an AWS Budget for monthly forecasted cost"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#create-a-monthly-cost-budget-for-your-account","text":"We will create a monthly cost budget which will notify if the forecasted amount exceeds the budget. Log into the console as an IAM user with the required permissions, go to the Billing console : Select Budgets from the left menu: Click on Create a budget : Ensure Cost Budget is selected, and click Set your budget > : Create a cost budget, enter the following details: Name : CostBudget1 Period : Monthly Budget effective dates : Recurring Budget Start Month : (select current month) Budget amount : Fixed Budgeted amount : $1 (enter an amount a lot LESS than last months cost), Other fields: leave as defaults: Scroll down and click Configure alerts > : Select: Send alert based on : Forecasted Costs Alert threshold : 100% of budgeted amount Email contacts : (your email address) Click on Confirm budget > : Review the configuration, and click Create : You should see the current forecast will exceed the budget (it should be red, you may need to refresh your browser): 10: You will receive an email similar to this within a few minutes: You have created a forecasted budget, when your forecasted costs for the entire account are predicted to exceed the forecast, you will receive a notification. You can also create an actual budget, for when your current costs actually exceed a defined amount.","title":"Create a monthly cost budget for your account"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#2-create-and-implement-an-aws-budget-for-ec2-actual-cost","text":"We will create a monthly EC2 actual cost budget, which will notify if the actual costs of EC2 instances exceeds the specified amount. Click Create budget : Select Cost budget , and click Set your budget > : Create a cost budget, enter the following details: Name : EC2_actual Period : Monthly Budget effective dates : Recurring Budget Start Month : (current month) Budget amount : Fixed Budgeted amount : $1 (enter an amount a lot LESS than last months cost), Other fields: leave a defaults Under FILTERING click on Service: Type Elastic in the search field, then select the checkbox next to EC2-Instances(Elastic Compute Cloud - Compute) and Click Apply filters : De-select Upfront reservation fees , and click Configure alerts > : Select: Send alert based on : Actual Costs Alert threshold : 100% of budgeted amount Email contacts : (your email address) Click on Confirm budget > : Review the configuration, and click Create : You can see the current amount exceeds the budget (it is red, you may need to refresh your browser): You will receive an email similar to the previous budget within a few minutes. You have created an actual cost budget for EC2 usage. You can extend this budget by adding specific filters such as linked accounts, tags or instance types. You can also create budgets for services other than EC2.","title":"2. Create and implement an AWS Budget for EC2 actual cost"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#3-create-and-implement-an-aws-budget-for-ec2-savings-plan-coverage","text":"We will create a monthly savings plan coverage budget which will notify if the coverage of Savings Plan for EC2 is below the specified amount. From the AWS Budgets dashboard in the console, click Create budget : Select Savings Plans budget , and click Set your budget > : Create a cost budget, enter the following details: Name : SP_Coverage Period : Monthly Savings Plans budget type : Savings Plans Coverage Coverage threshold : 90% Leave all other fields as defaults NOTE : NEVER create a utilization budget, unless you are doing it for a single and specific discount rate by using filters. For example you want to track the utilization of m5.large Linux discount. A utilization budget across different discounts will most likely lead to confusion and unnecessary work. Scroll down and click Configure alerts > : Enter an address for Email contacts and click Confirm budget > : Review the configuration, and click Create in the lower right: You have created an Savings Plans Coverage budget. High coverage is critical for cost optimization, as it ensures you are paying the lowest price for your resources. You will receive an email similar to this within a few minutes:","title":"3. Create and implement an AWS Budget for EC2 Savings Plan coverage"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#4-create-and-implement-an-aws-budget-report","text":"AWS Budgets Reports allow you to create and send daily, weekly, or monthly reports to monitor the performance of your AWS Budgets. From the Budgets dashboard, Click on Budgets Reports : Click Create budget report : Create a report with the following details: Report name : WeeklyBudgets Select all budgets Click Configure delivery settings > : Configure the delivery settings: Report frequency : Weekly Day of week : Monday Email recipients : Click Confirm budget report > : Review the configuration, click Create : Your budget report should now be complete: You should receive an email similar to the one below:","title":"4. Create and implement an AWS Budget Report"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#5-tear-down","text":"","title":"5. Tear down"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#delete-a-budget-report","text":"We will delete the bugdet report we created in section 4. From the Budgets Reports dashboard, click on the three dots next to the Weekly Budgets budget report, and click Delete : You can see there are no budget reports:","title":"Delete a budget report"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#delete-a-budget","text":"We will delete all three budgets that were configured in sections 1,2 and 3. From the Budgets dashboard, click on the budget name CostBudget1 : Click on the 3 dot menu in the top right, select Delete : Click on the other budget name EC2_actual : Click on the 3 dot menu in the top right, select Delete : Click on the other budget name SP_Coverage : Click on the 3 dot menu in the top right, select Delete : All budgets should be deleted that were created in this workshop:","title":"Delete a budget"},{"location":"Cost/Cost_Fundamentals/100_2_Cost_and_Usage_Governance/Lab_Guide.html#5-rate-this-lab","text":"","title":"5. Rate this lab"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/README.html","text":"Level 100: Pricing Models https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to perform an analysis of Savings Plans for your AWS cost and usage. The skills you learn will help you implement the correct pricing models for your workloads, in alignment with the AWS Well-Architected Framework. NOTE: There is a 200 level lab on building a Savings Plan analysis dashbord. Goals Perform basic analysis of your recommended Savings Plans Prerequisites Access to a master or payer AWS Account Completed all previous labs in the Cost Fundamentals series Permissions required Log in as the Cost Optimization team, created in AWS Account Setup Costs https://aws.amazon.com/aws-cost-management/pricing/ There are no costs for this lab Time to complete The lab should take approximately 15 minutes to complete Best Practice Checklist [ ] View Savings Plan recommendations [ ] Analyze and filter recommendations License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/README.html#level-100-pricing-models","text":"https://wellarchitectedlabs.com","title":"Level 100: Pricing Models"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/README.html#introduction","text":"This hands-on lab will guide you through the steps to perform an analysis of Savings Plans for your AWS cost and usage. The skills you learn will help you implement the correct pricing models for your workloads, in alignment with the AWS Well-Architected Framework. NOTE: There is a 200 level lab on building a Savings Plan analysis dashbord.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/README.html#goals","text":"Perform basic analysis of your recommended Savings Plans","title":"Goals"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/README.html#prerequisites","text":"Access to a master or payer AWS Account Completed all previous labs in the Cost Fundamentals series","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/README.html#permissions-required","text":"Log in as the Cost Optimization team, created in AWS Account Setup","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/README.html#costs","text":"https://aws.amazon.com/aws-cost-management/pricing/ There are no costs for this lab","title":"Costs"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/README.html#time-to-complete","text":"The lab should take approximately 15 minutes to complete","title":"Time to complete"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/README.html#best-practice-checklist","text":"[ ] View Savings Plan recommendations [ ] Analyze and filter recommendations","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/Lab_Guide.html","text":"Level 100: Pricing Models Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Savings Plan Introduction View your Savings Plan recommendations Understand your usage trend Analyze your Savings Plan recommendations Visualize your Savings Plan recommendations Tear down Rate this Lab 1. Introduction Savings Plans are a commitment based discount model. By making a commitment of the amount of resources you will use for 1 or 3 years, you receive a discount of up to 72%. They offer the same discounts as Reserved Instances, however offer a great deal more flexibility, and do not have the same management overhead. In this workshop we will take you through your recommendations, and help you choose the right savings plan for your future business requirements. 2. View your Savings Plan recommendations In this step we will look at the recommendation tooling and different Savings Plan options available. Log into the console as an IAM user with the required permissions, go to the Cost Explorer dashboard: Click on Savings Plans on the left menu: You can see a description and some examples of Savings Plans under the Savings Plans heading, and estimated savings at the bottom:: Click on Recommendations on the left menu: You can see the default options at the top, its a Compute savings plan, for 3-year , paying All upfront and based on the previous 30 days of usage. At the bottom you can see your estimated before and after spend, along with the percentage saving , this is an ideal starting point to understand the overall return you can get on your commitment: Click on Compute , 1-year and No upfront from the options above, and see the changes in before and after below. Note down the % saving, in this example it is 22% : Click on EC2 Instance , 3-year and All upfront , and note the % saving, in this example it is now 46% : This will typically be the highest and lowest savings you can achieve on the previous usage that was analyzed. You can vary those options to achieve the discount and features that most suit your business. You can also combine the options by purchasing multiple savings plans, making some commitment for 1-year with an upfront component, and some with no upfront commitment for a 3-year term. While the commitment is a full 1 or 3 years, a Savings Plan will typically be paid off much sooner. We will analyze this in the next step. 3. Understand your usage trend In large organizations usage can be distributed across many teams, and could take significant effort to collect. We can assist this effort by using tooling to understand your overall trends in usage to make an informed choice on Savings Plan commitments. You can use the Compute Savings Plan for this exercise if you plan on purchasing a compute plan. However we will use an EC2 Instance plan to provide more granularity and insights into usage. Click on Recommendations and then select EC2 Instance Savings Plans type, 1-year Savings Plans term, All upfront , and 60 days time period: Scroll down to Recommended EC2 Instance Savings Plans , take note of the Recommended commitment : Scroll up and change it to 30 days analysis: Scroll down to Recommended EC2 Instance Savings Plans , take note of the Recommended commitment : Scroll up and change it to 7 days analysis: Scroll down to Recommended EC2 Instance Savings Plans , take note of the Recommended commitment : Compare the trends in usage to see if your usage is increasing or decreasing. If usage is decreasing make a smaller initial hourly commitment, then re-analyze in 2-4 weeks. If usage is steady or increasing make a commitment closer to the recommended commitment: You now have an understanding of your overall usage trend, and can use this information to make a commitment that is matched to your business requirements. 4. Analyze your Savings Plan recommendations We have an understanding of the potential savings available to us, and how we can adjust that based on our business requirements. We also know our usage trends across the business, which will help us with our initial commitment. We will now go deeper to help you understand exactly what Savings Plan commitment is right for you. You can think of a single Savings Plan as a highly flexible group of Reserved Instances (RI's), without the same management overhead. Depending on the discount level and your usage, RI's can pay themselves off very quickly and offer large savings, or pay themselves off over a longer period with less savings. We will look into a Savings Plan to ensure our commitment pays off in the right amount of time and offers the amount of savings we need. We will analyze our Savings Plan to further refine our initial commitment level to purchase, this commitment will be very low risk and high return. Once that is purchased you then re-analyze every fortnight or month and \"top up\" your commitment levels. This ensures you maintain high levels of discounts, and you can continually adjust as your business evolves. Click on Recommendations and select EC2 Instance , 1-year , No upfront : Scroll down and click Download CSV There is a sample file here if you do not have data: Sample SavingsPlan Add the new column headings O , P and Q . and put in the formulas for each: Monthly OnDemand Cost, Cell O2 : =G2*730 Monthly Cost after SP, Cell P2 : =O2-K2 Fully Paid Day, Cell Q2 : =P2*12/O2 Updated Sample SavingsPlan The Fully Paid Day is the number of months it takes to pay off the entire 12 month savings plan. It is a combination of the discount level and your utilization level. At your current usage, after this day even if you turn off all your usage you will not lose money and be better off than paying on demand. The sooner the period the lower risk the purchase. We want the lowest risk purchases for our initial commitment, so sort by the Fully Paid Day , in order of smallest to largest , and insert some blank lines before a fully paid day of 9 months : Updated Sample SavingsPlan Sort the top group and bottom group by Estimated monthly savings amount , in order of largest to smallest , insert some lines above anything less than $50 in savings: Updated Sample SavingsPlan You now have four groups of Savings Plan usage: Very low risk, high return Very low risk, low return low to medium risk, high return low to medium risk, low return Combining this information with the previous exercises, your initial purchase will be typically focused on the low risk and high return, and some of the medium risk high return. Add up the hourly commitment for the recommendations that match your business requirements. In this example we have taken all of the very low risk high return, and 40% of the medium risk high return: Updated Sample SavingsPlan Take this commitment level, apply the findings from the previous exercises (type of Savings Plan and Usage Trend) to make your initial purchase. 5. Visualize your Savings Plan recommendations A visualization of your recommendation can be used as a quick double check, and also assist to demonstrate the savings and risks to other job functions. We will use the Cost Explorer hourly granularity feature to visualize Savings Plan recommendations. You need to have this enabled to view hourly usage, and there are associated costs. In the console go to the Billing Dashboard : Click on Savings Plans : Click on Recommendations , and select EC2 Instance , 1-year , All upfront and 7 days : Scroll down and pick a specific instance type: Scroll up and click Cost Explorer : Click the time period Last 7 days in this example, then click 7D and click Apply : Click the granularity Monthly in this example, and select Hourly : Click the Service filter, select EC2-Instances (Elastic Compute Cloud - Compute) and click Apply filters : Click Instance Type in the Group by menu: Click on Stack and select Line : Apply a filter on the region if you use multiple regions: If you have multiple instance types for a single family, you can select them by using a filter and choosing a stack graph. This will show the total costs for that family in the required region. Click More filters : Click Purchase Option , select On Demand and click Apply : Hover over the recommendation that matches the family you chose: Cost Explorer gives $0.51 hourly usage of C5.large which is the only instance in the C5 family. Our recommended commitment was $0.30 per hour for C5 all upfront. Go to the pricing page: https://aws.amazon.com/savingsplans/pricing/ Select the correct parameters that match the recommendation and view the rates: The savings is 41% . We were using $0.51 , so multiplied by 1-41% (0.59) = $0.30 . We can see the recommendation is accurate and valid, and see the usage pattern associated with the recommendation. 6. Tear down There is no tear down required for this lab. 7. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/Lab_Guide.html#level-100-pricing-models","text":"","title":"Level 100: Pricing Models"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/Lab_Guide.html#table-of-contents","text":"Savings Plan Introduction View your Savings Plan recommendations Understand your usage trend Analyze your Savings Plan recommendations Visualize your Savings Plan recommendations Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/Lab_Guide.html#1-introduction","text":"Savings Plans are a commitment based discount model. By making a commitment of the amount of resources you will use for 1 or 3 years, you receive a discount of up to 72%. They offer the same discounts as Reserved Instances, however offer a great deal more flexibility, and do not have the same management overhead. In this workshop we will take you through your recommendations, and help you choose the right savings plan for your future business requirements.","title":"1. Introduction"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/Lab_Guide.html#2-view-your-savings-plan-recommendations","text":"In this step we will look at the recommendation tooling and different Savings Plan options available. Log into the console as an IAM user with the required permissions, go to the Cost Explorer dashboard: Click on Savings Plans on the left menu: You can see a description and some examples of Savings Plans under the Savings Plans heading, and estimated savings at the bottom:: Click on Recommendations on the left menu: You can see the default options at the top, its a Compute savings plan, for 3-year , paying All upfront and based on the previous 30 days of usage. At the bottom you can see your estimated before and after spend, along with the percentage saving , this is an ideal starting point to understand the overall return you can get on your commitment: Click on Compute , 1-year and No upfront from the options above, and see the changes in before and after below. Note down the % saving, in this example it is 22% : Click on EC2 Instance , 3-year and All upfront , and note the % saving, in this example it is now 46% : This will typically be the highest and lowest savings you can achieve on the previous usage that was analyzed. You can vary those options to achieve the discount and features that most suit your business. You can also combine the options by purchasing multiple savings plans, making some commitment for 1-year with an upfront component, and some with no upfront commitment for a 3-year term. While the commitment is a full 1 or 3 years, a Savings Plan will typically be paid off much sooner. We will analyze this in the next step.","title":"2. View your Savings Plan recommendations"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/Lab_Guide.html#3-understand-your-usage-trend","text":"In large organizations usage can be distributed across many teams, and could take significant effort to collect. We can assist this effort by using tooling to understand your overall trends in usage to make an informed choice on Savings Plan commitments. You can use the Compute Savings Plan for this exercise if you plan on purchasing a compute plan. However we will use an EC2 Instance plan to provide more granularity and insights into usage. Click on Recommendations and then select EC2 Instance Savings Plans type, 1-year Savings Plans term, All upfront , and 60 days time period: Scroll down to Recommended EC2 Instance Savings Plans , take note of the Recommended commitment : Scroll up and change it to 30 days analysis: Scroll down to Recommended EC2 Instance Savings Plans , take note of the Recommended commitment : Scroll up and change it to 7 days analysis: Scroll down to Recommended EC2 Instance Savings Plans , take note of the Recommended commitment : Compare the trends in usage to see if your usage is increasing or decreasing. If usage is decreasing make a smaller initial hourly commitment, then re-analyze in 2-4 weeks. If usage is steady or increasing make a commitment closer to the recommended commitment: You now have an understanding of your overall usage trend, and can use this information to make a commitment that is matched to your business requirements.","title":"3. Understand your usage trend"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/Lab_Guide.html#4-analyze-your-savings-plan-recommendations","text":"We have an understanding of the potential savings available to us, and how we can adjust that based on our business requirements. We also know our usage trends across the business, which will help us with our initial commitment. We will now go deeper to help you understand exactly what Savings Plan commitment is right for you. You can think of a single Savings Plan as a highly flexible group of Reserved Instances (RI's), without the same management overhead. Depending on the discount level and your usage, RI's can pay themselves off very quickly and offer large savings, or pay themselves off over a longer period with less savings. We will look into a Savings Plan to ensure our commitment pays off in the right amount of time and offers the amount of savings we need. We will analyze our Savings Plan to further refine our initial commitment level to purchase, this commitment will be very low risk and high return. Once that is purchased you then re-analyze every fortnight or month and \"top up\" your commitment levels. This ensures you maintain high levels of discounts, and you can continually adjust as your business evolves. Click on Recommendations and select EC2 Instance , 1-year , No upfront : Scroll down and click Download CSV There is a sample file here if you do not have data: Sample SavingsPlan Add the new column headings O , P and Q . and put in the formulas for each: Monthly OnDemand Cost, Cell O2 : =G2*730 Monthly Cost after SP, Cell P2 : =O2-K2 Fully Paid Day, Cell Q2 : =P2*12/O2 Updated Sample SavingsPlan The Fully Paid Day is the number of months it takes to pay off the entire 12 month savings plan. It is a combination of the discount level and your utilization level. At your current usage, after this day even if you turn off all your usage you will not lose money and be better off than paying on demand. The sooner the period the lower risk the purchase. We want the lowest risk purchases for our initial commitment, so sort by the Fully Paid Day , in order of smallest to largest , and insert some blank lines before a fully paid day of 9 months : Updated Sample SavingsPlan Sort the top group and bottom group by Estimated monthly savings amount , in order of largest to smallest , insert some lines above anything less than $50 in savings: Updated Sample SavingsPlan You now have four groups of Savings Plan usage: Very low risk, high return Very low risk, low return low to medium risk, high return low to medium risk, low return Combining this information with the previous exercises, your initial purchase will be typically focused on the low risk and high return, and some of the medium risk high return. Add up the hourly commitment for the recommendations that match your business requirements. In this example we have taken all of the very low risk high return, and 40% of the medium risk high return: Updated Sample SavingsPlan Take this commitment level, apply the findings from the previous exercises (type of Savings Plan and Usage Trend) to make your initial purchase.","title":"4. Analyze your Savings Plan recommendations"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/Lab_Guide.html#5-visualize-your-savings-plan-recommendations","text":"A visualization of your recommendation can be used as a quick double check, and also assist to demonstrate the savings and risks to other job functions. We will use the Cost Explorer hourly granularity feature to visualize Savings Plan recommendations. You need to have this enabled to view hourly usage, and there are associated costs. In the console go to the Billing Dashboard : Click on Savings Plans : Click on Recommendations , and select EC2 Instance , 1-year , All upfront and 7 days : Scroll down and pick a specific instance type: Scroll up and click Cost Explorer : Click the time period Last 7 days in this example, then click 7D and click Apply : Click the granularity Monthly in this example, and select Hourly : Click the Service filter, select EC2-Instances (Elastic Compute Cloud - Compute) and click Apply filters : Click Instance Type in the Group by menu: Click on Stack and select Line : Apply a filter on the region if you use multiple regions: If you have multiple instance types for a single family, you can select them by using a filter and choosing a stack graph. This will show the total costs for that family in the required region. Click More filters : Click Purchase Option , select On Demand and click Apply : Hover over the recommendation that matches the family you chose: Cost Explorer gives $0.51 hourly usage of C5.large which is the only instance in the C5 family. Our recommended commitment was $0.30 per hour for C5 all upfront. Go to the pricing page: https://aws.amazon.com/savingsplans/pricing/ Select the correct parameters that match the recommendation and view the rates: The savings is 41% . We were using $0.51 , so multiplied by 1-41% (0.59) = $0.30 . We can see the recommendation is accurate and valid, and see the usage pattern associated with the recommendation.","title":"5. Visualize your Savings Plan recommendations"},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/Lab_Guide.html#6-tear-down","text":"There is no tear down required for this lab.","title":"6. Tear down "},{"location":"Cost/Cost_Fundamentals/100_3_Pricing_Models/Lab_Guide.html#7-rate-this-lab","text":"","title":"7. Rate this lab"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html","text":"Level 100: Cost and Usage Analysis https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to perform analysis of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework. Goals Perform basic analysis of your cost and usage Prerequisites A master AWS Account A linked AWS Account (preferred, not mandatory) Completed all previous labs in the Cost Fundamentals series Permissions required Log in as the Cost Optimization team, created in AWS Account Setup NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Start the Lab! Best Practice Checklist [ ] View your AWS Invoices [ ] View your cost and usage in detail through the console [ ] Download your monthly cost and usage CSV file License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#level-100-cost-and-usage-analysis","text":"https://wellarchitectedlabs.com","title":"Level 100: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#introduction","text":"This hands-on lab will guide you through the steps to perform analysis of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#goals","text":"Perform basic analysis of your cost and usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#prerequisites","text":"A master AWS Account A linked AWS Account (preferred, not mandatory) Completed all previous labs in the Cost Fundamentals series","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#permissions-required","text":"Log in as the Cost Optimization team, created in AWS Account Setup NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#best-practice-checklist","text":"[ ] View your AWS Invoices [ ] View your cost and usage in detail through the console [ ] Download your monthly cost and usage CSV file","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html","text":"Level 100: Cost and Usage Analysis Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents View your AWS Invoices View your cost and usage in detail Download your monthly cost and usage file Tear down Rate this Lab 1. View your AWS Invoices At the end of a billing cycle or at the time you choose to incur a one-time fee, AWS charges the payment method you have and issues your invoice as a PDF file. You can view these invoices through the AWS console, which will show summary information of all usage and cost incurred for that one off item, or billing period. Log into the console as an IAM user with the required permissions, go to the billing dashboard: Select Payment History from the menu on the left: Click on an Invoice/Receipt ID corresponding to the month you wish to view: It will download a PDF version of your invoice similar to below: 2. View your cost and usage in detail You can view past and present costs and usage through the console, which also provides more detailed information on cost and usage. We will go through accessing your cost and usage by service, and by linked account (if applicable). We will then drill down into a specific service. Go to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: You will be shown Bill details by service , where you can dynamically drill down into the specific service cost and usage. Pick your largest cost service and look into the region and line items: Select Bill details by account to see cost and usage for each account separately. Select the Account name , then drill down into the specific service cost and usage: 3. Download your monthly cost and usage file It is possible to download a CSV version of your summary cost and usage information. This can be accessed by a spreadsheet application for ease of use. We will download your monthly usage file and view it. Go to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: Click on Download CSV : It will download a CSV version of the bill you can use in a spreadsheet application. It is recommended to NOT use this data source for calculations and analysis, instead you should use the Cost and Usage Report, which is covered in 200_4_Cost_and_Usage_ansalysis . 4. Tear down There is no configuration performed within this lab, so no teardown is required. 5. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#level-100-cost-and-usage-analysis","text":"","title":"Level 100: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#table-of-contents","text":"View your AWS Invoices View your cost and usage in detail Download your monthly cost and usage file Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#1-view-your-aws-invoices","text":"At the end of a billing cycle or at the time you choose to incur a one-time fee, AWS charges the payment method you have and issues your invoice as a PDF file. You can view these invoices through the AWS console, which will show summary information of all usage and cost incurred for that one off item, or billing period. Log into the console as an IAM user with the required permissions, go to the billing dashboard: Select Payment History from the menu on the left: Click on an Invoice/Receipt ID corresponding to the month you wish to view: It will download a PDF version of your invoice similar to below:","title":"1. View your AWS Invoices "},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#2-view-your-cost-and-usage-in-detail","text":"You can view past and present costs and usage through the console, which also provides more detailed information on cost and usage. We will go through accessing your cost and usage by service, and by linked account (if applicable). We will then drill down into a specific service. Go to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: You will be shown Bill details by service , where you can dynamically drill down into the specific service cost and usage. Pick your largest cost service and look into the region and line items: Select Bill details by account to see cost and usage for each account separately. Select the Account name , then drill down into the specific service cost and usage:","title":"2. View your cost and usage in detail"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#3-download-your-monthly-cost-and-usage-file","text":"It is possible to download a CSV version of your summary cost and usage information. This can be accessed by a spreadsheet application for ease of use. We will download your monthly usage file and view it. Go to the billing dashboard: Click on Bills from the left menu: Select the Date you require from the drop down menu, by clicking on the menu item: Click on Download CSV : It will download a CSV version of the bill you can use in a spreadsheet application. It is recommended to NOT use this data source for calculations and analysis, instead you should use the Cost and Usage Report, which is covered in 200_4_Cost_and_Usage_ansalysis .","title":"3. Download your monthly cost and usage file"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#4-tear-down","text":"There is no configuration performed within this lab, so no teardown is required.","title":"4. Tear down"},{"location":"Cost/Cost_Fundamentals/100_4_Cost_and_Usage_Analysis/Lab_Guide.html#5-rate-this-lab","text":"","title":"5. Rate this lab"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html","text":"Level 100: Billing Visualization https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to perform visualization of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework. Goals Perform basic analysis of your cost and usage Prerequisites A master AWS Account A linked AWS Account (preferred, not mandatory) Completed all previous labs in the Cost Fundamentals series Permissions required Log in as the Cost Optimization team, created in AWS Account Setup NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Costs https://aws.amazon.com/aws-cost-management/pricing/ Variable costs will be incurred Cost Explorer: $0.01 per 1,000 usage records for hourly granularity reporting Time to complete The lab should take approximately 15 minutes to complete Best Practice Checklist [ ] View your cost and usage by service [ ] View your cost and usage by account [ ] View your Reserved Instance Coverage [ ] Create a custom EC2 report License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#level-100-billing-visualization","text":"https://wellarchitectedlabs.com","title":"Level 100: Billing Visualization"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#introduction","text":"This hands-on lab will guide you through the steps to perform visualization of your AWS cost and usage. The skills you learn will help you monitor your cost and usage, in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#goals","text":"Perform basic analysis of your cost and usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#prerequisites","text":"A master AWS Account A linked AWS Account (preferred, not mandatory) Completed all previous labs in the Cost Fundamentals series","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#permissions-required","text":"Log in as the Cost Optimization team, created in AWS Account Setup NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#costs","text":"https://aws.amazon.com/aws-cost-management/pricing/ Variable costs will be incurred Cost Explorer: $0.01 per 1,000 usage records for hourly granularity reporting","title":"Costs"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#time-to-complete","text":"The lab should take approximately 15 minutes to complete","title":"Time to complete"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#best-practice-checklist","text":"[ ] View your cost and usage by service [ ] View your cost and usage by account [ ] View your Reserved Instance Coverage [ ] Create a custom EC2 report","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html","text":"Level 100: Cost Visualization Authors Nathan Besh, Cost Lead Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents View your cost and usage by service View your cost and usage by account View your Savings Plan coverage View your Elasticity View your Reserved Instance coverage Create a custom EC2 report Tear down Rate this Lab 1. View your cost and usage by service AWS Cost Explorer is a free built in tool that lets you dive deeper into your cost and usage data to identify trends, pinpoint cost drivers, and detect anomalies. We will examine costs by service in this exercise. Log into the console as an IAM user with the required permissions, go to the billing dashboard: Select Cost Explorer from the menu on the left: Click on Launch Cost Explorer : Click on Saved reports from the left menu: You will be presented a list of pre-configured and saved reports. Click on Monthly costs by service : This is the monthly costs by service for the last 6 months, broken down by month (your usage will most likely be different): We will change to a daily view to highlight trends. Select the Monthly drop down and click on Daily : The bar graph is difficult to read, so we will switch to a line graph. Click on the Bar dropdown, then select Line : This is the same data with daily granularity and shows trends much more clearly. There are monthly peaks - these are monthly recurring reservation fees from Reserved Instances (Purple line): We will remove the RI recurring fees. Click on More filters then click Charge Type filter on the right, click the checkbox next to Recurring reservation fee , select Exclude only to remove the data. Then click Apply filters : We have now excluded the monthly recurring fees and the peaks have been removed. We can see the largest cost for our usage during this period is EC2-Instances: We will remove the EC2 service to show the other services with better clarity. Click on the Service filter from the right, click the checkbox next to EC2-Instances , select Exclude only , and click Apply filters : EC2-Instances has now been excluded, and all the other services can been seen easily: You have now viewed the costs by service and applied multiple filters. You can continue to modify the report by timeframe and apply other filters. 2. View your cost and usage by account We will now view usage by account. This helps to highlight where the costs and usage are by linked account. NOTE: you will need one or more multiple accounts for this exercise to be effective. Select Saved reports from the left menu: Click on Monthly costs by linked account : It will show the default last 6 months, with a monthly granularity. As above, change the graph to Daily granularity and from a bar graph to a Line graph: Here is the daily granularity line graph. You can see there is one account which has the most cost, so lets focus on that by applying a filter: On the right click on Linked Account , select the checkbox next to the account we want to focus on, then click Include only and Apply filters : You can now see this one accounts usage: Lets see the services breakdown for this account, click on Service to group by services and change it to a line graph: You can see the service breakdown for this account. Lets see the instance type breakdown for this account, click on Instance Type and change it to a line graph: You can see the instance type breakdown for this account. Lets see the usage type breakdown for this account, click on Usage Type and change it to a line graph: Here is the usage type breakdown: You have now viewed the costs by account and applied multiple filters. You can continue to modify the report by timeframe and apply other filters. 3. View your Savings Plan coverage To ensure you are paying the lowest prices for your resources, a high coverage of Savings Plan is required. A typical goal is to aim for approximately 90% of your baseload resources (\"always on\") covered by Savings Plans, here is how you can check your coverage. In Cost Explorer, click on Saved reports on the left: Click on Coverage report : You can see the coverage is 0%: Scroll down to the table, click on the arrow next to On-demand spend to sort from the largest spend to the lowest. This helps show your opportunity for cost savings: 4. View your Elasticity NOTE : This exercise requires you have enabled hourly granularity within Cost Explorer, this can be done by following the instructions here - AWS Account Setup , Step 5 item 5 - Enable Cost Explorer. There are additional costs to enable this granularity. A key part of cost optimization is ensuring that your systems scale with your usage. This visualization will show how your systems operate over time. Click on Cost Explorer to go back to the default view: Click the down arrow to change the period, select 14D and click Apply : Click on Monthly and change the granularity to Hourly : Click on Bar , then select Line : You will now have in depth insight to how your environment is operating. You can see in this example the EC2 Instances scaling every day, you can see a period of large ELB usage, and EC2-Other, which includes charges related to EC2 such as data transfer. 5. View your Reserved Instance coverage To ensure you are paying the lowest prices for your resources, a high coverage of Reserved Instances (RIs) is required. A typical goal is to aim for approximately 80% of baseload (\"always on\") instances covered by RI's, here is how you can check your coverage. In Cost Explorer, click on Saved reports on the left: Click on RI Coverage : You will see the default RI Coverage report. It is for the Last 3 Months , and is for the instances within the EC2 service: Scroll down below the graph and you can see a summary of the costs and usage. Note that depending on the instance type and size, the On-Demand costs will be different per hour: To help focus where you need to, click on the down arrow next to ON-DEMAND COST to sort by costs descending. This will put the highest on-demand costs at the top, which is where you should focus your RI purchases: You have now viewed your RI coverage, and have insight on where to increase your coverage. 6. Create custom EC2 reports We will now create some custom EC2 reports, which will help to show ongoing costs related to EC2 instances and their associated usage. From the left menu click Cost Explorer , click Reports , and click and click Monthly costs by service : You will have the default breakdown by Service. Click on the Service filter on the right, select EC2-Instances (Elastic Compute Cloud - Compute) and EC2-Other , then click Apply filters : You will now have monthly EC2 Instance and Other costs: Change the Group by to Usage Type : Change it to a Daily Line graph, then select More filters : click on Purchase Option , select On Demand and click Apply filters , which will ensure we are only looking at On-Demand costs: These are your on-demand EC2 costs, you should setup a report like this for your services that have the highest usage or costs. We will now save this, click on Save as... : Enter a report name and click Save Report > : Now click on the Service filter, and de-select EC2-Instances , so that only EC2-Other is selected: Now you can clearly see what makes up the Other charges, typically these are EBS volumes, Data Transfer and other costs associated with EC2 usage. Click Save as... (do NOT click Save): Enter a report name and click Save Report > : You can access these by clicking on Saved Reports : Here you can see both reports that were saved, note they do not have a lock symbol - which is reserved for AWS configured reports: 7. Tear down We will delete both custom reports that were created. Click on Saved reports on the left menu: Select the checkbox next to the two custom reports that you created above. Click on Delete : Verify the names of the reports you are going to delete, click Delete : The reports are no longer listed in the reports available: 8. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#level-100-cost-visualization","text":"","title":"Level 100: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead Well-Architected","title":"Authors"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#table-of-contents","text":"View your cost and usage by service View your cost and usage by account View your Savings Plan coverage View your Elasticity View your Reserved Instance coverage Create a custom EC2 report Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#1-view-your-cost-and-usage-by-service","text":"AWS Cost Explorer is a free built in tool that lets you dive deeper into your cost and usage data to identify trends, pinpoint cost drivers, and detect anomalies. We will examine costs by service in this exercise. Log into the console as an IAM user with the required permissions, go to the billing dashboard: Select Cost Explorer from the menu on the left: Click on Launch Cost Explorer : Click on Saved reports from the left menu: You will be presented a list of pre-configured and saved reports. Click on Monthly costs by service : This is the monthly costs by service for the last 6 months, broken down by month (your usage will most likely be different): We will change to a daily view to highlight trends. Select the Monthly drop down and click on Daily : The bar graph is difficult to read, so we will switch to a line graph. Click on the Bar dropdown, then select Line : This is the same data with daily granularity and shows trends much more clearly. There are monthly peaks - these are monthly recurring reservation fees from Reserved Instances (Purple line): We will remove the RI recurring fees. Click on More filters then click Charge Type filter on the right, click the checkbox next to Recurring reservation fee , select Exclude only to remove the data. Then click Apply filters : We have now excluded the monthly recurring fees and the peaks have been removed. We can see the largest cost for our usage during this period is EC2-Instances: We will remove the EC2 service to show the other services with better clarity. Click on the Service filter from the right, click the checkbox next to EC2-Instances , select Exclude only , and click Apply filters : EC2-Instances has now been excluded, and all the other services can been seen easily: You have now viewed the costs by service and applied multiple filters. You can continue to modify the report by timeframe and apply other filters.","title":"1. View your cost and usage by service"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#2-view-your-cost-and-usage-by-account","text":"We will now view usage by account. This helps to highlight where the costs and usage are by linked account. NOTE: you will need one or more multiple accounts for this exercise to be effective. Select Saved reports from the left menu: Click on Monthly costs by linked account : It will show the default last 6 months, with a monthly granularity. As above, change the graph to Daily granularity and from a bar graph to a Line graph: Here is the daily granularity line graph. You can see there is one account which has the most cost, so lets focus on that by applying a filter: On the right click on Linked Account , select the checkbox next to the account we want to focus on, then click Include only and Apply filters : You can now see this one accounts usage: Lets see the services breakdown for this account, click on Service to group by services and change it to a line graph: You can see the service breakdown for this account. Lets see the instance type breakdown for this account, click on Instance Type and change it to a line graph: You can see the instance type breakdown for this account. Lets see the usage type breakdown for this account, click on Usage Type and change it to a line graph: Here is the usage type breakdown: You have now viewed the costs by account and applied multiple filters. You can continue to modify the report by timeframe and apply other filters.","title":"2. View your cost and usage by account"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#3-view-your-savings-plan-coverage","text":"To ensure you are paying the lowest prices for your resources, a high coverage of Savings Plan is required. A typical goal is to aim for approximately 90% of your baseload resources (\"always on\") covered by Savings Plans, here is how you can check your coverage. In Cost Explorer, click on Saved reports on the left: Click on Coverage report : You can see the coverage is 0%: Scroll down to the table, click on the arrow next to On-demand spend to sort from the largest spend to the lowest. This helps show your opportunity for cost savings:","title":"3. View your Savings Plan coverage"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#4-view-your-elasticity","text":"NOTE : This exercise requires you have enabled hourly granularity within Cost Explorer, this can be done by following the instructions here - AWS Account Setup , Step 5 item 5 - Enable Cost Explorer. There are additional costs to enable this granularity. A key part of cost optimization is ensuring that your systems scale with your usage. This visualization will show how your systems operate over time. Click on Cost Explorer to go back to the default view: Click the down arrow to change the period, select 14D and click Apply : Click on Monthly and change the granularity to Hourly : Click on Bar , then select Line : You will now have in depth insight to how your environment is operating. You can see in this example the EC2 Instances scaling every day, you can see a period of large ELB usage, and EC2-Other, which includes charges related to EC2 such as data transfer.","title":"4. View your Elasticity"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#5-view-your-reserved-instance-coverage","text":"To ensure you are paying the lowest prices for your resources, a high coverage of Reserved Instances (RIs) is required. A typical goal is to aim for approximately 80% of baseload (\"always on\") instances covered by RI's, here is how you can check your coverage. In Cost Explorer, click on Saved reports on the left: Click on RI Coverage : You will see the default RI Coverage report. It is for the Last 3 Months , and is for the instances within the EC2 service: Scroll down below the graph and you can see a summary of the costs and usage. Note that depending on the instance type and size, the On-Demand costs will be different per hour: To help focus where you need to, click on the down arrow next to ON-DEMAND COST to sort by costs descending. This will put the highest on-demand costs at the top, which is where you should focus your RI purchases: You have now viewed your RI coverage, and have insight on where to increase your coverage.","title":"5. View your Reserved Instance coverage"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#6-create-custom-ec2-reports","text":"We will now create some custom EC2 reports, which will help to show ongoing costs related to EC2 instances and their associated usage. From the left menu click Cost Explorer , click Reports , and click and click Monthly costs by service : You will have the default breakdown by Service. Click on the Service filter on the right, select EC2-Instances (Elastic Compute Cloud - Compute) and EC2-Other , then click Apply filters : You will now have monthly EC2 Instance and Other costs: Change the Group by to Usage Type : Change it to a Daily Line graph, then select More filters : click on Purchase Option , select On Demand and click Apply filters , which will ensure we are only looking at On-Demand costs: These are your on-demand EC2 costs, you should setup a report like this for your services that have the highest usage or costs. We will now save this, click on Save as... : Enter a report name and click Save Report > : Now click on the Service filter, and de-select EC2-Instances , so that only EC2-Other is selected: Now you can clearly see what makes up the Other charges, typically these are EBS volumes, Data Transfer and other costs associated with EC2 usage. Click Save as... (do NOT click Save): Enter a report name and click Save Report > : You can access these by clicking on Saved Reports : Here you can see both reports that were saved, note they do not have a lock symbol - which is reserved for AWS configured reports:","title":"6. Create custom EC2 reports"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#7-tear-down","text":"We will delete both custom reports that were created. Click on Saved reports on the left menu: Select the checkbox next to the two custom reports that you created above. Click on Delete : Verify the names of the reports you are going to delete, click Delete : The reports are no longer listed in the reports available:","title":"7. Tear down"},{"location":"Cost/Cost_Fundamentals/100_5_Cost_Visualization/Lab_Guide.html#8-rate-this-lab","text":"","title":"8. Rate this lab"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html","text":"Level 200: Cost and Usage Governance https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements. Goals Implement IAM Policies to control usage Prerequisites An AWS Account Completed all previous labs in the Cost Fundamentals series Permissions required ./Code/IAM_policy IAM policy required for this lab NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Costs https://aws.amazon.com/iam/faqs/?nc=sn&loc=5 Costs will be less than $1 if all steps including the teardown are performed Time to complete The lab should take approximately 15 minutes to complete Best Practice Checklist [ ] Create an IAM Policy to restrict EC2 usage by region [ ] Create an IAM Policy to restirct EC2 usage by family [ ] Extend an IAM Policy to restrict EC2 usage by instance size [ ] Create an IAM policy to restrict EBS Volume creation by volume type License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#level-200-cost-and-usage-governance","text":"https://wellarchitectedlabs.com","title":"Level 200: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#introduction","text":"This hands-on lab will guide you through the steps to implement cost and usage governance. The skills you learn will help you control your cost and usage in alignment with your business requirements.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#goals","text":"Implement IAM Policies to control usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#prerequisites","text":"An AWS Account Completed all previous labs in the Cost Fundamentals series","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#permissions-required","text":"./Code/IAM_policy IAM policy required for this lab NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#costs","text":"https://aws.amazon.com/iam/faqs/?nc=sn&loc=5 Costs will be less than $1 if all steps including the teardown are performed","title":"Costs"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#time-to-complete","text":"The lab should take approximately 15 minutes to complete","title":"Time to complete"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#best-practice-checklist","text":"[ ] Create an IAM Policy to restrict EC2 usage by region [ ] Create an IAM Policy to restirct EC2 usage by family [ ] Extend an IAM Policy to restrict EC2 usage by instance size [ ] Create an IAM policy to restrict EBS Volume creation by volume type","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html","text":"Level 200: Cost and Usage Governance Authors Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Create a group of users for testing Create an IAM Policy to restrict EC2 usage by region Create an IAM Policy to restirct EC2 usage by family Extend an IAM Policy to restrict EC2 usage by instance size Create an IAM policy to restrict EBS Volume creation by volume type Tear down Rate this Lab 1. Create a group of users for testing This lab requires you to apply an IAM policy to a group of users, then login as a user in that group and verify the policy. We will create this test group. Go to the IAM service page: Click on Groups , click Create New Group : Set the group name to CostTest and click Next Step : Click Next Step : Click Create Group : Click Users : Click Add user : Configure the user as follows: Username : TestUser1 Access type : AWS Management Console access Console password : Autogenerated password Un-select Require password reset Click Next: Permissions Select the CostTest group, and click Next: Tags : Click Next: Review : Review the details and click Create user : Record the logon link , the User and the Password for later use, click Close : 2. Create an IAM Policy to restrict service usage by region To manage costs you need to manage and control your usage. AWS offers multiple regions, so depending on your business requirements you can limit access to AWS services depending on the region. This can be used to ensure usage is only allowed in specific regions which are more cost effective, and minimize associated usage and cost, such as data transfer. We will create a policy that allows all EC2, RDS and S3 access in a single region only. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed. 2.1 Create the IAM Policy Go to the IAM service page: Select Policies from the left menu: Click Create Policy : Click the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/Region_Restrict Click Review policy : Create the policy with the following details: Name : RegionRestrict Description : EC2, RDS, S3 access in us-east-1 only Click Create policy : You have successfully created the Policy. 2.2 Apply it to a group Select Groups from the left menu: Click on the CostTest group (created previously): Select the Permissions tab: Click Attach Policy : Click Policy Type and select Customer Managed : Select the checkbox next to Region_Restrict (created above) and click Attach Policy : You have successfully attached the policy to the CostTest group. Log out from the console 2.3 Verify the policy is in effect Logon to the console as the TestUser1 user, go to the EC2 Service dashboard: Click the current region in the top right, and select US West (N.California) : In the Old look EC2 console you will notice that there are authorization messages due to not having access in that region (the policy restricted EC2 usage to N. Virginia only), the new look EC2 console will not have errors: Try to launch an instance by clicking Launch Instance : Click on Select next to the Amazon Linux 2 AMI , You will receive an error when you select an AMI as you do not have permissions: You have successfully verified that you cannot launch any instances outside of the N.Virginia region. We will now verify we have access in us-east-1 (N.Virginia): Change the region by clicking the current region, and selecting US East (N.Virginia) : Now attempt to launch an instance, choose the Amazon Linux 2 AMI , leave 64-bit (x86) selected, click Select : Scroll down and select a c5.large , and click Review and Launch : Take note of the security group created (as you need to delete it), Click Launch : Select Proceed without a key pair , and click I acknowledge.. checkbox, and click Launch Instances : You will get a success message, click on the instance id: Ensure the correct instance is selected, click Actions , then Instance State , then Terminate : Confirm the instance ID is correct, click Yes, Terminate : You have successfully implemented an IAM policy that restricts all EC2, RDS and S3 operations to a single region. Log out of the console as TestUser1. 3. Create an IAM Policy to restrict EC2 usage by family AWS offers different instance families within EC2. Depending on your workload requirements - different types will be most cost effective. For non-specific environments such as testing or development, you can restrict the instance families in those accounts to the most cost effective generic types. It is also an effective way to increase RI utilization, by ensuring these accounts will consume any available Reserved Instances. We will create a policy that allows operations on specific instance families only. This will not only restrict launching an instance, but all other activities. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed. 3.1 Create the IAM Policy Log on to the console as your regular user with the required permissions, Go to the IAM service page: Select Policies from the left menu: Click Create Policy : Click on the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/EC2Family_Restrict Click Review policy : Enter the details: Name : EC2_FamilyRestrict Description : Restrict to t3, a1 and m5 families Click on Create Policy : 3.2 Attach the policy to the group Click on Groups from the left menu: Click on the CostTest group (created previously): We need to remove the RegionRestrict policy, as it permitted all EC2 actions. Click on Detach Policy for RegionRestrict : Click on Detach : Click on Attach Policy : Click on Policy Type , then click Customer Managed : Select the checkbox next to Ec2_FamilyRestrict , and click Attach Policy : Log out of the console 3.3 Verify the policy is in effect Logon to the console as the TestUser1 user, go to the EC2 Service dashboard: Click on Launch Instance : Click on Select next to the Amazon Linux 2 AMI: We will select an instance we are not able to launch first, so select a c5.large instance, click Review and Launch : Make note of the security group created, click Launch : Select Proceed without a key pair , and click I acknowledge that I will not be able to... , then click Launch Instances : You will receive an error, notice the failed step was Initiating launches . Click Back to Review Screen : Click Edit instance type : We will select an instance type we can launch (t3, a1 or m5) select t3.micro , and click Review and Launch : Select Yes, I want to continue with this instance type (t3.micro) , click Next : Click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will receive a success message. Click on the Instance ID and terminate the instance as above: You have successfully implemented an IAM policy that restricts all EC2 actions to T3, A1 and M5 instance types. Log out of the console as TestUser1. 4. Extend an IAM Policy to restrict EC2 usage by instance size We can also restrict the size of instance that can be launched. This can be used to ensure only low cost instances can be created within an account. This is ideal for testing and development, where high capacity instances may not be required. We will extend the EC2 family policy above, and add restrictions by adding the sizes of instances allowed. 4.1 Extend the EC2Family_Restrict IAM Policy Log on to the console as your regular user with the required permissions, go to the IAM service page: Click on Policies on the left menu: Click on Filter policies , then select Customer managed : Click on EC2_FamilyRestrict to modify it: Click on Edit policy : Click on the JSON tab: Modify the policy by adding in the sizes, add in nano , medium , large , be careful not to change the syntax and not remove the quote characters. Click on Review policy : Click on Save changes : 4.2 Verify the policy is in effect Logon to the console as the TestUser1 user, click on Services and go to the EC2 dashboard: Click on Launch Instance : Click on Select next to the Amazon Linux 2 AMI : We will attempt to launch a t3.micro which was successful before. Click on Review and Launch : Review the configuration and take note of the security group created, click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will get a failure, as it wasn't a size we allowed in the policy. Click Back to Review Screen : Click Edit instance type : We will now select a t3.nano which will succeed. Click Review and Launch : Select Yes, I want to continue with this instance type (t3.nano) , and click Next : Review the configuration and click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : It will succeed. Click on the Instance ID and terminate the instance as above: You have successfully implemented an IAM policy that restricts all EC2 instance operations by family and size. Log out of the console as TestUser1. 5. Create an IAM policy to restrict EBS Volume creation by volume type Extending cost optimization governance beyond compute instances will ensure overall higher levels of cost optimization. Similar to EC2 instances, there are different storage types. Governing the type of storage that can be created in an account can be effective to minimize cost. We will create an IAM policy that denies operations that contain provisioned IOPS (io1) EBS volume types. This will not only restrict creating a volume, but all other actions that attempt to use this volume type. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed. 5.1 Create the IAM Policy Log on to the console as your regular user with the required permissions, go to the IAM service page: Click on Policies on the left menu: Click Create policy : Click on the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/EC2EBS_Restrict Click on Review Policy : Configure the following details: Name : EC2EBS_Restrict Description : Dont allow EBS io1 volumes Click Create policy : 5.2 Attach the policy to the Cost Optimization group Click on Groups from the left menu: Click on the CostTest group: Click on Attach Policy : Click on Policy Type , then click Customer Managed : Select the checkbox next to EC2EBS_Restrict , and click Attach Policy : Log out from the console 5.3 Verify the policy is in effect Logon to the console as the TestUser1 user, click on Services then click EC2 : Click Launch Instance : Click Select next to Amazon Linux 2... : Select t3.nano (which is allowed as per our already applied policy, which we tested in the last exercise), click Next: Configure Instance Details : Click Next Add Storage : Click on Add New Volume , click on the dropdown , then select Provisioned IOPS SSD (io1) : Click Review and Launch : Take note of the security group created, and click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : The launch will fail, as it contained an io1 volume. Click Back to Review Screen : Scroll down and click Edit storage : Click the dropdown and change it to General Purpose SSD(gp2) , click Review and Launch : Click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : It will now succeed, as it doesn't contain an io1 volume type. Click on the instance ID and terminate the instance as above: You have successfully implemented an IAM policy that denies operations if there is an EBS volume of type io1. Log out of the console as TestUser1. 6. Tear down Log onto the console as your regular user with the required permissions. Delete a policy We will delete the IAM policies created above, as they are no longer applied to any groups. Go to the IAM Console: Click on Policies on the left: 3.Click on Filter Policies and select Customer managed : Select the policy you want to delete Region_Restrict : Click on Policy actions , and select Delete : Click on Delete : Perform the same steps above to delete the Ec2_FamilyRestrict and EC2EBS_Restrict policies. Click on Groups : Select the CostTest group, click Group Actions , click Delete Group : Click Yes, Delete : Click Users : Select TestUser1 , and click Delete user : Click Yes, delete : Go to the EC2 dashboard: Click Security Groups on the left: Select the security groups you took note of , ensure you have the correct groups that were created. Click Actions , select Delete Security Groups : Triple check they are the groups you wrote down, and click Yes, Delete : Confirm there are no unattached EBS volumes, go to the EC2 dashboard , click on Elastic Block Store , click Volumes . You can sort by the Created column to help identify volumes that were not terminated as part of this lab. 7. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#level-200-cost-and-usage-governance","text":"","title":"Level 200: Cost and Usage Governance"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead Well-Architected Spencer Marley, Commercial Architect","title":"Authors"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#table-of-contents","text":"Create a group of users for testing Create an IAM Policy to restrict EC2 usage by region Create an IAM Policy to restirct EC2 usage by family Extend an IAM Policy to restrict EC2 usage by instance size Create an IAM policy to restrict EBS Volume creation by volume type Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#1-create-a-group-of-users-for-testing","text":"This lab requires you to apply an IAM policy to a group of users, then login as a user in that group and verify the policy. We will create this test group. Go to the IAM service page: Click on Groups , click Create New Group : Set the group name to CostTest and click Next Step : Click Next Step : Click Create Group : Click Users : Click Add user : Configure the user as follows: Username : TestUser1 Access type : AWS Management Console access Console password : Autogenerated password Un-select Require password reset Click Next: Permissions Select the CostTest group, and click Next: Tags : Click Next: Review : Review the details and click Create user : Record the logon link , the User and the Password for later use, click Close :","title":"1. Create a group of users for testing"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#2-create-an-iam-policy-to-restrict-service-usage-by-region","text":"To manage costs you need to manage and control your usage. AWS offers multiple regions, so depending on your business requirements you can limit access to AWS services depending on the region. This can be used to ensure usage is only allowed in specific regions which are more cost effective, and minimize associated usage and cost, such as data transfer. We will create a policy that allows all EC2, RDS and S3 access in a single region only. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.","title":"2. Create an IAM Policy to restrict service usage by region"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#21-create-the-iam-policy","text":"Go to the IAM service page: Select Policies from the left menu: Click Create Policy : Click the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/Region_Restrict Click Review policy : Create the policy with the following details: Name : RegionRestrict Description : EC2, RDS, S3 access in us-east-1 only Click Create policy : You have successfully created the Policy.","title":"2.1 Create the IAM Policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#22-apply-it-to-a-group","text":"Select Groups from the left menu: Click on the CostTest group (created previously): Select the Permissions tab: Click Attach Policy : Click Policy Type and select Customer Managed : Select the checkbox next to Region_Restrict (created above) and click Attach Policy : You have successfully attached the policy to the CostTest group. Log out from the console","title":"2.2 Apply it to a group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#23-verify-the-policy-is-in-effect","text":"Logon to the console as the TestUser1 user, go to the EC2 Service dashboard: Click the current region in the top right, and select US West (N.California) : In the Old look EC2 console you will notice that there are authorization messages due to not having access in that region (the policy restricted EC2 usage to N. Virginia only), the new look EC2 console will not have errors: Try to launch an instance by clicking Launch Instance : Click on Select next to the Amazon Linux 2 AMI , You will receive an error when you select an AMI as you do not have permissions: You have successfully verified that you cannot launch any instances outside of the N.Virginia region. We will now verify we have access in us-east-1 (N.Virginia): Change the region by clicking the current region, and selecting US East (N.Virginia) : Now attempt to launch an instance, choose the Amazon Linux 2 AMI , leave 64-bit (x86) selected, click Select : Scroll down and select a c5.large , and click Review and Launch : Take note of the security group created (as you need to delete it), Click Launch : Select Proceed without a key pair , and click I acknowledge.. checkbox, and click Launch Instances : You will get a success message, click on the instance id: Ensure the correct instance is selected, click Actions , then Instance State , then Terminate : Confirm the instance ID is correct, click Yes, Terminate : You have successfully implemented an IAM policy that restricts all EC2, RDS and S3 operations to a single region. Log out of the console as TestUser1.","title":"2.3 Verify the policy is in effect"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#3-create-an-iam-policy-to-restrict-ec2-usage-by-family","text":"AWS offers different instance families within EC2. Depending on your workload requirements - different types will be most cost effective. For non-specific environments such as testing or development, you can restrict the instance families in those accounts to the most cost effective generic types. It is also an effective way to increase RI utilization, by ensuring these accounts will consume any available Reserved Instances. We will create a policy that allows operations on specific instance families only. This will not only restrict launching an instance, but all other activities. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.","title":"3. Create an IAM Policy to restrict EC2 usage by family"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#31-create-the-iam-policy","text":"Log on to the console as your regular user with the required permissions, Go to the IAM service page: Select Policies from the left menu: Click Create Policy : Click on the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/EC2Family_Restrict Click Review policy : Enter the details: Name : EC2_FamilyRestrict Description : Restrict to t3, a1 and m5 families Click on Create Policy :","title":"3.1 Create the IAM Policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#32-attach-the-policy-to-the-group","text":"Click on Groups from the left menu: Click on the CostTest group (created previously): We need to remove the RegionRestrict policy, as it permitted all EC2 actions. Click on Detach Policy for RegionRestrict : Click on Detach : Click on Attach Policy : Click on Policy Type , then click Customer Managed : Select the checkbox next to Ec2_FamilyRestrict , and click Attach Policy : Log out of the console","title":"3.2 Attach the policy to the group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#33-verify-the-policy-is-in-effect","text":"Logon to the console as the TestUser1 user, go to the EC2 Service dashboard: Click on Launch Instance : Click on Select next to the Amazon Linux 2 AMI: We will select an instance we are not able to launch first, so select a c5.large instance, click Review and Launch : Make note of the security group created, click Launch : Select Proceed without a key pair , and click I acknowledge that I will not be able to... , then click Launch Instances : You will receive an error, notice the failed step was Initiating launches . Click Back to Review Screen : Click Edit instance type : We will select an instance type we can launch (t3, a1 or m5) select t3.micro , and click Review and Launch : Select Yes, I want to continue with this instance type (t3.micro) , click Next : Click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will receive a success message. Click on the Instance ID and terminate the instance as above: You have successfully implemented an IAM policy that restricts all EC2 actions to T3, A1 and M5 instance types. Log out of the console as TestUser1.","title":"3.3 Verify the policy is in effect"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#4-extend-an-iam-policy-to-restrict-ec2-usage-by-instance-size","text":"We can also restrict the size of instance that can be launched. This can be used to ensure only low cost instances can be created within an account. This is ideal for testing and development, where high capacity instances may not be required. We will extend the EC2 family policy above, and add restrictions by adding the sizes of instances allowed.","title":"4. Extend an IAM Policy to restrict EC2 usage by instance size"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#41-extend-the-ec2family_restrict-iam-policy","text":"Log on to the console as your regular user with the required permissions, go to the IAM service page: Click on Policies on the left menu: Click on Filter policies , then select Customer managed : Click on EC2_FamilyRestrict to modify it: Click on Edit policy : Click on the JSON tab: Modify the policy by adding in the sizes, add in nano , medium , large , be careful not to change the syntax and not remove the quote characters. Click on Review policy : Click on Save changes :","title":"4.1 Extend the EC2Family_Restrict IAM Policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#42-verify-the-policy-is-in-effect","text":"Logon to the console as the TestUser1 user, click on Services and go to the EC2 dashboard: Click on Launch Instance : Click on Select next to the Amazon Linux 2 AMI : We will attempt to launch a t3.micro which was successful before. Click on Review and Launch : Review the configuration and take note of the security group created, click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : You will get a failure, as it wasn't a size we allowed in the policy. Click Back to Review Screen : Click Edit instance type : We will now select a t3.nano which will succeed. Click Review and Launch : Select Yes, I want to continue with this instance type (t3.nano) , and click Next : Review the configuration and click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : It will succeed. Click on the Instance ID and terminate the instance as above: You have successfully implemented an IAM policy that restricts all EC2 instance operations by family and size. Log out of the console as TestUser1.","title":"4.2 Verify the policy is in effect"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#5-create-an-iam-policy-to-restrict-ebs-volume-creation-by-volume-type","text":"Extending cost optimization governance beyond compute instances will ensure overall higher levels of cost optimization. Similar to EC2 instances, there are different storage types. Governing the type of storage that can be created in an account can be effective to minimize cost. We will create an IAM policy that denies operations that contain provisioned IOPS (io1) EBS volume types. This will not only restrict creating a volume, but all other actions that attempt to use this volume type. NOTE: it is best practice to provide only the minimum access required, the policy used here is for brevity and simplicity, and should only be implemented as a demonstration before being removed.","title":"5. Create an IAM policy to restrict EBS Volume creation by volume type"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#51-create-the-iam-policy","text":"Log on to the console as your regular user with the required permissions, go to the IAM service page: Click on Policies on the left menu: Click Create policy : Click on the JSON tab: Open the following text file, copy and paste the policy into the console: NOTE Ensure you copy the entire policy, including the start '{' and end '}' ./Code/EC2EBS_Restrict Click on Review Policy : Configure the following details: Name : EC2EBS_Restrict Description : Dont allow EBS io1 volumes Click Create policy :","title":"5.1 Create the IAM Policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#52-attach-the-policy-to-the-cost-optimization-group","text":"Click on Groups from the left menu: Click on the CostTest group: Click on Attach Policy : Click on Policy Type , then click Customer Managed : Select the checkbox next to EC2EBS_Restrict , and click Attach Policy : Log out from the console","title":"5.2 Attach the policy to the Cost Optimization group"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#53-verify-the-policy-is-in-effect","text":"Logon to the console as the TestUser1 user, click on Services then click EC2 : Click Launch Instance : Click Select next to Amazon Linux 2... : Select t3.nano (which is allowed as per our already applied policy, which we tested in the last exercise), click Next: Configure Instance Details : Click Next Add Storage : Click on Add New Volume , click on the dropdown , then select Provisioned IOPS SSD (io1) : Click Review and Launch : Take note of the security group created, and click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : The launch will fail, as it contained an io1 volume. Click Back to Review Screen : Scroll down and click Edit storage : Click the dropdown and change it to General Purpose SSD(gp2) , click Review and Launch : Click Launch : Select Proceed without a key pair , and click I acknowledge that i will not be able to... , then click Launch Instances : It will now succeed, as it doesn't contain an io1 volume type. Click on the instance ID and terminate the instance as above: You have successfully implemented an IAM policy that denies operations if there is an EBS volume of type io1. Log out of the console as TestUser1.","title":"5.3 Verify the policy is in effect"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#6-tear-down","text":"Log onto the console as your regular user with the required permissions.","title":"6. Tear down"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#delete-a-policy","text":"We will delete the IAM policies created above, as they are no longer applied to any groups. Go to the IAM Console: Click on Policies on the left: 3.Click on Filter Policies and select Customer managed : Select the policy you want to delete Region_Restrict : Click on Policy actions , and select Delete : Click on Delete : Perform the same steps above to delete the Ec2_FamilyRestrict and EC2EBS_Restrict policies. Click on Groups : Select the CostTest group, click Group Actions , click Delete Group : Click Yes, Delete : Click Users : Select TestUser1 , and click Delete user : Click Yes, delete : Go to the EC2 dashboard: Click Security Groups on the left: Select the security groups you took note of , ensure you have the correct groups that were created. Click Actions , select Delete Security Groups : Triple check they are the groups you wrote down, and click Yes, Delete : Confirm there are no unattached EBS volumes, go to the EC2 dashboard , click on Elastic Block Store , click Volumes . You can sort by the Created column to help identify volumes that were not terminated as part of this lab.","title":"Delete a policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Lab_Guide.html#7-rate-this-lab","text":"","title":"7. Rate this lab"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Code/EC2EBS_Restrict.html","text":"{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Deny\", \"Action\": \"ec2:*\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"ec2:VolumeType\": \"io1\" } } } ] }","title":"EC2EBS Restrict"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Code/EC2Family_Restrict.html","text":"{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"ec2:*\", \"Resource\": \"*\", \"Condition\": { \"ForAllValues:StringLike\": { \"ec2:InstanceType\": [ \"t3.*\", \"a1.*\", \"m5.*\" ] } } } ] }","title":"EC2Family Restrict"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Code/IAM_policy.html","text":"NOTE: Policy is only required to complete the exercise to create a cost optimization team. It can be delelted once the first exercise is complete. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"iam:ListPolicies\", \"iam:GetPolicyVersion\", \"iam:CreateGroup\", \"iam:GetPolicy\", \"iam:DeletePolicy\", \"iam:DetachGroupPolicy\", \"iam:ListGroupPolicies\", \"iam:AttachUserPolicy\", \"iam:CreateUser\", \"iam:GetGroup\", \"iam:CreatePolicy\", \"iam:CreateLoginProfile\", \"iam:AddUserToGroup\", \"iam:ListPolicyVersions\", \"iam:AttachGroupPolicy\", \"iam:ListUsers\", \"iam:ListAttachedGroupPolicies\", \"iam:ListGroups\", \"iam:GetGroupPolicy\", \"iam:CreatePolicyVersion\", \"iam:DeletePolicyVersion\", \"iam:GetLoginProfile\" ], \"Resource\": \"*\" } ] }","title":"IAM policy"},{"location":"Cost/Cost_Fundamentals/200_2_Cost_and_Usage_Governance/Code/Region_Restrict.html","text":"{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ec2:*\", \"rds:*\", \"s3:*\" ], \"Resource\": \"*\", \"Condition\": {\"StringEquals\": {\"aws:RequestedRegion\": \"us-east-1\"}} } ] }","title":"Region Restrict"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html","text":"Level 200: Pricing Models https://wellarchitectedlabs.com Introduction In a highly dynamic cloud environment it can be challenging to forecast your usage. This hands-on lab will guide you through the steps to perform a Reserved Instance analysis, and make low risk, high return RI purchases at scale. The skills you learn will help you ensure your workloads utilize different pricing models in alignment with the AWS Well-Architected Framework. Goals Perform a Reserved Instance analysis Filter and sort the recommendations Prerequisites An AWS Account Cost_and_Usage_Governance has been completed Permissions required Log in as the Cost Optimization team, created in AWS Account Setup and modified in - Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Best Practice Checklist [ ] Review RI Recommendations [ ] Sort and filter RI Recommendations across an account [ ] Prepare a final list of low risk, high return RI's based on usage patterns License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#level-200-pricing-models","text":"https://wellarchitectedlabs.com","title":"Level 200: Pricing Models"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#introduction","text":"In a highly dynamic cloud environment it can be challenging to forecast your usage. This hands-on lab will guide you through the steps to perform a Reserved Instance analysis, and make low risk, high return RI purchases at scale. The skills you learn will help you ensure your workloads utilize different pricing models in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#goals","text":"Perform a Reserved Instance analysis Filter and sort the recommendations","title":"Goals"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#prerequisites","text":"An AWS Account Cost_and_Usage_Governance has been completed","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#permissions-required","text":"Log in as the Cost Optimization team, created in AWS Account Setup and modified in - Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#best-practice-checklist","text":"[ ] Review RI Recommendations [ ] Sort and filter RI Recommendations across an account [ ] Prepare a final list of low risk, high return RI's based on usage patterns","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html","text":"Level 200: Pricing Models Authors Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect Paul Lambden, Principal Technical Account Manager Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents View an RI report Download and prepare the RI CSV files Sort and filter the RI CSV files Teardown Rate this Lab 1. View an RI report We are going to view the RI reports within AWS Cost Explorer, to understand the recommendations and possible purchases we should make. Log into the console as an IAM user with the required permissions, go to the AWS Cost Explorer service page: In the left menu select Recommendations : Click View all next to Reservation purchase recommendations : On the right select the filters: RI term 1 year, Payment Option (your preference), Based on the past 7 days: The top section will show the estimated savings and number of recommendations, take note of the Purchase Recommendations On the right select the filter: Based on the past 30 days: View the Purcahse Recommendations , if the 30 day recommendation is less than 7 days recommendation - your usage is increasing and the recommendations are lower risk. If the 7 days recommendation is less than 30 days, then your usage is decreasing and you need to look further into your usage patterns to see which RI's would be suitable. 2. Download and prepare the RI CSV files 1 - Download the CSV for both the 7 day and 30 day recommendation files, by selecting the filter 7 days or 30 days , and clicking on Download CSV : 2 - If you do not have sufficient usage, you can download the two sample files: Ctrl-click to open them in a new tab, then copy the text and paste it into a spreadsheet application. Paste the 30day recommendations into one worksheet, and the 7day recommendations into another worksheet called 7Day Rec , in the same spreadsheet. 7_day_EC2_R_Rec.csv 30_day_EC2_R_Rec.csv 3 - Create a new column called RI ID to the left of the Recommendation column on both 30Day and 7Day sheets, which is a unique identifier of the RI Type, the formula for this cell will concatenate the columns: Instance Type , Location , OS and Tenancy . On row 5 of the sample files, paste the formula below. NOTE If using your own files modify the row numbers from 5 to 2 . =CONCATENATE(C5,L5,M5,N5) 4 - Add a column in the 30Day worksheet to the right of the Recommendation column. This will be for the 7Day recommendations. Add a VLOOKUP formula to get the values from the 7Day worksheet, modify this formula for the number of rows you require (U column). Paste this on row 5 of the sample files, or modify if using your own: =VLOOKUP(T5,'7Day Rec'!T$4:U$48,2,FALSE) 5 - We will now create a Fully Paid Day column. This shows us how long it will take to pay off the full term of the RI, and will help to measure risk. The closer to 12months the fully paid day is, the higher the risk. This is because it takes longer to pay off the investment/commitment. The break even is the wrong measure, as it only shows how quickly you pay off the upfront component, and not the full amount. Paste the following formula into the last column z : =(R5+S5*12)/(R5/12+S5+W5) The formula is: (yearly RI cost) / (monthly on-demand cost) (Upfront cost + recurring monthly cost x 12) / (upfront cost/12 + recurring monthly cost + estimated monthly savings) 6 - Delete the following columns as they are not necessary: Recommendation Date , Size Flexible Recommendation , Max hourly normalized unit usage in Historical Period , Min hourly normalized unit usage in Historical Period , Average hourly normalized unit usage in Historical Period , Payment Option , Break Even Months . We now have the required data required to be able to analyze, and filter out the high risk and low return RIs. 3. Sort and filter the RI CSV files RI purchases should be done frequently (bi-weekly or monthly), so for each cycle we want: low risk and high return purchases, and purchase the top 50-75% of recommendations. This will ensure you have sufficiently high coverage, while minimizing the risk of unused RIs. 3.1 Filter out low risk, and high return RIs 1 - To get the lowest risk, we sort by Fully Paid Day smallest to largest, as these will be fully paid off in the shortest amount of time. You can see that some of the RI's below are fully paid off in around 7months - so if they are used for 7 months - they have paid themselves off completely. 2 - We will separate the very low, low, and medium risk recommendations. Add in some empty lines between Fully Paid Day of 8, 10, and copy the header line across: 3 - We have categorized the risk, so we will now look for the highest return recommendations in each category. Sort each of the three groups by Estimated Monthly Savings , largest to smallest : 4 - Depending on your usage and business, chose a minimum estimated monthly savings - a typical value for larger customers is in the range of $50-100. While they save money, these recommendations do not save enough - aim for the top 50-70% of recommendations. We have chosen $100, grey out anything less than this: 3.2 Filter out usage patterns It would be a large amount of effort to view the daily usage patterns over the month for every recommendation - checking for declining usage or erratic usage, but we can do this programatically. By looking at the columns, we can assess the underlying usage pattern. 1 - If the Max hourly usage is close to Min hourly usage , within 75-100% - then the usage would be relatively flat, with low variance. Go through and highlight these cells green. You could do this with a formula, but a very fast judgment is ok: 2 - If the Average hourly usage is close to the Max hourly usage , then the minimum was only a small duration, so highlight anything green where the Average is roughly within 75-100% of the Max : 3 - Minimum utilization required for an RI varies by the discount level. The lowest discount level is approximately 20%, so we would look for a minimum utilization of >80%. While this is reflected through the Fully Paid Day (if utilization is low, Fully Paid would be very late), we'll double check & filter out only the very high utilization. Highlight anything above 90% in green: 4 - Now we look for a declining usage pattern. If the recommendation for the last 7 days is less than the 30 days, usage is declining - and you should consult your business to determine if usage will continue to fall. If the 7day Recommended Instance Quantity is equal or more than the 30day Recommended Instance Quantity then highlight the cell green: 5 - Now we will see if the recommendation is close to the average, if its not then usage is varying. If the recommendation is NOT above, equal or close to the average (within 10%) then remove the highlighting from the recommendation column: The processed sample files are available here: - Combined_EC2_RI_Rec.xls 3.3 Making recommendations We look at each of the risk categories as follows: 1 - Very low and low risk For any recommendations that are highlighted in the 7Day column, recommend the lowest of the 30Day or 7Day Columns. For any recommendations that are highlighted in the Average hourly usage and Projected RI Utilization , select a percentage of either the 30Day or 7Day column (which ever is lower). 2 - Medium risk From the recommendations highlighted in the 7Day column, select a portion of these on a case by case basis based on business knowledge Other suggestions for recommendations that do not fall into the categories above: Re-evaluate in another 7-14 days to observe the usage trend Purchase a lower percentage of the average hourly Purchase a higher percentage of the minimum hourly You have successfully filtered and processed all the recommendations. You can now make low risk and high return recommendations that are suitable based on your ongoing usage patterns with high confidence. You can then take those recommendations, and purchase the quantity in the required accounts, with the required payment option (All upfront, Partial upfront, No upfront), and class (standard or convertible). 4. Teardown There are no resources or configuration items that are created during this workshop. 5. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#level-200-pricing-models","text":"","title":"Level 200: Pricing Models"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect Paul Lambden, Principal Technical Account Manager","title":"Authors"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#table-of-contents","text":"View an RI report Download and prepare the RI CSV files Sort and filter the RI CSV files Teardown Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#1-view-an-ri-report","text":"We are going to view the RI reports within AWS Cost Explorer, to understand the recommendations and possible purchases we should make. Log into the console as an IAM user with the required permissions, go to the AWS Cost Explorer service page: In the left menu select Recommendations : Click View all next to Reservation purchase recommendations : On the right select the filters: RI term 1 year, Payment Option (your preference), Based on the past 7 days: The top section will show the estimated savings and number of recommendations, take note of the Purchase Recommendations On the right select the filter: Based on the past 30 days: View the Purcahse Recommendations , if the 30 day recommendation is less than 7 days recommendation - your usage is increasing and the recommendations are lower risk. If the 7 days recommendation is less than 30 days, then your usage is decreasing and you need to look further into your usage patterns to see which RI's would be suitable.","title":"1. View an RI report"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#2-download-and-prepare-the-ri-csv-files","text":"1 - Download the CSV for both the 7 day and 30 day recommendation files, by selecting the filter 7 days or 30 days , and clicking on Download CSV : 2 - If you do not have sufficient usage, you can download the two sample files: Ctrl-click to open them in a new tab, then copy the text and paste it into a spreadsheet application. Paste the 30day recommendations into one worksheet, and the 7day recommendations into another worksheet called 7Day Rec , in the same spreadsheet. 7_day_EC2_R_Rec.csv 30_day_EC2_R_Rec.csv 3 - Create a new column called RI ID to the left of the Recommendation column on both 30Day and 7Day sheets, which is a unique identifier of the RI Type, the formula for this cell will concatenate the columns: Instance Type , Location , OS and Tenancy . On row 5 of the sample files, paste the formula below. NOTE If using your own files modify the row numbers from 5 to 2 . =CONCATENATE(C5,L5,M5,N5) 4 - Add a column in the 30Day worksheet to the right of the Recommendation column. This will be for the 7Day recommendations. Add a VLOOKUP formula to get the values from the 7Day worksheet, modify this formula for the number of rows you require (U column). Paste this on row 5 of the sample files, or modify if using your own: =VLOOKUP(T5,'7Day Rec'!T$4:U$48,2,FALSE) 5 - We will now create a Fully Paid Day column. This shows us how long it will take to pay off the full term of the RI, and will help to measure risk. The closer to 12months the fully paid day is, the higher the risk. This is because it takes longer to pay off the investment/commitment. The break even is the wrong measure, as it only shows how quickly you pay off the upfront component, and not the full amount. Paste the following formula into the last column z : =(R5+S5*12)/(R5/12+S5+W5) The formula is: (yearly RI cost) / (monthly on-demand cost) (Upfront cost + recurring monthly cost x 12) / (upfront cost/12 + recurring monthly cost + estimated monthly savings) 6 - Delete the following columns as they are not necessary: Recommendation Date , Size Flexible Recommendation , Max hourly normalized unit usage in Historical Period , Min hourly normalized unit usage in Historical Period , Average hourly normalized unit usage in Historical Period , Payment Option , Break Even Months . We now have the required data required to be able to analyze, and filter out the high risk and low return RIs.","title":"2. Download and prepare the RI CSV files"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#3-sort-and-filter-the-ri-csv-files","text":"RI purchases should be done frequently (bi-weekly or monthly), so for each cycle we want: low risk and high return purchases, and purchase the top 50-75% of recommendations. This will ensure you have sufficiently high coverage, while minimizing the risk of unused RIs.","title":"3. Sort and filter the RI CSV files"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#31-filter-out-low-risk-and-high-return-ris","text":"1 - To get the lowest risk, we sort by Fully Paid Day smallest to largest, as these will be fully paid off in the shortest amount of time. You can see that some of the RI's below are fully paid off in around 7months - so if they are used for 7 months - they have paid themselves off completely. 2 - We will separate the very low, low, and medium risk recommendations. Add in some empty lines between Fully Paid Day of 8, 10, and copy the header line across: 3 - We have categorized the risk, so we will now look for the highest return recommendations in each category. Sort each of the three groups by Estimated Monthly Savings , largest to smallest : 4 - Depending on your usage and business, chose a minimum estimated monthly savings - a typical value for larger customers is in the range of $50-100. While they save money, these recommendations do not save enough - aim for the top 50-70% of recommendations. We have chosen $100, grey out anything less than this:","title":"3.1 Filter out low risk, and high return RIs"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#32-filter-out-usage-patterns","text":"It would be a large amount of effort to view the daily usage patterns over the month for every recommendation - checking for declining usage or erratic usage, but we can do this programatically. By looking at the columns, we can assess the underlying usage pattern. 1 - If the Max hourly usage is close to Min hourly usage , within 75-100% - then the usage would be relatively flat, with low variance. Go through and highlight these cells green. You could do this with a formula, but a very fast judgment is ok: 2 - If the Average hourly usage is close to the Max hourly usage , then the minimum was only a small duration, so highlight anything green where the Average is roughly within 75-100% of the Max : 3 - Minimum utilization required for an RI varies by the discount level. The lowest discount level is approximately 20%, so we would look for a minimum utilization of >80%. While this is reflected through the Fully Paid Day (if utilization is low, Fully Paid would be very late), we'll double check & filter out only the very high utilization. Highlight anything above 90% in green: 4 - Now we look for a declining usage pattern. If the recommendation for the last 7 days is less than the 30 days, usage is declining - and you should consult your business to determine if usage will continue to fall. If the 7day Recommended Instance Quantity is equal or more than the 30day Recommended Instance Quantity then highlight the cell green: 5 - Now we will see if the recommendation is close to the average, if its not then usage is varying. If the recommendation is NOT above, equal or close to the average (within 10%) then remove the highlighting from the recommendation column: The processed sample files are available here: - Combined_EC2_RI_Rec.xls","title":"3.2 Filter out usage patterns"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#33-making-recommendations","text":"We look at each of the risk categories as follows: 1 - Very low and low risk For any recommendations that are highlighted in the 7Day column, recommend the lowest of the 30Day or 7Day Columns. For any recommendations that are highlighted in the Average hourly usage and Projected RI Utilization , select a percentage of either the 30Day or 7Day column (which ever is lower). 2 - Medium risk From the recommendations highlighted in the 7Day column, select a portion of these on a case by case basis based on business knowledge Other suggestions for recommendations that do not fall into the categories above: Re-evaluate in another 7-14 days to observe the usage trend Purchase a lower percentage of the average hourly Purchase a higher percentage of the minimum hourly You have successfully filtered and processed all the recommendations. You can now make low risk and high return recommendations that are suitable based on your ongoing usage patterns with high confidence. You can then take those recommendations, and purchase the quantity in the required accounts, with the required payment option (All upfront, Partial upfront, No upfront), and class (standard or convertible).","title":"3.3 Making recommendations"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#4-teardown","text":"There are no resources or configuration items that are created during this workshop.","title":"4. Teardown"},{"location":"Cost/Cost_Fundamentals/200_3_Pricing_Models/Lab_Guide.html#5-rate-this-lab","text":"","title":"5. Rate this lab"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html","text":"Level 200: Cost and Usage Analysis https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to setup a platform to analyze your cost and usage reports. The skills you learn will help you perform analysis on your cost and usage, in alignment with the AWS Well-Architected Framework. Goals Setup an analysis platform for your cost and usage data Perform basic analysis of your cost and usage Prerequisites A master AWS Account A linked AWS Account (preferred, not mandatory) Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup AWS Account Setup has been completed Have usage that is tagged (preferred, not mandatory) Permissions required Log in as the Cost Optimization team, created in AWS Account Setup NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Costs Variable depending on bill size and analysis performed Time to complete The lab should take approximately 20 minutes to complete Start the Lab! Best Practice Checklist [ ] Load your CUR files into Athena for analysis [ ] Create a separate table to contain only a specific member accounts usage [ ] Analyze your cost and usage by executing queries against your CUR files License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#level-200-cost-and-usage-analysis","text":"https://wellarchitectedlabs.com","title":"Level 200: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#introduction","text":"This hands-on lab will guide you through the steps to setup a platform to analyze your cost and usage reports. The skills you learn will help you perform analysis on your cost and usage, in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#goals","text":"Setup an analysis platform for your cost and usage data Perform basic analysis of your cost and usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#prerequisites","text":"A master AWS Account A linked AWS Account (preferred, not mandatory) Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup AWS Account Setup has been completed Have usage that is tagged (preferred, not mandatory)","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#permissions-required","text":"Log in as the Cost Optimization team, created in AWS Account Setup NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#costs","text":"Variable depending on bill size and analysis performed","title":"Costs"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#time-to-complete","text":"The lab should take approximately 20 minutes to complete","title":"Time to complete"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#best-practice-checklist","text":"[ ] Load your CUR files into Athena for analysis [ ] Create a separate table to contain only a specific member accounts usage [ ] Analyze your cost and usage by executing queries against your CUR files","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html","text":"Level 200: Cost and Usage Analysis Authors Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Verify your CUR files are being delivered Use AWS Glue to enable access to CUR files via Amazon Athena Cost and Usage analysis Tear down Rate this Lab 1. Verify your CUR files are being delivered We will verify the CUR files are being delivered, they are in the correct format and the region they are in. Log into the console as an IAM user with the required permissions, go to the Billing console, and view the CUR report you created in AWS Account Setup , confirm the S3 bucket , and Report path prefix : Go to the S3 Service console: Verify the region where the bucket is located (here it is US East N.Virginia ), and click on the bucket name where the report is delivered(it is blacked out here): You should see a aws-programmatic-access-test-object which was put there to verify AWS can deliver reports, and also the folder which is the report prefix - cur . Click on the folder name for the prefix (here it is cur): Click on the folder name which is also part of the prefix (here it is WorkshopCUR): Click on the prefix folder, here it is WorkshopCUR, then drill down in the current year and month: You can see the delivered CUR file, it is in the parquet format: You have successfully verified that the CUR files are being delivered and in the correct format. Sample Files You may not have substantial or interesting usage, in this case there are sample files that you can use in the code section. You will need to create the required structure in S3, download these files and then upload them into s3. NOTE : Do not save the links below, open them in a new window and download the files. They should be approximately 1Mb in size each, if you have files that are 65kb - then you have downloaded the web page and not the parquet files. Create a folder structure, such as -bucket name-/cur/WorkshopCUR/WorkshopCUR/year=2018/month=12 and copy the parquet files below into each months folder. October 2018 Usage November 2018 Usage December 2018 Usage 2. Use AWS Glue to enable access to CUR files via Amazon Athena We will use AWS Glue and setup a scheduled Crawler, which will run each day. This crawler will scan the CUR files and create a database and tables for the delivered files. If there are new versions of a CUR, or new months delivered - they will be automatically included. We will use Athena to access and view our CUR files via SQL. Athena is a serverless solution to be able to execute SQL queries across very large amounts of data. Athena is only charged for data that is scanned, and there are no ongoing costs if data is not being queried, unlike a traditional database solution. 1 - Go to the Glue console: 2 - Click on Get started if you have not used Glue before 3 - Ensure you are in the region where your CUR files are delivered, click on Crawlers and click Add crawler : 4 - Enter a Crawler name and click Next : 5 - Select Data stores , and click Next : 6 - Ensure you select Specified path in my account , and click the Folder icon : 7 - Select the bucket with the CUR files, and click Select : 8 - Enter the following exclude patterns (1 per line), and click Next : **.json, **.yml, **.sql, **.csv, **.gz, **.zip 9 - Click Next : 10 - Select Create an IAM role , enter a role name , and click Next : 11 - Click the Down arrow , and select a Daily Frequency: 12 - Enter in a Start Hour and Start Minute , then click Next : 13 - Click Add database : 14 - Enter a Database name , and click Create , do NOT use a hyphen character '-': 15 - Click Next : 16 - Review the crawler and click Finish : 17 - Select the checkbox next to the crawler, click Run crawler : 18 - You will see the Crawler was successful and created a table: 19 - Click Databases 20 - Select the CUR database that Glue created: 21 - Click Tables in cur : 22 - Click the table name: 24 - Verify the recordCount is not zero, if it is - go back and verify the steps above: 25 - Go to the Athena Console: 26 - Select the drop down arrow, and click on the new database: 27 - A new table called workshop_c_u_r will have been created, we will now load the partitions. Click on the 3 dot menu and select Load partitions : 22 - You will see it execute the command MSCK REPAIR TABLE , and in the results it may add partitions to the metastore for each month that has a billing file: NOTE: It may or may not add partitions and show teh messages as per the image above. Check - The folder names year and month are in S3 and the case matches - There are parquet files in each of the month folders 23 - We will now preview the data. Click on the 3 dot menu and select Preview table : 24 - It will execute a Select * from query, and in the results you will see the first 10 lines of your CUR file: 25 - (Optional if you have a linked account) We will create a member account table, this is for large organizations or partners - that want only a single accounts usage to be visible to them. Copy and paste the following code: CREATE TABLE linked_AccountID WITH ( format = 'Parquet', parquet_compression = 'SNAPPY') AS SELECT * FROM \"workshopcur\".\"workshop_c_u_r\" where \"line_item_usage_account_id\" = 'AccountID' NOTE: replace AccountID with the 12 digit account ID of your member account. You will see a new table created on the left: NOTE: You can restrict and grant access to this specific member account table through IAM policies. This will be covered in the 300 level billing analysis lab You have successfully setup your CUR file to be analyzed. You can now query your usage and costs via SQL. 3. Cost and Usage analysis We will now perform some common analysis of your usage through SQL queries. You will be charged for Athena usage by the amount of data that is scanned - the source files are monthly, and in parquet format - which is compressed and partitioned to minimise cost. Be careful to include limit 10 or similar at the end of your queries to limit the amount of data that comes back. NOTE : You may need to change the database and table name depending on how you configured your CUR. Replace workshopcur.workshop_c_u_r with your database and table name. For each of the queries below, copy and paste each query into the query window, click Run query and view the results. We will restrict the queries to a single month (December, 2018) by including the following line: where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 3.1 What data is available in the CUR file? We will learn how to find out what data is available for querying in the CUR files, this will show what columns there are and some sample data in those columns. What columns and data are in the CUR table? select * from \"workshopcur\".\"workshop_c_u_r\" where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; What are all the different values in a column? (the column we use is line_item_line_item_description ) select distinct \"line_item_line_item_description\" from \"workshopcur\".\"workshop_c_u_r\" where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; 3 Give me all columns from the CUR, where a specific value is in a column (here the column line_item_line_item_type contains the word Usage somewhere, note the capital 'U'): select * from \"workshopcur\".\"workshop_c_u_r\" where \"line_item_line_item_type\" like '%Usage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; What billing periods are available? SELECT distinct bill_billing_period_start_date FROM \"workshopcur\".\"workshop_c_u_r\" limit 10; 3.2 Top Costs To efficiently optimize its useful to view the top costs in different categories, such as service, description or tags. Top10 Costs by AccountID: select \"line_item_usage_account_id\", round(sum(\"line_item_unblended_cost\"),2) as cost from \"workshopcur\".\"workshop_c_u_r\" where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"line_item_usage_account_id\" order by cost desc limit 10; Top10 Costs by Product: select \"line_item_product_code\", round(sum(\"line_item_unblended_cost\"),2) as cost from \"workshopcur\".\"workshop_c_u_r\" where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"line_item_product_code\" order by cost desc limit 10; Top Costs by Line Item Description select \"line_item_product_code\", \"line_item_line_item_description\", round(sum(\"line_item_unblended_cost\"),2) as cost from \"workshopcur\".\"workshop_c_u_r\" where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"line_item_product_code\", \"line_item_line_item_description\" order by cost desc limit 10; Top EC2 Costs select \"line_item_product_code\", \"line_item_line_item_description\", round(sum(\"line_item_unblended_cost\"),2) as cost from \"workshopcur\".\"workshop_c_u_r\" where \"line_item_product_code\" like '%AmazonEC2%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"line_item_product_code\", \"line_item_line_item_description\" order by cost desc limit 10; Top EC2 OnDemand Costs select \"line_item_product_code\", \"line_item_line_item_description\", round(sum(\"line_item_unblended_cost\"),2) as cost from \"workshopcur\".\"workshop_c_u_r\" where \"line_item_product_code\" like '%AmazonEC2%' and \"line_item_usage_type\" like '%BoxUsage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"line_item_product_code\", \"line_item_line_item_description\" order by cost desc limit 10; 3.3 Tagging and Cost Attribution Common in large organizations is the requirement to allocate costs back to specific business units. It is also critical for optimization to be able to allocate costs to workloads, to measure workload efficiency. NOTE : This will only work if you have tags enabled in your billing files, and they are the same as the examples here - resource_tags_user_cost_center Top 20 Costs by line item description and CostCenter Tag SELECT \"bill_payer_account_id\", \"product_product_name\", \"line_item_usage_type\", \"line_item_line_item_description\", resource_tags_user_cost_center, round(sum(line_item_unblended_cost),2) as cost FROM \"workshopcur\".\"workshop_c_u_r\" where length(\"resource_tags_user_cost_center\") >0 and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"resource_tags_user_cost_center\", \"bill_payer_account_id\", \"product_product_name\", \"line_item_usage_type\", \"line_item_line_item_description\" order by cost desc limit 20 Top 20 costs by line item description, without a CostCenter Tag SELECT \"bill_payer_account_id\", \"product_product_name\", \"line_item_usage_type\", \"line_item_line_item_description\", round(sum(line_item_unblended_cost),2) as cost FROM \"workshopcur\".\"workshop_c_u_r\" where length(\"resource_tags_user_cost_center\") = 0 and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"bill_payer_account_id\", \"product_product_name\", \"line_item_usage_type\", \"line_item_line_item_description\" order by cost desc limit 20 3.4 Reserved Instance, On Demand and Spot Usage To improve the use of pricing models across a business, these queries can assist to highlight the top opportunities for Reserved Instance (top On Demand cost), and also identify who is successful with pricing models (Top users of spot). NOTE : You will need specific usage in your account that matches the instance types below, for this to work correctly. Who used Reserved Instances Identify which accounts used the available RIs, and what they would have paid with public pricing. Ideal for chargeback within an organization. select \"bill_payer_account_id\", \"bill_billing_period_start_date\", \"line_item_usage_account_id\", \"reservation_reservation_a_r_n\", \"line_item_product_code\", \"line_item_usage_type\", sum(\"line_item_usage_amount\") as Usage, \"line_item_unblended_rate\", sum(\"line_item_unblended_cost\") as Cost, \"line_item_line_item_description\", \"pricing_public_on_demand_rate\", sum(\"pricing_public_on_demand_cost\") as PublicCost from \"workshopcur\".\"workshop_c_u_r\" where \"line_item_line_item_Type\" like '%DiscountedUsage%' group by \"bill_payer_account_id\", \"bill_billing_period_start_date\", \"line_item_usage_account_id\", \"reservation_reservation_a_r_n\", \"line_item_product_code\", \"line_item_usage_type\", \"line_item_unblended_rate\", \"line_item_line_item_description\", \"pricing_public_on_demand_rate\" T2 family instance usage Observe how much is being spent on each different family (usage type) and how much is covered by Reserved instances. select \"line_item_usage_type\", sum(\"line_item_usage_amount\") as usage, round(sum(\"line_item_unblended_cost\"),2) as cost from \"workshopcur\".\"workshop_c_u_r\" where \"line_item_usage_type\" like '%t2.%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"line_item_usage_type\" order by \"line_item_usage_type\" Costs By running type Divide the cost by usage (hrs), and see how much is being spent per hour on each of the usage types. Compare BoxUsage (On Demand), to HeavyUsage (Reserved instance), to SpotUsage (Spot). select \"line_item_usage_type\", round(sum(\"line_item_usage_amount\"),2) as usage, round(sum(\"line_item_unblended_cost\"),2) as cost, round(avg(\"line_item_unblended_cost\"/\"line_item_usage_amount\"),4) as hourly_rate from \"workshopcur\".\"workshop_c_u_r\" where \"line_item_product_code\" like '%AmazonEC2%' and \"line_item_usage_type\" like '%Usage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"line_item_usage_type\" order by \"line_item_usage_type\" Show unused Reserved Instances This will show how much of your reserved instances are not being used, and sorts it via cost of unused portion (recurring fee). You can use this in two ways: See where you have spare RI's and modify instances to match, so they will use the RIs Convert your existing RI's if possible select bill_billing_period_start_date, product_region, line_item_usage_type, reservation_reservation_a_r_n, reservation_unused_quantity, reservation_unused_recurring_fee from \"workshopcur\".\"workshop_c_u_r\" where length(reservation_reservation_a_r_n) > 0 and reservation_unused_quantity > 0 order by bill_billing_period_start_date, reservation_unused_recurring_fee desc 4. Tear down Amazon Athena only charges when it is being used, i.e. data is being scanned - so if it is not being actively queried, there are no charges. There may be some charges from AWS Glue if it is above the free tier limit. It is also best practice to regularly analyze your usage and cost, so there is no teardown for this lab. 5. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#level-200-cost-and-usage-analysis","text":"","title":"Level 200: Cost and Usage Analysis"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead, Well-Architected Spencer Marley, Commercial Architect","title":"Authors"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#table-of-contents","text":"Verify your CUR files are being delivered Use AWS Glue to enable access to CUR files via Amazon Athena Cost and Usage analysis Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#1-verify-your-cur-files-are-being-delivered","text":"We will verify the CUR files are being delivered, they are in the correct format and the region they are in. Log into the console as an IAM user with the required permissions, go to the Billing console, and view the CUR report you created in AWS Account Setup , confirm the S3 bucket , and Report path prefix : Go to the S3 Service console: Verify the region where the bucket is located (here it is US East N.Virginia ), and click on the bucket name where the report is delivered(it is blacked out here): You should see a aws-programmatic-access-test-object which was put there to verify AWS can deliver reports, and also the folder which is the report prefix - cur . Click on the folder name for the prefix (here it is cur): Click on the folder name which is also part of the prefix (here it is WorkshopCUR): Click on the prefix folder, here it is WorkshopCUR, then drill down in the current year and month: You can see the delivered CUR file, it is in the parquet format: You have successfully verified that the CUR files are being delivered and in the correct format. Sample Files You may not have substantial or interesting usage, in this case there are sample files that you can use in the code section. You will need to create the required structure in S3, download these files and then upload them into s3. NOTE : Do not save the links below, open them in a new window and download the files. They should be approximately 1Mb in size each, if you have files that are 65kb - then you have downloaded the web page and not the parquet files. Create a folder structure, such as -bucket name-/cur/WorkshopCUR/WorkshopCUR/year=2018/month=12 and copy the parquet files below into each months folder. October 2018 Usage November 2018 Usage December 2018 Usage","title":"1. Verify your CUR files are being delivered"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#2-use-aws-glue-to-enable-access-to-cur-files-via-amazon-athena","text":"We will use AWS Glue and setup a scheduled Crawler, which will run each day. This crawler will scan the CUR files and create a database and tables for the delivered files. If there are new versions of a CUR, or new months delivered - they will be automatically included. We will use Athena to access and view our CUR files via SQL. Athena is a serverless solution to be able to execute SQL queries across very large amounts of data. Athena is only charged for data that is scanned, and there are no ongoing costs if data is not being queried, unlike a traditional database solution. 1 - Go to the Glue console: 2 - Click on Get started if you have not used Glue before 3 - Ensure you are in the region where your CUR files are delivered, click on Crawlers and click Add crawler : 4 - Enter a Crawler name and click Next : 5 - Select Data stores , and click Next : 6 - Ensure you select Specified path in my account , and click the Folder icon : 7 - Select the bucket with the CUR files, and click Select : 8 - Enter the following exclude patterns (1 per line), and click Next : **.json, **.yml, **.sql, **.csv, **.gz, **.zip 9 - Click Next : 10 - Select Create an IAM role , enter a role name , and click Next : 11 - Click the Down arrow , and select a Daily Frequency: 12 - Enter in a Start Hour and Start Minute , then click Next : 13 - Click Add database : 14 - Enter a Database name , and click Create , do NOT use a hyphen character '-': 15 - Click Next : 16 - Review the crawler and click Finish : 17 - Select the checkbox next to the crawler, click Run crawler : 18 - You will see the Crawler was successful and created a table: 19 - Click Databases 20 - Select the CUR database that Glue created: 21 - Click Tables in cur : 22 - Click the table name: 24 - Verify the recordCount is not zero, if it is - go back and verify the steps above: 25 - Go to the Athena Console: 26 - Select the drop down arrow, and click on the new database: 27 - A new table called workshop_c_u_r will have been created, we will now load the partitions. Click on the 3 dot menu and select Load partitions : 22 - You will see it execute the command MSCK REPAIR TABLE , and in the results it may add partitions to the metastore for each month that has a billing file: NOTE: It may or may not add partitions and show teh messages as per the image above. Check - The folder names year and month are in S3 and the case matches - There are parquet files in each of the month folders 23 - We will now preview the data. Click on the 3 dot menu and select Preview table : 24 - It will execute a Select * from query, and in the results you will see the first 10 lines of your CUR file: 25 - (Optional if you have a linked account) We will create a member account table, this is for large organizations or partners - that want only a single accounts usage to be visible to them. Copy and paste the following code: CREATE TABLE linked_AccountID WITH ( format = 'Parquet', parquet_compression = 'SNAPPY') AS SELECT * FROM \"workshopcur\".\"workshop_c_u_r\" where \"line_item_usage_account_id\" = 'AccountID' NOTE: replace AccountID with the 12 digit account ID of your member account. You will see a new table created on the left: NOTE: You can restrict and grant access to this specific member account table through IAM policies. This will be covered in the 300 level billing analysis lab You have successfully setup your CUR file to be analyzed. You can now query your usage and costs via SQL.","title":"2. Use AWS Glue to enable access to CUR files via Amazon Athena"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#3-cost-and-usage-analysis","text":"We will now perform some common analysis of your usage through SQL queries. You will be charged for Athena usage by the amount of data that is scanned - the source files are monthly, and in parquet format - which is compressed and partitioned to minimise cost. Be careful to include limit 10 or similar at the end of your queries to limit the amount of data that comes back. NOTE : You may need to change the database and table name depending on how you configured your CUR. Replace workshopcur.workshop_c_u_r with your database and table name. For each of the queries below, copy and paste each query into the query window, click Run query and view the results. We will restrict the queries to a single month (December, 2018) by including the following line: where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018","title":"3. Cost and Usage analysis"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#31-what-data-is-available-in-the-cur-file","text":"We will learn how to find out what data is available for querying in the CUR files, this will show what columns there are and some sample data in those columns. What columns and data are in the CUR table? select * from \"workshopcur\".\"workshop_c_u_r\" where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; What are all the different values in a column? (the column we use is line_item_line_item_description ) select distinct \"line_item_line_item_description\" from \"workshopcur\".\"workshop_c_u_r\" where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; 3 Give me all columns from the CUR, where a specific value is in a column (here the column line_item_line_item_type contains the word Usage somewhere, note the capital 'U'): select * from \"workshopcur\".\"workshop_c_u_r\" where \"line_item_line_item_type\" like '%Usage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 limit 10; What billing periods are available? SELECT distinct bill_billing_period_start_date FROM \"workshopcur\".\"workshop_c_u_r\" limit 10;","title":"3.1 What data is available in the CUR file?"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#32-top-costs","text":"To efficiently optimize its useful to view the top costs in different categories, such as service, description or tags. Top10 Costs by AccountID: select \"line_item_usage_account_id\", round(sum(\"line_item_unblended_cost\"),2) as cost from \"workshopcur\".\"workshop_c_u_r\" where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"line_item_usage_account_id\" order by cost desc limit 10; Top10 Costs by Product: select \"line_item_product_code\", round(sum(\"line_item_unblended_cost\"),2) as cost from \"workshopcur\".\"workshop_c_u_r\" where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"line_item_product_code\" order by cost desc limit 10; Top Costs by Line Item Description select \"line_item_product_code\", \"line_item_line_item_description\", round(sum(\"line_item_unblended_cost\"),2) as cost from \"workshopcur\".\"workshop_c_u_r\" where month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"line_item_product_code\", \"line_item_line_item_description\" order by cost desc limit 10; Top EC2 Costs select \"line_item_product_code\", \"line_item_line_item_description\", round(sum(\"line_item_unblended_cost\"),2) as cost from \"workshopcur\".\"workshop_c_u_r\" where \"line_item_product_code\" like '%AmazonEC2%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"line_item_product_code\", \"line_item_line_item_description\" order by cost desc limit 10; Top EC2 OnDemand Costs select \"line_item_product_code\", \"line_item_line_item_description\", round(sum(\"line_item_unblended_cost\"),2) as cost from \"workshopcur\".\"workshop_c_u_r\" where \"line_item_product_code\" like '%AmazonEC2%' and \"line_item_usage_type\" like '%BoxUsage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"line_item_product_code\", \"line_item_line_item_description\" order by cost desc limit 10;","title":"3.2 Top Costs"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#33-tagging-and-cost-attribution","text":"Common in large organizations is the requirement to allocate costs back to specific business units. It is also critical for optimization to be able to allocate costs to workloads, to measure workload efficiency. NOTE : This will only work if you have tags enabled in your billing files, and they are the same as the examples here - resource_tags_user_cost_center Top 20 Costs by line item description and CostCenter Tag SELECT \"bill_payer_account_id\", \"product_product_name\", \"line_item_usage_type\", \"line_item_line_item_description\", resource_tags_user_cost_center, round(sum(line_item_unblended_cost),2) as cost FROM \"workshopcur\".\"workshop_c_u_r\" where length(\"resource_tags_user_cost_center\") >0 and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"resource_tags_user_cost_center\", \"bill_payer_account_id\", \"product_product_name\", \"line_item_usage_type\", \"line_item_line_item_description\" order by cost desc limit 20 Top 20 costs by line item description, without a CostCenter Tag SELECT \"bill_payer_account_id\", \"product_product_name\", \"line_item_usage_type\", \"line_item_line_item_description\", round(sum(line_item_unblended_cost),2) as cost FROM \"workshopcur\".\"workshop_c_u_r\" where length(\"resource_tags_user_cost_center\") = 0 and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"bill_payer_account_id\", \"product_product_name\", \"line_item_usage_type\", \"line_item_line_item_description\" order by cost desc limit 20","title":"3.3 Tagging and Cost Attribution"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#34-reserved-instance-on-demand-and-spot-usage","text":"To improve the use of pricing models across a business, these queries can assist to highlight the top opportunities for Reserved Instance (top On Demand cost), and also identify who is successful with pricing models (Top users of spot). NOTE : You will need specific usage in your account that matches the instance types below, for this to work correctly. Who used Reserved Instances Identify which accounts used the available RIs, and what they would have paid with public pricing. Ideal for chargeback within an organization. select \"bill_payer_account_id\", \"bill_billing_period_start_date\", \"line_item_usage_account_id\", \"reservation_reservation_a_r_n\", \"line_item_product_code\", \"line_item_usage_type\", sum(\"line_item_usage_amount\") as Usage, \"line_item_unblended_rate\", sum(\"line_item_unblended_cost\") as Cost, \"line_item_line_item_description\", \"pricing_public_on_demand_rate\", sum(\"pricing_public_on_demand_cost\") as PublicCost from \"workshopcur\".\"workshop_c_u_r\" where \"line_item_line_item_Type\" like '%DiscountedUsage%' group by \"bill_payer_account_id\", \"bill_billing_period_start_date\", \"line_item_usage_account_id\", \"reservation_reservation_a_r_n\", \"line_item_product_code\", \"line_item_usage_type\", \"line_item_unblended_rate\", \"line_item_line_item_description\", \"pricing_public_on_demand_rate\" T2 family instance usage Observe how much is being spent on each different family (usage type) and how much is covered by Reserved instances. select \"line_item_usage_type\", sum(\"line_item_usage_amount\") as usage, round(sum(\"line_item_unblended_cost\"),2) as cost from \"workshopcur\".\"workshop_c_u_r\" where \"line_item_usage_type\" like '%t2.%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"line_item_usage_type\" order by \"line_item_usage_type\" Costs By running type Divide the cost by usage (hrs), and see how much is being spent per hour on each of the usage types. Compare BoxUsage (On Demand), to HeavyUsage (Reserved instance), to SpotUsage (Spot). select \"line_item_usage_type\", round(sum(\"line_item_usage_amount\"),2) as usage, round(sum(\"line_item_unblended_cost\"),2) as cost, round(avg(\"line_item_unblended_cost\"/\"line_item_usage_amount\"),4) as hourly_rate from \"workshopcur\".\"workshop_c_u_r\" where \"line_item_product_code\" like '%AmazonEC2%' and \"line_item_usage_type\" like '%Usage%' and month(bill_billing_period_start_date) = 12 and year(bill_billing_period_start_date) = 2018 group by \"line_item_usage_type\" order by \"line_item_usage_type\" Show unused Reserved Instances This will show how much of your reserved instances are not being used, and sorts it via cost of unused portion (recurring fee). You can use this in two ways: See where you have spare RI's and modify instances to match, so they will use the RIs Convert your existing RI's if possible select bill_billing_period_start_date, product_region, line_item_usage_type, reservation_reservation_a_r_n, reservation_unused_quantity, reservation_unused_recurring_fee from \"workshopcur\".\"workshop_c_u_r\" where length(reservation_reservation_a_r_n) > 0 and reservation_unused_quantity > 0 order by bill_billing_period_start_date, reservation_unused_recurring_fee desc","title":"3.4 Reserved Instance, On Demand and Spot Usage"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#4-tear-down","text":"Amazon Athena only charges when it is being used, i.e. data is being scanned - so if it is not being actively queried, there are no charges. There may be some charges from AWS Glue if it is above the free tier limit. It is also best practice to regularly analyze your usage and cost, so there is no teardown for this lab.","title":"4. Tear down"},{"location":"Cost/Cost_Fundamentals/200_4_Cost_and_Usage_Analysis/Lab_Guide.html#5-rate-this-lab","text":"","title":"5. Rate this lab"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html","text":"Level 200: Cost Visualization https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to visualize your cost and usage. The skills you learn will help you analyze your cost and usage, in alignment with the AWS Well-Architected Framework. Goals Setup Amazon QuickSight Configure QuickSight to view your Cost and Usage reports Create a dashboard of cost and usage Prerequisites A master AWS Account Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup AWS Account Setup has been completed Cost_and_Usage_Governance has been completed Permissions required Log in as the Cost Optimization team, created in AWS Account Setup and modified in - Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required. Best Practice Checklist [ ] Load your CUR files into Amazon QuickSight [ ] Analyze Cost and Usage data visually License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#level-200-cost-visualization","text":"https://wellarchitectedlabs.com","title":"Level 200: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#introduction","text":"This hands-on lab will guide you through the steps to visualize your cost and usage. The skills you learn will help you analyze your cost and usage, in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#goals","text":"Setup Amazon QuickSight Configure QuickSight to view your Cost and Usage reports Create a dashboard of cost and usage","title":"Goals"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#prerequisites","text":"A master AWS Account Have your Cost and Usage Report (CUR) enabled as per 100_1_Account Setup AWS Account Setup has been completed Cost_and_Usage_Governance has been completed","title":"Prerequisites"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#permissions-required","text":"Log in as the Cost Optimization team, created in AWS Account Setup and modified in - Cost_and_Usage_Governance NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.","title":"Permissions required"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#best-practice-checklist","text":"[ ] Load your CUR files into Amazon QuickSight [ ] Analyze Cost and Usage data visually","title":"Best Practice Checklist"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html","text":"Level 200: Cost Visualization Authors Spencer Marley, Commercial Architect Nathan Besh, Cost Lead, Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Setup Amazon QuickSight Create a data set Create visualizations Share your Analysis and Dashboard Tear down Rate this Lab 1. Setup Amazon QuickSight The first step is to setup Amazon QuickSight, so that you can use the service in your account, and it has access to all the resources in your account. Log into the console as an IAM user with the required permissions, go to the Amazon QuickSight console: 1.1 Setup QuickSight for the first time If you havent used QuickSight before click on Sign up for QuickSight , otherwise login and go to step 5 : Select the Standard edition, and click Continue : Enter your QuickSight account name , Notification email address , select the QuickSight region (which matches your S3 bucket and Athena setup), select Amazon Athena and click Finish : Click Go to Amazon QuickSight : Click on your profile icon in the top right, and select Manage QuickSight : Click Account settings , then click Manage QuickSight permissions Click Choose S3 buckets : Select your billing bucket where the CUR files are delivered, and click Select buckets : Click Apply : Click on the QuickSight logo in the top left: 2. Create a data set We will create a data set so that QuickSight can access our Athena data set, and visualize our CUR data. Click Manage data in the top right: Click New data set : Click Athena : Enter a Data source name , and click Create data source : Select the workshpocur database (or the name you setup previously), and then the workshop_c_u_r table you created in Athena, and click Select : Select Directly query your data , and click Visualize : You have now configured QuickSight to access your Athena data set, and have access to your CUR data. 3. Create visualizations We will now start to visualize our costs and usage, and create a dashboard. 3.1 Cost by account and product The first visualization of the dashboard will do is a visualization of costs by linkedaccountID, and product. This will highlight top spend by account and product. Select line_item_unblendedcost from the Fields list, and it will show Sum of Line_item_unblended_cost: Select line_item_usage_account_id , which will add it to the graph: Expand the field wells by clicking on the two arrows in the top right. Drag line_item_product_code into the Group/Color field: Select the dropdown next to the title, and chose Format visual : Click on the down arrows under format visual , change: Y-Axis label : Linked Account ID X-Axis label : Cost Double click the title to set it: Title: Cost by Account and Product Modify the graph so that all elements are visible, with the lower corner and vertical bars : (you may need to increase the size of the graph) Sort the accounts by cost, click the dropdown under the X-Axis (Cost label), and select Sort by descending : The visualization is complete and the layout should look similar to: Click on the highest usage bar, in this example it is AWSGlue , and select Exclude AWSGlue : You will notice that AWSGlue (or the service you selected) is no longer showing, and on the left it has automatically created and applied a filter : 3.2 Elasticity The next visualization on the dashboard we will create is a visualization that shows usage for every hour, by purchase type (On Demand, Spot, Reserved Instance). In the CUR file there is no single field which shows the purchase type for EC2 Instances \u2013 so we\u2019ll make one with a calculated field. Click Add in the top left corner, then select Add calculated field : Copy and paste this formula into the Formula box: ifelse(split({line_item_usage_type},':',1) = 'SpotUsage','Spot',ifelse(right(split({product_usagetype},':',1), 8) = 'BoxUsage',{pricing_term},'other')) Description : - Ifelse( , , ) If statement evaluated and returns if true, otherwise - Right( , ) Returns the right most characters from a string - Split( , , ) Returns the substring when is split by , position is the index of the array starting at 1 Formula Logic : If the first part of \u2018lineitem/usagetype\u2019 is \u2018SpotUsage\u2019 then PurchaseOption = \u2018Spot\u2019, otherwise check part of \u2018product/usagetype\u2019 is \u2018BoxUsage\u2019, if it is then PurchaseOption = \u2018pricing/term\u2019, otherwise PurchaseOption = \u2018other\u2019. Enter a Calculated field name of PurchaseOption , and click Create : The new field will appear in the list of fields in the data source Click Add then select Add visual from the top left: Click the field line_item_usage_amount to add it to the visualization: Click line_item_usage_start_date to add it to the visualization x-axis: Change the aggregation of time to hourly , expand the field wells wih the arrows at the top right , click the down arrow* next to line_item_usage_start_date , click the arrow next to Aggregate: Day , and click Hour**: Click and drag PurchaseOption to the Color field: Now we will filter out other , click Filter on the left, and click Create One... : Select PurchaseOption : Click on the filter name PurchaseOption to edit it: Change the filter type to Custom filter list , enter other and click the + , change the Current list to Exclude : Click Apply : Select the empty line, and right click and select exclude : Update the title to Usage Elasticity , and you now have your elasticity graph, showing hourly usage by purchase option: NOTE : In the top left it states SHOWING TOP 200, and on the x-axis it has changed the range from Nov 10th to Nov 18th (most recent data points). Line charts show up to 2500 data points on the X axis when no color field is selected. When color is populated, line charts show up to 200 data points on the X axis and up to 25 data points for color. To work within this limitation, you can to add a filter to see each purchase option (OnDemand, Reserved, Spot) and remove the color field, we will do that next. We will now add instance type to the visualization, to be able to further drill down on usage. We will use another calculated field to get the instance type. Click on Add , and click Add calculated field : Copy and paste the following formula: split({line_item_usage_type},':',2) Name the field InstanceType , click Create : Drag InstanceType across to the Color field, the bottom of the box so it says Add drill-down layer: Select InstanceType and it will display the hourly usage by instance type (which is all usage regardless of purchase option): Now select PurchaseOption : Now we\u2019ll focus only on ondemand . Click on the blue line & select Focus only on OnDemand : You can see it automatically added a filter on the left , now click InstanceType : It will now only show hourly usage of OnDemand instances : You can enable/disable the filter to quickly cycle through the different options, by clicking on the checkbox next to the filter : This is also useful to work within the limitations of the number of data points on visuals. Remove the color field & enable/disable the filters to switch between data. Hourly usage of on demand instances is useful when making Reserved Instance purchase decisions and verifying usage to confirm if a purchase should be made. 3.3 Cost by line item description The previous visual showed instance usage, however instances vary in cost and your organization may have significant spend in other services and other components of EC2. So now we\u2019ll create a visualization that looks at daily costs by line_item_line_item_descrption, this will help to identify exactly where your costs are by within each service, across all services on a daily basis. Click Add and select Add visual : Click on line_item_unblendedcost to add it to the visualization: Click on line_item_usage_start_date to add it to the visualization, and you will have the Sum of Line_item_unblended_cost by line_item_usage_start_date : The data source for our workshop is 3 months of data, so we\u2019ll narrow that down with a filter to make it faster. Click on Filter and click Create one\u2026 Select bill_billing_period_start_date : Click on the filter name, bill_billing_period_start_date : Select a Relative dates filter, by Months and select This month , then click Apply : Click Visualize : Drag line_item_line_item_description to the Color field well , to add it to the visualization: You may have a visualization similar to below, which doesn\u2019t look very meaningful: Click on the Vertical stacked bar chart icon under Visual Types : You should get a graph similar to below which highlights cost more efficiently: Hover over the large usage and you can see the actual costs. To use this graph, observe the top costs, then exclude them and continue to drill down on the highest cost visible. 3.4 Dashboard Complete Your dashboard is now complete, you should have a similar dashboard to below: 4. Share your Analysis and Dashboard Now that your QuickSight Analysis is complete, it is time to share the Analysis or publish a Dashboard. An Analysis is a read and write copy of the Visuals and Data Set that you created. A dashboard is a read-only version, allowing the user to apply filters but not make any changes to the Visuals or Data Set. 4.1 Share an analysis To share an analysis, click on Share on the top right, then select Share analysis : Share with Authors and Admins in your QuickSight account by searching by email address. Once you have added all the users, click Share : The users will then receive an email similar to the one below. When they click on Click to View they\u2019ll be taken straight to the analysis, and they will have full access to modify the analysis as we have been doing in this workshop.: 4.2 Publish a dashboard To publish a dashboard click on Share in the upper right, and select Publish dashboard : Enter a name for the dashboard , and click Publish dashboard : Share with users in your QuickSight account by searching by email address. Once you have added all the users, select their permission levels and click Share . For the permissions, Viewer: can view, filter and sort the dashboard data, they can also use controls. Co-owner: can edit and share the dashboard. Click the x button in the top right to close the Manage dashboard sharing dialog: You will then have the dashboard on screen: All users will receive an email: 5. Tear down It is best practice to regularly analyze your usage and cost, so you should not tear down this lab unless you have an alternative visualization solution. 5.1 Cancel your QuickSight subscription Click on your profile icon in the top left, select Manage QuickSight : Click on Account settings : Cluck on Unsubscribe : Review the notifications, click Unsubscribe : 6. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#level-200-cost-visualization","text":"","title":"Level 200: Cost Visualization"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#authors","text":"Spencer Marley, Commercial Architect Nathan Besh, Cost Lead, Well-Architected","title":"Authors"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#table-of-contents","text":"Setup Amazon QuickSight Create a data set Create visualizations Share your Analysis and Dashboard Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#1-setup-amazon-quicksight","text":"The first step is to setup Amazon QuickSight, so that you can use the service in your account, and it has access to all the resources in your account. Log into the console as an IAM user with the required permissions, go to the Amazon QuickSight console:","title":"1. Setup Amazon QuickSight"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#11-setup-quicksight-for-the-first-time","text":"If you havent used QuickSight before click on Sign up for QuickSight , otherwise login and go to step 5 : Select the Standard edition, and click Continue : Enter your QuickSight account name , Notification email address , select the QuickSight region (which matches your S3 bucket and Athena setup), select Amazon Athena and click Finish : Click Go to Amazon QuickSight : Click on your profile icon in the top right, and select Manage QuickSight : Click Account settings , then click Manage QuickSight permissions Click Choose S3 buckets : Select your billing bucket where the CUR files are delivered, and click Select buckets : Click Apply : Click on the QuickSight logo in the top left:","title":"1.1 Setup QuickSight for the first time"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#2-create-a-data-set","text":"We will create a data set so that QuickSight can access our Athena data set, and visualize our CUR data. Click Manage data in the top right: Click New data set : Click Athena : Enter a Data source name , and click Create data source : Select the workshpocur database (or the name you setup previously), and then the workshop_c_u_r table you created in Athena, and click Select : Select Directly query your data , and click Visualize : You have now configured QuickSight to access your Athena data set, and have access to your CUR data.","title":"2. Create a data set"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#3-create-visualizations","text":"We will now start to visualize our costs and usage, and create a dashboard.","title":"3. Create visualizations"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#31-cost-by-account-and-product","text":"The first visualization of the dashboard will do is a visualization of costs by linkedaccountID, and product. This will highlight top spend by account and product. Select line_item_unblendedcost from the Fields list, and it will show Sum of Line_item_unblended_cost: Select line_item_usage_account_id , which will add it to the graph: Expand the field wells by clicking on the two arrows in the top right. Drag line_item_product_code into the Group/Color field: Select the dropdown next to the title, and chose Format visual : Click on the down arrows under format visual , change: Y-Axis label : Linked Account ID X-Axis label : Cost Double click the title to set it: Title: Cost by Account and Product Modify the graph so that all elements are visible, with the lower corner and vertical bars : (you may need to increase the size of the graph) Sort the accounts by cost, click the dropdown under the X-Axis (Cost label), and select Sort by descending : The visualization is complete and the layout should look similar to: Click on the highest usage bar, in this example it is AWSGlue , and select Exclude AWSGlue : You will notice that AWSGlue (or the service you selected) is no longer showing, and on the left it has automatically created and applied a filter :","title":"3.1 Cost by account and product"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#32-elasticity","text":"The next visualization on the dashboard we will create is a visualization that shows usage for every hour, by purchase type (On Demand, Spot, Reserved Instance). In the CUR file there is no single field which shows the purchase type for EC2 Instances \u2013 so we\u2019ll make one with a calculated field. Click Add in the top left corner, then select Add calculated field : Copy and paste this formula into the Formula box: ifelse(split({line_item_usage_type},':',1) = 'SpotUsage','Spot',ifelse(right(split({product_usagetype},':',1), 8) = 'BoxUsage',{pricing_term},'other')) Description : - Ifelse( , , ) If statement evaluated and returns if true, otherwise - Right( , ) Returns the right most characters from a string - Split( , , ) Returns the substring when is split by , position is the index of the array starting at 1 Formula Logic : If the first part of \u2018lineitem/usagetype\u2019 is \u2018SpotUsage\u2019 then PurchaseOption = \u2018Spot\u2019, otherwise check part of \u2018product/usagetype\u2019 is \u2018BoxUsage\u2019, if it is then PurchaseOption = \u2018pricing/term\u2019, otherwise PurchaseOption = \u2018other\u2019. Enter a Calculated field name of PurchaseOption , and click Create : The new field will appear in the list of fields in the data source Click Add then select Add visual from the top left: Click the field line_item_usage_amount to add it to the visualization: Click line_item_usage_start_date to add it to the visualization x-axis: Change the aggregation of time to hourly , expand the field wells wih the arrows at the top right , click the down arrow* next to line_item_usage_start_date , click the arrow next to Aggregate: Day , and click Hour**: Click and drag PurchaseOption to the Color field: Now we will filter out other , click Filter on the left, and click Create One... : Select PurchaseOption : Click on the filter name PurchaseOption to edit it: Change the filter type to Custom filter list , enter other and click the + , change the Current list to Exclude : Click Apply : Select the empty line, and right click and select exclude : Update the title to Usage Elasticity , and you now have your elasticity graph, showing hourly usage by purchase option: NOTE : In the top left it states SHOWING TOP 200, and on the x-axis it has changed the range from Nov 10th to Nov 18th (most recent data points). Line charts show up to 2500 data points on the X axis when no color field is selected. When color is populated, line charts show up to 200 data points on the X axis and up to 25 data points for color. To work within this limitation, you can to add a filter to see each purchase option (OnDemand, Reserved, Spot) and remove the color field, we will do that next. We will now add instance type to the visualization, to be able to further drill down on usage. We will use another calculated field to get the instance type. Click on Add , and click Add calculated field : Copy and paste the following formula: split({line_item_usage_type},':',2) Name the field InstanceType , click Create : Drag InstanceType across to the Color field, the bottom of the box so it says Add drill-down layer: Select InstanceType and it will display the hourly usage by instance type (which is all usage regardless of purchase option): Now select PurchaseOption : Now we\u2019ll focus only on ondemand . Click on the blue line & select Focus only on OnDemand : You can see it automatically added a filter on the left , now click InstanceType : It will now only show hourly usage of OnDemand instances : You can enable/disable the filter to quickly cycle through the different options, by clicking on the checkbox next to the filter : This is also useful to work within the limitations of the number of data points on visuals. Remove the color field & enable/disable the filters to switch between data. Hourly usage of on demand instances is useful when making Reserved Instance purchase decisions and verifying usage to confirm if a purchase should be made.","title":"3.2 Elasticity"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#33-cost-by-line-item-description","text":"The previous visual showed instance usage, however instances vary in cost and your organization may have significant spend in other services and other components of EC2. So now we\u2019ll create a visualization that looks at daily costs by line_item_line_item_descrption, this will help to identify exactly where your costs are by within each service, across all services on a daily basis. Click Add and select Add visual : Click on line_item_unblendedcost to add it to the visualization: Click on line_item_usage_start_date to add it to the visualization, and you will have the Sum of Line_item_unblended_cost by line_item_usage_start_date : The data source for our workshop is 3 months of data, so we\u2019ll narrow that down with a filter to make it faster. Click on Filter and click Create one\u2026 Select bill_billing_period_start_date : Click on the filter name, bill_billing_period_start_date : Select a Relative dates filter, by Months and select This month , then click Apply : Click Visualize : Drag line_item_line_item_description to the Color field well , to add it to the visualization: You may have a visualization similar to below, which doesn\u2019t look very meaningful: Click on the Vertical stacked bar chart icon under Visual Types : You should get a graph similar to below which highlights cost more efficiently: Hover over the large usage and you can see the actual costs. To use this graph, observe the top costs, then exclude them and continue to drill down on the highest cost visible.","title":"3.3 Cost by line item description"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#34-dashboard-complete","text":"Your dashboard is now complete, you should have a similar dashboard to below:","title":"3.4 Dashboard Complete"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#4-share-your-analysis-and-dashboard","text":"Now that your QuickSight Analysis is complete, it is time to share the Analysis or publish a Dashboard. An Analysis is a read and write copy of the Visuals and Data Set that you created. A dashboard is a read-only version, allowing the user to apply filters but not make any changes to the Visuals or Data Set.","title":"4. Share your Analysis and Dashboard"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#41-share-an-analysis","text":"To share an analysis, click on Share on the top right, then select Share analysis : Share with Authors and Admins in your QuickSight account by searching by email address. Once you have added all the users, click Share : The users will then receive an email similar to the one below. When they click on Click to View they\u2019ll be taken straight to the analysis, and they will have full access to modify the analysis as we have been doing in this workshop.:","title":"4.1 Share an analysis"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#42-publish-a-dashboard","text":"To publish a dashboard click on Share in the upper right, and select Publish dashboard : Enter a name for the dashboard , and click Publish dashboard : Share with users in your QuickSight account by searching by email address. Once you have added all the users, select their permission levels and click Share . For the permissions, Viewer: can view, filter and sort the dashboard data, they can also use controls. Co-owner: can edit and share the dashboard. Click the x button in the top right to close the Manage dashboard sharing dialog: You will then have the dashboard on screen: All users will receive an email:","title":"4.2 Publish a dashboard"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#5-tear-down","text":"It is best practice to regularly analyze your usage and cost, so you should not tear down this lab unless you have an alternative visualization solution.","title":"5. Tear down"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#51-cancel-your-quicksight-subscription","text":"Click on your profile icon in the top left, select Manage QuickSight : Click on Account settings : Cluck on Unsubscribe : Review the notifications, click Unsubscribe :","title":"5.1 Cancel your QuickSight subscription"},{"location":"Cost/Cost_Fundamentals/200_5_Cost_Visualization/Lab_Guide.html#6-rate-this-lab","text":"","title":"6. Rate this lab"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html","text":"Level 300: Automated CUR Updates and Ingestion https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to enable automated updates of your CUR files into Athena. The skills you learn will help you perform cost and usage analysis in alignment with the AWS Well-Architected Framework. Goals Automatically update the CUR table in Athena/Glue when a new report arrives Automatically update the CUR table for multiple Cost and Usage Reports in the same bucket Prerequisites An AWS Account CUR enabled and delivered into S3, with Athena integration 6-12 months AWS experience, able to navigate the console, and have an understanding of the underlying services and features Start the Lab! Best Practice Checklist [ ] Run the CloudFormation template to update a single CUR in AWS Glue/Athena [ ] Modify and run a CloudFormation template to update multiple CURs in AWS Glue/Athena License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#level-300-automated-cur-updates-and-ingestion","text":"https://wellarchitectedlabs.com","title":"Level 300: Automated CUR Updates and Ingestion"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#introduction","text":"This hands-on lab will guide you through the steps to enable automated updates of your CUR files into Athena. The skills you learn will help you perform cost and usage analysis in alignment with the AWS Well-Architected Framework.","title":"Introduction"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#goals","text":"Automatically update the CUR table in Athena/Glue when a new report arrives Automatically update the CUR table for multiple Cost and Usage Reports in the same bucket","title":"Goals"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#prerequisites","text":"An AWS Account CUR enabled and delivered into S3, with Athena integration 6-12 months AWS experience, able to navigate the console, and have an understanding of the underlying services and features","title":"Prerequisites"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#best-practice-checklist","text":"[ ] Run the CloudFormation template to update a single CUR in AWS Glue/Athena [ ] Modify and run a CloudFormation template to update multiple CURs in AWS Glue/Athena","title":"Best Practice Checklist"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html","text":"Level 300: Automated CUR Updates and Ingestion Authors Nathan Besh, Cost Lead, Well-Architected Derrick Gold, Software Development Engineer, AWS Insights Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Create the CloudFormation stack Multiple CURs Tear down Rate this Lab 1. Create the CloudFormation Stack This step is used when there is a single CUR being delivered, and have it automatically update Athena/Glue when there are new versions and new months data. We will follow the steps here: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/setting-up-athena.html#use-athena-cf to implement the CloudFormation template, which will automatically update existing CURs, and include new CURs when they are delivered. NOTE: IAM roles will be created, these are used to: - Add event notification to existing S3 buckets - Create s3 buckets and upload objects - Create and run a Glue crawler - Create and update a Glue database and tables Please review the CloudFormation template with your security team. We will build the following solution: Log into the console as an IAM user with the required permissions. Go to the S3 dashboard, go to the bucket and folders which contain your CUR file. Open the CloudFormation(CF) file and save it locally: Here is a sample of the CF file: Go to the CloudFormation dashboard and create a stack: Load the template and click Next : Specify the details for the stack and click Next : Review the configuration, click I acknowledge that AWS CloudFormation might create IAM resources , and click Create stack : You will see the stack will start in CREATE_IN_PROGRESS : Once complete, the stack will show CREATE_COMPLETE : Click on Resources to view the resources that it will create: Go to the AWS Glue dashboard: Click on Databases and click the database starting with athenacurcfn : View the table within that database and its properties: You will see that the table is populated, the recordCount should be greater than 0. You can now go to Athena and load the partitions and view the cost and usage reports. 2. Multiple CURs This step is used when there are multiple CURs being delivered into the same bucket - for example a CUR with hourly granularity and one with daily granularity. This will automatically update Athena/Glue when there are new versions and new months data for both reports. The easiest way to work with multiple CURs is to deliver each CUR to a different S3 bucket, and follow the previous process. If you must deliver to a single bucket, configure your CURs with different prefixes or folders and follow this process. Log into the console as an IAM user with the required permissions, verify you have multiple CURs with different prefixes being delivered into the same bucket. We will have the following configuration: Format: <bucket name>/<prefix>/<report_name>/ Configuration: <bucket name>/DailyCUR/daily/ <bucket name>/HourlyCUR/hourly/ Open the S3 console, and navigate to one of the directories where CURs are stored. Open and save the crawler-cfn.yml file: Open the file in your favourite text editor Modify the following lines to remove all references to the prefix or report name. Replace the first line with the second in each case: Under AWSCurDatabase: Name: 'athenacurcfn_daily' Name: 'athenacurcfn' Under AWSCURCrawlerComponentFunction: Resource: arn:aws:s3:::<bucket name>/DailyCUR/daily/daily* Resource: arn:aws:s3:::<bucket name>* Under AWSCURCrawler: Name: AWSCURCrawler-daily Name: AWSCURCrawler and Path: 's3://<bucket name>/DailyCUR/daily/daily' Path: 's3://<bucket name>' and under Exclusions after .zip add: 'aws-programmatic-access-test-object' Under AWSPutS3CURNotification: ReportKey: 'DailyCUR/daily/daily' ReportKey: '' Under AWSCURReportStatusTable: DatabaseName: athenacurcfn_daily DatabaseName: athenacurcfn and Location: 's3://<bucket name>/DailyCUR/daily/cost_and_usage_data_status/' Location: 's3://<bucket name>/cost_and_usage_data_status/' A modified sample is provided here: Code/crawler-cfn.yml Look for the comments: ### New line Save the template file. Go to the CloudFormation dashboard and execute the template you just created Go to the Glue dashboard and verify that there is a single database, containing multiple tables: 3. Tear down Delete the Glue database, select the database name, click Action and click Delete database : Delete the CloudFormation stack, select the stack, click Actions and click Delete stack : 4. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#level-300-automated-cur-updates-and-ingestion","text":"","title":"Level 300: Automated CUR Updates and Ingestion"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead, Well-Architected Derrick Gold, Software Development Engineer, AWS Insights","title":"Authors"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#table-of-contents","text":"Create the CloudFormation stack Multiple CURs Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#1-create-the-cloudformation-stack","text":"This step is used when there is a single CUR being delivered, and have it automatically update Athena/Glue when there are new versions and new months data. We will follow the steps here: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/setting-up-athena.html#use-athena-cf to implement the CloudFormation template, which will automatically update existing CURs, and include new CURs when they are delivered. NOTE: IAM roles will be created, these are used to: - Add event notification to existing S3 buckets - Create s3 buckets and upload objects - Create and run a Glue crawler - Create and update a Glue database and tables Please review the CloudFormation template with your security team. We will build the following solution: Log into the console as an IAM user with the required permissions. Go to the S3 dashboard, go to the bucket and folders which contain your CUR file. Open the CloudFormation(CF) file and save it locally: Here is a sample of the CF file: Go to the CloudFormation dashboard and create a stack: Load the template and click Next : Specify the details for the stack and click Next : Review the configuration, click I acknowledge that AWS CloudFormation might create IAM resources , and click Create stack : You will see the stack will start in CREATE_IN_PROGRESS : Once complete, the stack will show CREATE_COMPLETE : Click on Resources to view the resources that it will create: Go to the AWS Glue dashboard: Click on Databases and click the database starting with athenacurcfn : View the table within that database and its properties: You will see that the table is populated, the recordCount should be greater than 0. You can now go to Athena and load the partitions and view the cost and usage reports.","title":"1. Create the CloudFormation Stack"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#2-multiple-curs","text":"This step is used when there are multiple CURs being delivered into the same bucket - for example a CUR with hourly granularity and one with daily granularity. This will automatically update Athena/Glue when there are new versions and new months data for both reports. The easiest way to work with multiple CURs is to deliver each CUR to a different S3 bucket, and follow the previous process. If you must deliver to a single bucket, configure your CURs with different prefixes or folders and follow this process. Log into the console as an IAM user with the required permissions, verify you have multiple CURs with different prefixes being delivered into the same bucket. We will have the following configuration: Format: <bucket name>/<prefix>/<report_name>/ Configuration: <bucket name>/DailyCUR/daily/ <bucket name>/HourlyCUR/hourly/ Open the S3 console, and navigate to one of the directories where CURs are stored. Open and save the crawler-cfn.yml file: Open the file in your favourite text editor Modify the following lines to remove all references to the prefix or report name. Replace the first line with the second in each case: Under AWSCurDatabase: Name: 'athenacurcfn_daily' Name: 'athenacurcfn' Under AWSCURCrawlerComponentFunction: Resource: arn:aws:s3:::<bucket name>/DailyCUR/daily/daily* Resource: arn:aws:s3:::<bucket name>* Under AWSCURCrawler: Name: AWSCURCrawler-daily Name: AWSCURCrawler and Path: 's3://<bucket name>/DailyCUR/daily/daily' Path: 's3://<bucket name>' and under Exclusions after .zip add: 'aws-programmatic-access-test-object' Under AWSPutS3CURNotification: ReportKey: 'DailyCUR/daily/daily' ReportKey: '' Under AWSCURReportStatusTable: DatabaseName: athenacurcfn_daily DatabaseName: athenacurcfn and Location: 's3://<bucket name>/DailyCUR/daily/cost_and_usage_data_status/' Location: 's3://<bucket name>/cost_and_usage_data_status/' A modified sample is provided here: Code/crawler-cfn.yml Look for the comments: ### New line Save the template file. Go to the CloudFormation dashboard and execute the template you just created Go to the Glue dashboard and verify that there is a single database, containing multiple tables:","title":"2. Multiple CURs"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#3-tear-down","text":"Delete the Glue database, select the database name, click Action and click Delete database : Delete the CloudFormation stack, select the stack, click Actions and click Delete stack :","title":"3. Tear down"},{"location":"Cost/Cost_and_Usage_Analysis/300_Automated_CUR_Updates_and_Ingestion/Lab_Guide.html#4-rate-this-lab","text":"","title":"4. Rate this lab"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html","text":"Level 300: Multi Account CUR Access https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the different methods to share and analyze cost and usage data across accounts. This will ensure that all users and business units throughout your organization can access their cost and usage information, critical to ensuring they can track and further optimize their cost. Goals Allow users in another account (either a member/linked or another master/payer) account, access to the (full) payer account cost and usage data Prerequisites Multiple AWS Accounts (At least two) Billing reports auto update configured as per 300_Automated_CUR_Updates_and_Ingestion Permissions required IAM - create role, users, groups, and policies Modify S3 Bucket Policies Modify an existing Lambda function Upload a file to your S3 billing bucket Start the Lab! Best Practice Checklist [ ] Create a role to allow cross account least privilege access to resouces [ ] Serverless automated access configuration License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#level-300-multi-account-cur-access","text":"https://wellarchitectedlabs.com","title":"Level 300: Multi Account CUR Access"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#introduction","text":"This hands-on lab will guide you through the different methods to share and analyze cost and usage data across accounts. This will ensure that all users and business units throughout your organization can access their cost and usage information, critical to ensuring they can track and further optimize their cost.","title":"Introduction"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#goals","text":"Allow users in another account (either a member/linked or another master/payer) account, access to the (full) payer account cost and usage data","title":"Goals"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#prerequisites","text":"Multiple AWS Accounts (At least two) Billing reports auto update configured as per 300_Automated_CUR_Updates_and_Ingestion","title":"Prerequisites"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#permissions-required","text":"IAM - create role, users, groups, and policies Modify S3 Bucket Policies Modify an existing Lambda function Upload a file to your S3 billing bucket","title":"Permissions required"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#best-practice-checklist","text":"[ ] Create a role to allow cross account least privilege access to resouces [ ] Serverless automated access configuration","title":"Best Practice Checklist"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html","text":"Level 300: Multi Account CUR Access https://wellarchitectedlabs.com Authors Nathan Besh, Cost Lead, Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Access Master/Payer via a Role Access Master/Payer via a User Use Athena to access a CUR in Master/Payer Tear down Rate this Lab 1. Access Master/Payer via a Role To provide access to the Cost and Usage data in the master/payer account to another account, you can create a role for the users in that account to assume. The users then have access to the data, in the same way a user in the master/payer account would. This is an ideal solution when you want to provide access to the data by allowing them to use only specific services in your account (Athena/Glue), and requires minimum coding and complexity. 1 - Login to the AWS console as an IAM user with the required permissions, go to the IAM dashboard 2 - Create a new policy Athena_List_Read with the required permissions. A sample policy that can be used as a starting point is here: ./Code/IAM_Athena It provides: Athena: List, Read, and Start Query (write) access to all Athena resources; Glue: Read access to all resources; S3: Read access to the bucket containing the Cost and Usage reports; S3: List, Read, Write access to the bucket containing Athena query results. NOTE: You must modify this policy in line with security best practices (least privilege) before implementation. Next we will create a role Sub_Acct_Athena and attach the newly created policy. 3 - From the IAM dashboard, create a role for Another AWS Account , enter in the Sub-Accounts Account ID : 4 - Attach the policy Athena_List_Read 5 - Add any required tags 6 - Enter a Role name and Role description , review and create the role: 7 - The completed role should be similar to this: 8 - Logout from your master/payer account, then login to the other account (which will access your master/payer CUR file). 9 - From the IAM console check the users have permissions to assume a role, the IAM policy they require is: NOTE: Replace (Account ID) and (Role name) inside the brackets { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": \"sts:AssumeRole\", \"Resource\": \"arn:aws:iam::(Account ID):role/(Role name)\" } ] } NOTE: Replace (Account ID) and (Role name) inside the brackets 10 - The users in the member/linked account can then assume the role. They will then be able to access Athena in the payer/linked account. You have successfully allowed users in another account to get access to your CUR. They assume the role in the master/payer account, and can use Athena to query the CUR just as a user in the master/payer account would. 2. Access Master/Payer via a User You can also provide access to the Cost and Usage information in the payer account by giving people a login to the payer account and assigning the required permissions. This is against security best practices, as roles and centralized federation should be utilized. If there is a specific reason you need to implement users for this, follow the following procedure: 1 - Login to the AWS console as an IAM user with the required permissions, go to the IAM dashboard 2 - Create the policy Athena_List_Read as defined above 3 - Create a user and group, and assign them the IAM policy 4 - Users can now login to the primary account and have access to Athena 3. Use Athena to access a CUR in Master/Payer You can use Athena in your member/linked account or an account outside your organization to access the CUR in your master/payer account. This requires cross account S3 access, and a permissions change on the delivered CUR files. This is an ideal solution when you want to make the data available to another account via S3, that account can then use any services within their own account to access the data. This solution provides additional flexibility to the other account, and can be extended with additional features later. NOTE: We assume you have completed the lab 300_Automated_CUR_Updates_and_Ingestion , which creates a Lambda function and puts an S3 Event on your billing bucket, so we will extend this existing solution. First we will add the s3 bucket permissions allowing the member/linked account access. 1 - Login to the Master/Payer account AWS console as an IAM user with the required permissions, navigate to the S3 Dashboard : 2 - Select your bucket containing the CUR reports 3 - Click on Permissions 4 - Click on Bucket Policy 5 - Edit the following lines and add them to the bucket policy. Change [Sub-Account ID] and [S3 Bucket Name] to your member/linked account and S3 bucket: { \"Sid\": \"Stmt1546900919345\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::[Sub-Account ID]:root\" }, \"Action\": \"s3:ListBucket\", \"Resource\": \"arn:aws:s3:::[S3 Bucket Name]\" }, { \"Sid\": \"Stmt1546901049588\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::[Sub-Account ID]:root\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::[S3 Bucket Name]/*\" } 6 - Save the new bucket policy. 7 - A sample complete policy is here: ./Code/S3_Bucket_Policy Finally, we will update the Cloudformation stack with the code, this will ensure that a new permissions ACL is written on all delivered CUR files - so that newly delivered CURs will be accessible to the other account. 8 - Open the following modified CloudFormation template in a new window: ./Code/crawler-cfn.md 9 - Open the CloudFormation dashboard, select the CUR-Update stack and view the current template: 10 - Copy the current template into a text editor, save this template for rollback. 11 - Make the changes required as per below, and save the new yml file. - Effect: Allow Action: - 's3:PutObjectVersionAcl' - 's3:PutObjectAcl' Resource: 'arn:aws:s3:::(CUR Billing Bucket)/*' const util = require('util'); // Read options from the event. console.log(\"Reading options from event:\\n\", util.inspect(event, {depth: 5})); var srcBucket = event.Records[0].s3.bucket.name; // Object key may have spaces or unicode non-ASCII characters. var srcKey = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, \" \")); // New Object ACL to be written var params = { Bucket: srcBucket, Key: srcKey, AccessControlPolicy: { 'Owner': { 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Grants': [ { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Permission': 'FULL_CONTROL' }, { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name2)', 'ID': '(Canonical ID2)' }, 'Permission': 'READ' }, ] } }; // get reference to S3 client var s3 = new AWS.S3(); // Put the ACL on the object s3.putObjectAcl(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); 12 - In the CloudFormation console under Stack actions , Create change set for current stack , Replace current template , and upload the edited file. 13 - Execute the change set: 14 - You can verify the changes to the stack by viewing the IAM console - CUR-Update-AWSCURCrawlerLambdaExecutor role, and the Lambda console - function CUR-Update-AWSCURInitializer , ensure they contain the changes you made. 15 - We will now test the function. Upload a file to the CUR billing bucket, preferably next to the CUR files for the current month. 16 - Click on the file and you should see that the bucket owner is the owner, and it has multiple Grantees for read: 17 - Click on the permissions and you can confirm the bucket owner has full access, and the required Canonical IDs also have read access, then delete this test file : At this point the other account will have the required access to any NEW CUR files delivered. You will need to modify any previous months billing files and ensure they have the correct permissions. NOTE: If you do not modify previous CUR file permissions, Athena queries from the member/linked account will not work. To change previous CUR files you can use S3 batch operations: (NOTE: the inventory will take time to generate to be able to complete this step) Go to the S3 Console Create a destination bucket for the inventory file Go to the CUR folder, Select Management , select Inventory , then create an inventory to produce a CSV file of all files. Wait until the inventory populates Go to the S3 console, select Batch operations , create job Select the inventory file Choose to replace the ACL (Get this from the Lambda function) Check you have the policy and trust to run the job Start the job Confirm the old CURs are updated The other account will now need to create the tables in Athena, and also update these tables when new versions and new months are delivered. We will do this with a recurring Glue crawler. 18 - Login to the other account as an IAM user with the required permissions, and go into the Glue console . 19 - Add a Crawler with the following details: - Include path : the S3 bucket in the account with the delivered CURs - Exclude patterns (1 per line): **.json, **.yml, **.sql, **.csv, **.gz, **.zip 20 - Create a daily schedule to update the tables each morning before you come into work NOTE: CUR files are updated at least every day, a crawler with a daily schedule is a simple code-free solution for this. 21 - Run the crawler, and check that it has created the database, the tables, and that the tables contain data. 26 - Open up Athena & execute a query to verify access: You have now given the sub account access to the Master/Payer CUR file. This will be automatically updated on any new versions delivered, or any new months delivered. 4. Tear down Execute either of these steps depending on the implementation you chose above. 4.1 Access Master/Payer via a Role 1 - Go to the IAM Dashboard , delete the role Sub_Acct_Athena 2 - Delete the policy Athena_List_Read 4.2 Access Master/Payer via a User 1 - Go to the IAM Dashboard , delete the group, delete the user 2 - Delete the policy Athena_List_Read 4.3 Use Athena to access a CUR in Master/Payer 1 - Go to the CloudFormation Dashboard 2 - Update the stack and implement the original yml file. If you didn't save this, it will be in the S3 bucket that contains the CUR files. 3 - If you change the permissions ACL on the old CUR files, follow the same process - but remove the member/linked account from the ACL. 4 - Login to the member/linked account, go to the Glue Dashboard 5 - Delete the database that was created 5. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#level-300-multi-account-cur-access","text":"https://wellarchitectedlabs.com","title":"Level 300: Multi Account CUR Access"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead, Well-Architected","title":"Authors"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#table-of-contents","text":"Access Master/Payer via a Role Access Master/Payer via a User Use Athena to access a CUR in Master/Payer Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#1-access-masterpayer-via-a-role","text":"To provide access to the Cost and Usage data in the master/payer account to another account, you can create a role for the users in that account to assume. The users then have access to the data, in the same way a user in the master/payer account would. This is an ideal solution when you want to provide access to the data by allowing them to use only specific services in your account (Athena/Glue), and requires minimum coding and complexity. 1 - Login to the AWS console as an IAM user with the required permissions, go to the IAM dashboard 2 - Create a new policy Athena_List_Read with the required permissions. A sample policy that can be used as a starting point is here: ./Code/IAM_Athena It provides: Athena: List, Read, and Start Query (write) access to all Athena resources; Glue: Read access to all resources; S3: Read access to the bucket containing the Cost and Usage reports; S3: List, Read, Write access to the bucket containing Athena query results. NOTE: You must modify this policy in line with security best practices (least privilege) before implementation. Next we will create a role Sub_Acct_Athena and attach the newly created policy. 3 - From the IAM dashboard, create a role for Another AWS Account , enter in the Sub-Accounts Account ID : 4 - Attach the policy Athena_List_Read 5 - Add any required tags 6 - Enter a Role name and Role description , review and create the role: 7 - The completed role should be similar to this: 8 - Logout from your master/payer account, then login to the other account (which will access your master/payer CUR file). 9 - From the IAM console check the users have permissions to assume a role, the IAM policy they require is: NOTE: Replace (Account ID) and (Role name) inside the brackets { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": \"sts:AssumeRole\", \"Resource\": \"arn:aws:iam::(Account ID):role/(Role name)\" } ] } NOTE: Replace (Account ID) and (Role name) inside the brackets 10 - The users in the member/linked account can then assume the role. They will then be able to access Athena in the payer/linked account. You have successfully allowed users in another account to get access to your CUR. They assume the role in the master/payer account, and can use Athena to query the CUR just as a user in the master/payer account would.","title":"1. Access Master/Payer via a Role  "},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#2-access-masterpayer-via-a-user","text":"You can also provide access to the Cost and Usage information in the payer account by giving people a login to the payer account and assigning the required permissions. This is against security best practices, as roles and centralized federation should be utilized. If there is a specific reason you need to implement users for this, follow the following procedure: 1 - Login to the AWS console as an IAM user with the required permissions, go to the IAM dashboard 2 - Create the policy Athena_List_Read as defined above 3 - Create a user and group, and assign them the IAM policy 4 - Users can now login to the primary account and have access to Athena","title":"2. Access Master/Payer via a User "},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#3-use-athena-to-access-a-cur-in-masterpayer","text":"You can use Athena in your member/linked account or an account outside your organization to access the CUR in your master/payer account. This requires cross account S3 access, and a permissions change on the delivered CUR files. This is an ideal solution when you want to make the data available to another account via S3, that account can then use any services within their own account to access the data. This solution provides additional flexibility to the other account, and can be extended with additional features later. NOTE: We assume you have completed the lab 300_Automated_CUR_Updates_and_Ingestion , which creates a Lambda function and puts an S3 Event on your billing bucket, so we will extend this existing solution. First we will add the s3 bucket permissions allowing the member/linked account access. 1 - Login to the Master/Payer account AWS console as an IAM user with the required permissions, navigate to the S3 Dashboard : 2 - Select your bucket containing the CUR reports 3 - Click on Permissions 4 - Click on Bucket Policy 5 - Edit the following lines and add them to the bucket policy. Change [Sub-Account ID] and [S3 Bucket Name] to your member/linked account and S3 bucket: { \"Sid\": \"Stmt1546900919345\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::[Sub-Account ID]:root\" }, \"Action\": \"s3:ListBucket\", \"Resource\": \"arn:aws:s3:::[S3 Bucket Name]\" }, { \"Sid\": \"Stmt1546901049588\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::[Sub-Account ID]:root\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::[S3 Bucket Name]/*\" } 6 - Save the new bucket policy. 7 - A sample complete policy is here: ./Code/S3_Bucket_Policy Finally, we will update the Cloudformation stack with the code, this will ensure that a new permissions ACL is written on all delivered CUR files - so that newly delivered CURs will be accessible to the other account. 8 - Open the following modified CloudFormation template in a new window: ./Code/crawler-cfn.md 9 - Open the CloudFormation dashboard, select the CUR-Update stack and view the current template: 10 - Copy the current template into a text editor, save this template for rollback. 11 - Make the changes required as per below, and save the new yml file. - Effect: Allow Action: - 's3:PutObjectVersionAcl' - 's3:PutObjectAcl' Resource: 'arn:aws:s3:::(CUR Billing Bucket)/*' const util = require('util'); // Read options from the event. console.log(\"Reading options from event:\\n\", util.inspect(event, {depth: 5})); var srcBucket = event.Records[0].s3.bucket.name; // Object key may have spaces or unicode non-ASCII characters. var srcKey = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, \" \")); // New Object ACL to be written var params = { Bucket: srcBucket, Key: srcKey, AccessControlPolicy: { 'Owner': { 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Grants': [ { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Permission': 'FULL_CONTROL' }, { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name2)', 'ID': '(Canonical ID2)' }, 'Permission': 'READ' }, ] } }; // get reference to S3 client var s3 = new AWS.S3(); // Put the ACL on the object s3.putObjectAcl(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); 12 - In the CloudFormation console under Stack actions , Create change set for current stack , Replace current template , and upload the edited file. 13 - Execute the change set: 14 - You can verify the changes to the stack by viewing the IAM console - CUR-Update-AWSCURCrawlerLambdaExecutor role, and the Lambda console - function CUR-Update-AWSCURInitializer , ensure they contain the changes you made. 15 - We will now test the function. Upload a file to the CUR billing bucket, preferably next to the CUR files for the current month. 16 - Click on the file and you should see that the bucket owner is the owner, and it has multiple Grantees for read: 17 - Click on the permissions and you can confirm the bucket owner has full access, and the required Canonical IDs also have read access, then delete this test file : At this point the other account will have the required access to any NEW CUR files delivered. You will need to modify any previous months billing files and ensure they have the correct permissions. NOTE: If you do not modify previous CUR file permissions, Athena queries from the member/linked account will not work. To change previous CUR files you can use S3 batch operations: (NOTE: the inventory will take time to generate to be able to complete this step) Go to the S3 Console Create a destination bucket for the inventory file Go to the CUR folder, Select Management , select Inventory , then create an inventory to produce a CSV file of all files. Wait until the inventory populates Go to the S3 console, select Batch operations , create job Select the inventory file Choose to replace the ACL (Get this from the Lambda function) Check you have the policy and trust to run the job Start the job Confirm the old CURs are updated The other account will now need to create the tables in Athena, and also update these tables when new versions and new months are delivered. We will do this with a recurring Glue crawler. 18 - Login to the other account as an IAM user with the required permissions, and go into the Glue console . 19 - Add a Crawler with the following details: - Include path : the S3 bucket in the account with the delivered CURs - Exclude patterns (1 per line): **.json, **.yml, **.sql, **.csv, **.gz, **.zip 20 - Create a daily schedule to update the tables each morning before you come into work NOTE: CUR files are updated at least every day, a crawler with a daily schedule is a simple code-free solution for this. 21 - Run the crawler, and check that it has created the database, the tables, and that the tables contain data. 26 - Open up Athena & execute a query to verify access: You have now given the sub account access to the Master/Payer CUR file. This will be automatically updated on any new versions delivered, or any new months delivered.","title":"3. Use Athena to access a CUR in Master/Payer "},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#4-tear-down","text":"Execute either of these steps depending on the implementation you chose above.","title":"4. Tear down "},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#41-access-masterpayer-via-a-role","text":"1 - Go to the IAM Dashboard , delete the role Sub_Acct_Athena 2 - Delete the policy Athena_List_Read","title":"4.1 Access Master/Payer via a Role"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#42-access-masterpayer-via-a-user","text":"1 - Go to the IAM Dashboard , delete the group, delete the user 2 - Delete the policy Athena_List_Read","title":"4.2 Access Master/Payer via a User"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#43-use-athena-to-access-a-cur-in-masterpayer","text":"1 - Go to the CloudFormation Dashboard 2 - Update the stack and implement the original yml file. If you didn't save this, it will be in the S3 bucket that contains the CUR files. 3 - If you change the permissions ACL on the old CUR files, follow the same process - but remove the member/linked account from the ACL. 4 - Login to the member/linked account, go to the Glue Dashboard 5 - Delete the database that was created","title":"4.3 Use Athena to access a CUR in Master/Payer"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Lab_Guide.html#5-rate-this-lab","text":"","title":"5. Rate this lab"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Code/IAM_Athena.html","text":"IAM policy for access to Athena NOTE: This Policy is to be used as a starting point only. Ensure to follow security best practices and only provide the minimum required access. You will also need to modify the and fields before use. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"athena:StartQueryExecution\", \"glue:GetCrawler\", \"glue:GetDataCatalogEncryptionSettings\", \"glue:GetTableVersions\", \"glue:GetPartitions\", \"athena:GetQueryResults\", \"athena:ListWorkGroups\", \"athena:GetNamedQuery\", \"glue:GetDevEndpoint\", \"glue:GetSecurityConfiguration\", \"glue:GetResourcePolicy\", \"glue:GetTrigger\", \"glue:GetUserDefinedFunction\", \"athena:GetExecutionEngine\", \"glue:GetJobRun\", \"athena:GetExecutionEngines\", \"s3:HeadBucket\", \"glue:GetUserDefinedFunctions\", \"glue:GetClassifier\", \"s3:PutAccountPublicAccessBlock\", \"athena:GetQueryResultsStream\", \"glue:GetJobs\", \"glue:GetTables\", \"glue:GetTriggers\", \"athena:GetNamespace\", \"athena:GetQueryExecutions\", \"athena:GetCatalogs\", \"athena:ListNamedQueries\", \"athena:GetNamespaces\", \"glue:GetPartition\", \"glue:GetDevEndpoints\", \"athena:GetTables\", \"athena:GetTable\", \"athena:BatchGetNamedQuery\", \"athena:BatchGetQueryExecution\", \"glue:GetJob\", \"glue:GetConnections\", \"glue:GetCrawlers\", \"glue:GetClassifiers\", \"athena:ListQueryExecutions\", \"glue:GetCatalogImportStatus\", \"athena:GetWorkGroup\", \"glue:GetConnection\", \"glue:BatchGetPartition\", \"glue:GetSecurityConfigurations\", \"glue:GetDatabases\", \"athena:ListTagsForResource\", \"glue:GetTable\", \"glue:GetDatabase\", \"s3:GetAccountPublicAccessBlock\", \"glue:GetDataflowGraph\", \"s3:ListAllMyBuckets\", \"athena:GetQueryExecution\", \"glue:GetPlan\", \"glue:GetCrawlerMetrics\", \"glue:GetJobRuns\" ], \"Resource\": \"*\" }, { \"Sid\": \"VisualEditor1\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\", \"s3:GetObject\", \"s3:ListBucketMultipartUploads\", \"s3:AbortMultipartUpload\", \"s3:CreateBucket\", \"s3:ListBucket\", \"s3:GetBucketLocation\", \"s3:ListMultipartUploadParts\" ], \"Resource\": [ \"arn:aws:s3:::aws-athena-query-results-<Account ID>-us-east-1\", \"arn:aws:s3:::aws-athena-query-results-<Account ID>-us-east-1/*\" ] }, { \"Sid\": \"VisualEditor2\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucketByTags\", \"s3:GetLifecycleConfiguration\", \"s3:GetBucketTagging\", \"s3:GetInventoryConfiguration\", \"s3:GetObjectVersionTagging\", \"s3:ListBucketVersions\", \"s3:GetBucketLogging\", \"s3:ListBucket\", \"s3:GetAccelerateConfiguration\", \"s3:GetBucketPolicy\", \"s3:GetObjectVersionTorrent\", \"s3:GetObjectAcl\", \"s3:GetEncryptionConfiguration\", \"s3:GetBucketRequestPayment\", \"s3:GetObjectVersionAcl\", \"s3:GetObjectTagging\", \"s3:GetMetricsConfiguration\", \"s3:GetBucketPublicAccessBlock\", \"s3:GetBucketPolicyStatus\", \"s3:ListBucketMultipartUploads\", \"s3:GetBucketWebsite\", \"s3:GetBucketVersioning\", \"s3:GetBucketAcl\", \"s3:GetBucketNotification\", \"s3:GetReplicationConfiguration\", \"s3:ListMultipartUploadParts\", \"s3:GetObject\", \"s3:GetObjectTorrent\", \"s3:GetBucketCORS\", \"s3:GetAnalyticsConfiguration\", \"s3:GetObjectVersionForReplication\", \"s3:GetBucketLocation\", \"s3:GetObjectVersion\" ], \"Resource\": [ \"arn:aws:s3:::<S3 CUR Bucket>/*\", \"arn:aws:s3:::<S3 CUR Bucket>\" ] } ] }","title":"IAM Athena"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Code/S3_Bucket_Policy.html","text":"Bucket policy for member/linked account access to CUR files NOTE: Replace the Account ID [Sub-Account ID] with your own account ID, and the bucket name [S3 Bucket Name] with your bucket name. { \"Version\": \"2008-10-17\", \"Id\": \"Policy1335892530063\", \"Statement\": [ { \"Sid\": \"Stmt1335892150622\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::386209384616:root\" }, \"Action\": [ \"s3:GetBucketAcl\", \"s3:GetBucketPolicy\" ], \"Resource\": \"arn:aws:s3:::[S3 Bucket Name]\" }, { \"Sid\": \"Stmt1335892526596\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::386209384616:root\" }, \"Action\": \"s3:PutObject\", \"Resource\": \"arn:aws:s3:::[S3 Bucket Name]/*\" }, { \"Sid\": \"Stmt1546900919345\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::[Sub-Account ID]:root\" }, \"Action\": \"s3:ListBucket\", \"Resource\": \"arn:aws:s3:::[S3 Bucket Name]\" }, { \"Sid\": \"Stmt1546901049588\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::[Sub-Account ID]:root\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::[S3 Bucket Name]/*\" } ] }","title":"S3 Bucket Policy"},{"location":"Cost/Cost_and_Usage_Analysis/300_Multi_Account_CUR_Access/Code/crawler-cfn.html","text":"Below is a copy of the crawler config file. Modifications are between ' * ' characters. Variables that need to be changed below: (CUR Billing Bucket) (name): the account name of the Payer containing the CUR, this is the email excluding @companyname.com (name2): the account name of the linked account accessing the CUR (Canonical ID): the Canonical User ID for the Payer containing the CUR, to get the Canonical ID, refer to: https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html (Canonical ID2): the Canonical User ID for the linked account accessing the CUR AWSTemplateFormatVersion: 2010-09-09 Resources: AWSCURDatabase: Type: 'AWS::Glue::Database' Properties: DatabaseInput: Name: 'athenacurcfn_workshop_c_u_r' CatalogId: !Ref AWS::AccountId AWSCURCrawlerComponentFunction: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - glue.amazonaws.com Action: - 'sts:AssumeRole' Path: / ManagedPolicyArns: - 'arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole' Policies: - PolicyName: AWSCURCrawlerComponentFunction PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' Resource: 'arn:aws:logs:*:*:*' - Effect: Allow Action: - 'glue:UpdateDatabase' - 'glue:UpdatePartition' - 'glue:CreateTable' - 'glue:UpdateTable' - 'glue:ImportCatalogToGlue' Resource: '*' - Effect: Allow Action: - 's3:GetObject' - 's3:PutObject' Resource: arn:aws:s3:::(CUR Billing Bucket)* AWSCURCrawlerLambdaExecutor: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - 'sts:AssumeRole' Path: / Policies: - PolicyName: AWSCURCrawlerLambdaExecutor PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' Resource: 'arn:aws:logs:*:*:*' - Effect: Allow Action: - 'glue:StartCrawler' Resource: '*' ***** - Effect: Allow Action: - 's3:PutObjectVersionAcl' - 's3:PutObjectAcl' Resource: 'arn:aws:s3:::(CUR Billing Bucket)/*' ***** AWSCURCrawler: Type: 'AWS::Glue::Crawler' DependsOn: - AWSCURDatabase - AWSCURCrawlerComponentFunction Properties: Name: AWSCURCrawler-WorkshopCUR Description: A recurring crawler that keeps your CUR table in Athena up-to-date. Role: !GetAtt AWSCURCrawlerComponentFunction.Arn DatabaseName: !Ref AWSCURDatabase Targets: S3Targets: - Path: 's3://(CUR Billing Bucket)' Exclusions: - '**.json' - '**.yml' - '**.sql' - '**.csv' - '**.gz' - '**.zip' SchemaChangePolicy: UpdateBehavior: UPDATE_IN_DATABASE DeleteBehavior: DELETE_FROM_DATABASE AWSCURInitializer: Type: 'AWS::Lambda::Function' DependsOn: AWSCURCrawler Properties: Code: ZipFile: > const AWS = require('aws-sdk'); const response = require('cfn-response'); ***** const util = require('util'); ***** exports.handler = function(event, context, callback) { if (event.RequestType === 'Delete') { response.send(event, context, response.SUCCESS); } else { ***** // Read options from the event. console.log(\"Reading options from event:\\n\", util.inspect(event, {depth: 5})); var srcBucket = event.Records[0].s3.bucket.name; // Object key may have spaces or unicode non-ASCII characters. var srcKey = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, \" \")); // New Object ACL to be written var params = { Bucket: srcBucket, Key: srcKey, AccessControlPolicy: { 'Owner': { 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Grants': [ { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name)', 'ID': '(Canonical ID)' }, 'Permission': 'FULL_CONTROL' }, { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': '(name2)', 'ID': '(Canonical ID2)' }, 'Permission': 'READ' }, ] } }; // get reference to S3 client var s3 = new AWS.S3(); // Put the ACL on the object s3.putObjectAcl(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); ***** const glue = new AWS.Glue(); glue.startCrawler({ Name: 'AWSCURCrawler-WorkshopCUR' }, function(err, data) { if (err) { const responseData = JSON.parse(this.httpResponse.body); if (responseData['__type'] == 'CrawlerRunningException') { callback(null, responseData.Message); } else { const responseString = JSON.stringify(responseData); if (event.ResponseURL) { response.send(event, context, response.FAILED,{ msg: responseString }); } else { callback(responseString); } } } else { if (event.ResponseURL) { response.send(event, context, response.SUCCESS); } else { callback(null, response.SUCCESS); } } }); } }; Handler: 'index.handler' Timeout: 30 Runtime: nodejs8.10 ReservedConcurrentExecutions: 1 Role: !GetAtt AWSCURCrawlerLambdaExecutor.Arn AWSStartCURCrawler: Type: 'Custom::AWSStartCURCrawler' Properties: ServiceToken: !GetAtt AWSCURInitializer.Arn AWSS3CUREventLambdaPermission: Type: AWS::Lambda::Permission Properties: Action: 'lambda:InvokeFunction' FunctionName: !GetAtt AWSCURInitializer.Arn Principal: 's3.amazonaws.com' SourceAccount: !Ref AWS::AccountId SourceArn: 'arn:aws:s3:::(CUR Billing Bucket)' AWSS3CURLambdaExecutor: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - 'sts:AssumeRole' Path: / Policies: - PolicyName: AWSS3CURLambdaExecutor PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' Resource: 'arn:aws:logs:*:*:*' - Effect: Allow Action: - 's3:PutBucketNotification' Resource: 'arn:aws:s3:::(CUR Billing Bucket)' AWSS3CURNotification: Type: 'AWS::Lambda::Function' DependsOn: - AWSCURInitializer - AWSS3CUREventLambdaPermission - AWSS3CURLambdaExecutor Properties: Code: ZipFile: > const AWS = require('aws-sdk'); const response = require('cfn-response'); exports.handler = function(event, context, callback) { const s3 = new AWS.S3(); const putConfigRequest = function(notificationConfiguration) { return new Promise(function(resolve, reject) { s3.putBucketNotificationConfiguration({ Bucket: event.ResourceProperties.BucketName, NotificationConfiguration: notificationConfiguration }, function(err, data) { if (err) reject({ msg: this.httpResponse.body.toString(), error: err, data: data }); else resolve(data); }); }); }; const newNotificationConfig = {}; if (event.RequestType !== 'Delete') { newNotificationConfig.LambdaFunctionConfigurations = [{ Events: [ 's3:ObjectCreated:*' ], LambdaFunctionArn: event.ResourceProperties.TargetLambdaArn || 'missing arn', Filter: { Key: { FilterRules: [ { Name: 'prefix', Value: event.ResourceProperties.ReportKey } ] } } }]; } putConfigRequest(newNotificationConfig).then(function(result) { response.send(event, context, response.SUCCESS, result); callback(null, result); }).catch(function(error) { response.send(event, context, response.FAILED, error); console.log(error); callback(error); }); }; Handler: 'index.handler' Timeout: 30 Runtime: nodejs8.10 ReservedConcurrentExecutions: 1 Role: !GetAtt AWSS3CURLambdaExecutor.Arn AWSPutS3CURNotification: Type: 'Custom::AWSPutS3CURNotification' Properties: ServiceToken: !GetAtt AWSS3CURNotification.Arn TargetLambdaArn: !GetAtt AWSCURInitializer.Arn BucketName: '(CUR Billing Bucket)' ReportKey: 'cur/WorkshopCUR/WorkshopCUR' AWSCURReportStatusTable: Type: 'AWS::Glue::Table' DependsOn: AWSCURDatabase Properties: DatabaseName: athenacurcfn_workshop_c_u_r CatalogId: !Ref AWS::AccountId TableInput: Name: 'cost_and_usage_data_status' TableType: 'EXTERNAL_TABLE' StorageDescriptor: Columns: - Name: status Type: 'string' InputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OutputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' SerdeInfo: SerializationLibrary: 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' Location: 's3://(CUR Billing Bucket)/cur/WorkshopCUR/cost_and_usage_data_status/'","title":"Crawler cfn"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html","text":"Level 300: Splitting the CUR and Sharing Access https://wellarchitectedlabs.com Introduction This hands-on lab will guide you on how to automatically extract part of your CUR file, and then deliver it to another S3 bucket and folder to allow another account to access it. This is useful to allow sub accounts or business units to access their data, but not see the rest of the original CUR file. You can also exclude specific columns such as pricing - only allowing a sub account to view their usage information. Common use cases are: Separate linked account data, so each linked account can see only their data Providing sub accounts their data without pricing Separate out specific usage, by tag or service The lab has been designed to configure a system that can expand easily, for any new requirement: Create a new folder in S3 with the required bucket policy Do the one-off back fill for previous months (if required) Create the saved queries in Athena Specify the permissions in the Lambda script Goals Automatically extract a portion of the CUR file each time it is delivered Deliver this to a location that is accessible to another account Prerequisites Multiple AWS Accounts (At least two) Billing reports auto update configured as per 300_Automated_CUR_Updates_and_Ingestion Permissions required Create IAM policies and roles Create and modify S3 Buckets, including policies and events Create and modify Lambda functions Modify CloudFormation templates Create, save and execute Athena queries Create and run a Glue crawler Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#level-300-splitting-the-cur-and-sharing-access","text":"https://wellarchitectedlabs.com","title":"Level 300: Splitting the CUR and Sharing Access"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#introduction","text":"This hands-on lab will guide you on how to automatically extract part of your CUR file, and then deliver it to another S3 bucket and folder to allow another account to access it. This is useful to allow sub accounts or business units to access their data, but not see the rest of the original CUR file. You can also exclude specific columns such as pricing - only allowing a sub account to view their usage information. Common use cases are: Separate linked account data, so each linked account can see only their data Providing sub accounts their data without pricing Separate out specific usage, by tag or service The lab has been designed to configure a system that can expand easily, for any new requirement: Create a new folder in S3 with the required bucket policy Do the one-off back fill for previous months (if required) Create the saved queries in Athena Specify the permissions in the Lambda script","title":"Introduction"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#goals","text":"Automatically extract a portion of the CUR file each time it is delivered Deliver this to a location that is accessible to another account","title":"Goals"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#prerequisites","text":"Multiple AWS Accounts (At least two) Billing reports auto update configured as per 300_Automated_CUR_Updates_and_Ingestion","title":"Prerequisites"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#permissions-required","text":"Create IAM policies and roles Create and modify S3 Buckets, including policies and events Create and modify Lambda functions Modify CloudFormation templates Create, save and execute Athena queries Create and run a Glue crawler","title":"Permissions required"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html","text":"Level 300: Splitting the CUR and Sharing Access https://wellarchitectedlabs.com Authors Nathan Besh, Cost Lead, Well-Architected Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Setup Output S3 Bucket Perform one off Fill of Member/Linked Data Create Athena Saved Queries to Write new Data Create Lambda function to run the Saved Queries Trigger the Lambda When a CUR is Delivered Sub Account Crawler Setup Tear down Rate this Lab 1. Setup Output S3 Bucket We need to provide a location to deliver the output from the Athena queries, so that it can be secured and restricted to the sub accounts. We'll need to create the S3 bucket, and implement a Lambda function to re-write the object ACLs when new objects are delivered. So what we'll do is as follows: Create the output S3 bucket with the required bucket policy Create an IAM policy that will allow a Lambda function to re-write object ACLs Implement the Lambda function 1 - Login to the master/payer account as an IAM user with the required permissions. 2 - Go to the S3 console 3 - Create the output S3 bucket 4 - The lab has been designed to allow multiple statements to output to a single bucket, each in a different folder. Create one folder for each Athena statement you will run, a convenient name for the folders is the Account ID of the sub account. 5 - Go to Permissions , and implement a bucket policy to allow sub accounts access, ensure you follow security best practices and allow least privilege: You can modify this sample policy as a starting point: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowListingOfFolders\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::(account ID):root\" }, \"Action\": \"s3:ListBucket\", \"Resource\": \"arn:aws:s3:::(bucket)\" }, { \"Sid\": \"AllowAllS3ActionsInSubFolder\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::(account ID):root\" }, \"Action\": \"s3:*\", \"Resource\": \"arn:aws:s3:::(bucket)/(folder)/*\" } ] } 6 - Go to the IAM Dashboard 7 - Create an IAM policy Lambda_S3Linked_PutACL to allow lambda to write ACLs: You can modify the following sample policy as a starting point: NOTE: replace (bucket name): { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObjectVersionAcl\", \"s3:PutObjectAcl\" ], \"Resource\": \"arn:aws:s3:::(bucket name)/*\" } ] } 8 - Create an IAM role for Lambda named Lambda_Put_Linked_S3ACL 9 - Attach the Lambda_S3Linked_PutACL policy: 10 - Go to the Lambda service dashboard 11 - Create the lambda function S3LinkedPutACL with the following details: Node.js Role: Lambda_Put_Linked_S3ACL Code: ./Code/S3LinkedPutACL.md 12 - Go to the S3 service dashboard 13 - Select the Output Bucket , go to Properties , and add an S3 event to trigger on All object create events , and have it run the S3LinkedPutACL Lambda function: 14 - Test the configuration is working correctly by uploading a file into the S3 folder. Verify that it has multiple Grantees to the required accounts: 15 - Delete the file and ensure all folders are empty. The output bucket setup is now complete. Every time the Athena query runs and outputs a file into the S3 bucket, it will automatically have its permissions ACL updated to allow access to the sub account. 2. Perform one off Fill of Member/Linked Data Perform this step if you want to generate data for all previous months available in your current CUR files. This is a one off step that is performed manually. We create a temporary table in Athena, and write the output to the S3 location created above, for the member/linked account to access it. We then delete the temporary table - which does not delete the S3 output data. 1 - In the master/payer account go into the Athena service dashboard 2 - Create your query using the template below: The following statement will copy all columns from the source table if the line_item_usage_account_id matches a specific Account ID. It will output each month into a separate folder by using partitioning on the year and month , and output it to the S3 output folder. CREATE TABLE (database).temp_table WITH ( format = 'Parquet', parquet_compression = 'GZIP', external_location = 's3://(bucket)/(folder)', partitioned_by=ARRAY['year_1','month_1']) AS SELECT *, year as year_1, month as month_1 FROM \"(database)\".\"(table)\" where line_item_usage_account_id like '(account ID)' Some key points for your queries: Partitioning will allow us to write only the current months data each time, and not write all the data Parquet format is used, which allows faster access and reduced costs through reduced data scanning GZIP compression produces smaller output files than SNAPPY SNAPPY is faster than GZIP to run Example of performance with a source CUR of 6.3Gb: Using Parquet and GZIP, it will take approximate 11min 16sec, and produce 8.4Gb of output files Using Parquet and SNAPPY, it will take approximately 7min 8sec, and produce 12.2Gb of output files 3 - Execute the statement in Athena: 4 - Go into the S3 service dashboard 5 - Go to the output bucket and folder 6 - Verify the data has been populated into the S3 folders 7 - Verify the permissions are correct on the files - there should be multiple Grantees : 8 - Then delete the temp table from Athena by modifying the following code: (this will NOT delete the s3 data) DROP TABLE (database).temp_table 3. Create Athena Saved Queries to Write new Data Next we setup your recurring Athena queries. These will run each time a new CUR file is delivered, separate out the information for the sub accounts, and write it to the output S3 location. These queries will be very similar to the one above, except it will only extract data for the current month. You must write one query for the extraction of the data, which will create a temporary table, and then a second query to delete the table. As the system has been written for future expansion, you must adhere to the guidelines below when writing and naming statements (other wise you will need to change the code): The queries MUST start with: create_linked_ and delete_linked_ otherwise you'll need to modify the Lambda function. As Lambda looks for this string to identify these queries to automatically run when new files are delivered The output location must also end in the actual word subfolder as this will be re-written by the lambda function, to the current year and month The queries must include the component CAST(bill_billing_period_start_date as VARCHAR) like concat(substr(CAST(current_date as VARCHAR),1,7),'-01%') which ensures the query only gets data from the current month There is no need to include the columns year as year_1 and month as month_1 , as that was only used for partitioning 1 - Create the saved query in Athena named create_linked_(folder name) , the following sample code is the accompanying query for the previous query above: CREATE TABLE (database).temp_table WITH ( format = 'Parquet', parquet_compression = 'GZIP', external_location = 's3://(bucket)/(folder)/subfolder') AS SELECT * FROM \"(database)\".\"(table)\" where line_item_usage_account_id like '(some value)' and CAST(bill_billing_period_start_date as VARCHAR) like concat(substr(CAST(current_date as VARCHAR),1,7),'-01%') 2 - Create the accompanying delete statement named delete_linked_(folder name) to delete the temporary table: drop TABLE IF EXISTS (database).temp_table; 3 - Repeat the steps above for any additional create and delete queries as required. 4. Create Lambda function to run the Saved Queries This Lambda function ties everything together, it will remove all objects in the current months S3 folders, find the Athena queries to run, and then execute the saved Athena queries. First we will create the role with permissions for Lambda to use, then the Lambda function itself. 1 - Go to the IAM service dashboard 2 - Create a policy named LambdaSubAcctSplit 3 - Edit the following policy inline with security best practices, and add it to the policy: ./Code/SubAcctSplit_Role.md 4 - Create a Role for Lambda to call services 5 - Attach the LambdaSubAcctSplit policy 6 - Name the role LambdaSubAcctSplit 7 - Go into the Lambda service dashboard 8 - Create a function named SubAcctSplit , Author from scratch using the Python 3.7 Runtime and role LambdaSubAcctSplit : 9 - Copy the code into the editor from here: ./Code/Sub_Account_Split.md 10 - Edit the code as per the instructions at the top. 11 - Under Basic settings set the Timeout to 30seconds, and review this after the test at the end 12 - Change the Execution role to LambdaSubAcctSplit 13 - Save the function 14 - Test that the function by clicking on the Test button at the top, and make sure that it executes correctly: 15 - Go into the S3 Service dashboard, view the output folder and verify that there are files for the current month. Check the Last modified time stamp to ensure they were created at the time of the test. You have now setup the Lambda function which executes the queries. The final step is to trigger this Lambda function every time a new CUR file is delivered. 5. Trigger the Lambda When a CUR is Delivered It is assumed that you have completed 300_Automated_CUR_Updates_and_Ingestion, so there is an existing Lambda function that is being executed when a new CUR file is delivered. We will add code into this setup to trigger the new Lambda function. 1 - Go to the CloudFormation service Dashboard 2 - Select the current stack which updates the Glue database 3 - Download the current template (crawler-cfn.yml file), and save this for later (if Teardown is required) 4 - Open the template up in a text editor of your choice 5 - A sample crawler file is below: ./Code/crawler-cfn.md 6 - Update the AWSCURCrawlerLambdaExecutor IAM role section, inside the PolicyName AWSCURCrawlerLambdaExecutor section: Add the following Action: 'lambda:InvokeFunction' Edit the following line , and add the following resource - 'arn:aws:lambda:<region>:<accountID>:function:SubAcctSplit' 7 - Make the following amendments to the AWSCURInitializer Lambda function section, inside the else statement after the glue section: var lambda = new AWS.Lambda(); var params = { FunctionName: 'SubAcctSplit' }; lambda.invoke(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); 8 - Save the new template file 9 - In the CloudFormation console update the stack 10 - Replace the current template with the new one, and upload your modified template 11 - After the stack has successfully updated, you can test the function 12 - Go to the S3 service dashboard, navigate to the source bucket and folder containing the current months original master/payer CUR file 13 - Download the CUR file, and delete the object from the bucket 14 - Re-upload the current CUR file back into its bucket 15 - Navigate to the output bucket and folder for the current month 16 - Check the Last modified time stamp on the object/s is/are the current time, and check that it has the correct Grantees in the permissions Setup is now complete for the payer account. When new CUR files are delivered, it will execute the Athena queries and extract the required data for the current month, and output it to the required S3 folder with the required permissions. 6. Sub Account Crawler Setup The final step is to setup the sub account to automatically scan the S3 folders each morning using a Glue Crawler, and update a local Athena database. 1 - Login to the sub account as an IAM user with the required permissions, and go into the Glue console . 2 - Add a Crawler with the following details: Include path : the S3 bucket in the account with the delivered CURs Exclude patterns : **.json, **.yml, **.sql, **.csv, **.gz, **.zip (1 per line) 3 - Create a new role for the crawler to use 4 - Create a daily schedule to update the tables each morning before you come into work 5 - Create a new database 6 - Review the crawler configuration and finish: 7 - Run the crawler, and check that it has added tables. 8 - Go into Athena and execute a preview query to verify access and the data. You have now given the sub account access to their specific CUR files as extracted from the Master/Payer CUR file. This will be automatically updated on any new versions delivered, or any new months delivered. 7. Tear Down We will tear down this lab, removing any data, resources and configuration that it created. We will restore any modified code or resources to their original state before the lab. 7.1 Sub Account 1 - Log into the sub account as an IAM user with the required privileges 2 - Go to the Glue service dashboard 3 - Delete the created database and tables 4 - Delete the recurring Glue crawler 7.2 Master/Payer Account 1 - Log into the master/payer account as an IAM user with the required privileges 2 - Go to the Cloudformation service dashboard, and select the CUR update stack 3 - Update the stack and use the original Template yml file 4 - Go to the Lambda service dashboard 5 - Delete the SubAcctSplit and S3LinkedPutACL Lambda functions 6 - Go to the IAM service dashboard 7 - Delete the LambdaSubAcctSplit and Lambda_Put_Linked_S3ACL roles 8 - Delete the LambdaSubAcctSplit and Lambda_S3Linked_PutACL policies 9 - Go to the Athena service dashboard 10 - Delete the create_linked_ and delete_linked_ Athena saved queries 11 - Delete any temp tables 12 - Go into the S3 service dashboard 13 - Delete the S3 output folder 8. Rate this lab","title":"Lab Guide"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#level-300-splitting-the-cur-and-sharing-access","text":"https://wellarchitectedlabs.com","title":"Level 300: Splitting the CUR and Sharing Access"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#authors","text":"Nathan Besh, Cost Lead, Well-Architected","title":"Authors"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#table-of-contents","text":"Setup Output S3 Bucket Perform one off Fill of Member/Linked Data Create Athena Saved Queries to Write new Data Create Lambda function to run the Saved Queries Trigger the Lambda When a CUR is Delivered Sub Account Crawler Setup Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#1-setup-output-s3-bucket","text":"We need to provide a location to deliver the output from the Athena queries, so that it can be secured and restricted to the sub accounts. We'll need to create the S3 bucket, and implement a Lambda function to re-write the object ACLs when new objects are delivered. So what we'll do is as follows: Create the output S3 bucket with the required bucket policy Create an IAM policy that will allow a Lambda function to re-write object ACLs Implement the Lambda function 1 - Login to the master/payer account as an IAM user with the required permissions. 2 - Go to the S3 console 3 - Create the output S3 bucket 4 - The lab has been designed to allow multiple statements to output to a single bucket, each in a different folder. Create one folder for each Athena statement you will run, a convenient name for the folders is the Account ID of the sub account. 5 - Go to Permissions , and implement a bucket policy to allow sub accounts access, ensure you follow security best practices and allow least privilege: You can modify this sample policy as a starting point: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowListingOfFolders\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::(account ID):root\" }, \"Action\": \"s3:ListBucket\", \"Resource\": \"arn:aws:s3:::(bucket)\" }, { \"Sid\": \"AllowAllS3ActionsInSubFolder\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::(account ID):root\" }, \"Action\": \"s3:*\", \"Resource\": \"arn:aws:s3:::(bucket)/(folder)/*\" } ] } 6 - Go to the IAM Dashboard 7 - Create an IAM policy Lambda_S3Linked_PutACL to allow lambda to write ACLs: You can modify the following sample policy as a starting point: NOTE: replace (bucket name): { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObjectVersionAcl\", \"s3:PutObjectAcl\" ], \"Resource\": \"arn:aws:s3:::(bucket name)/*\" } ] } 8 - Create an IAM role for Lambda named Lambda_Put_Linked_S3ACL 9 - Attach the Lambda_S3Linked_PutACL policy: 10 - Go to the Lambda service dashboard 11 - Create the lambda function S3LinkedPutACL with the following details: Node.js Role: Lambda_Put_Linked_S3ACL Code: ./Code/S3LinkedPutACL.md 12 - Go to the S3 service dashboard 13 - Select the Output Bucket , go to Properties , and add an S3 event to trigger on All object create events , and have it run the S3LinkedPutACL Lambda function: 14 - Test the configuration is working correctly by uploading a file into the S3 folder. Verify that it has multiple Grantees to the required accounts: 15 - Delete the file and ensure all folders are empty. The output bucket setup is now complete. Every time the Athena query runs and outputs a file into the S3 bucket, it will automatically have its permissions ACL updated to allow access to the sub account.","title":"1. Setup Output S3 Bucket "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#2-perform-one-off-fill-of-memberlinked-data","text":"Perform this step if you want to generate data for all previous months available in your current CUR files. This is a one off step that is performed manually. We create a temporary table in Athena, and write the output to the S3 location created above, for the member/linked account to access it. We then delete the temporary table - which does not delete the S3 output data. 1 - In the master/payer account go into the Athena service dashboard 2 - Create your query using the template below: The following statement will copy all columns from the source table if the line_item_usage_account_id matches a specific Account ID. It will output each month into a separate folder by using partitioning on the year and month , and output it to the S3 output folder. CREATE TABLE (database).temp_table WITH ( format = 'Parquet', parquet_compression = 'GZIP', external_location = 's3://(bucket)/(folder)', partitioned_by=ARRAY['year_1','month_1']) AS SELECT *, year as year_1, month as month_1 FROM \"(database)\".\"(table)\" where line_item_usage_account_id like '(account ID)' Some key points for your queries: Partitioning will allow us to write only the current months data each time, and not write all the data Parquet format is used, which allows faster access and reduced costs through reduced data scanning GZIP compression produces smaller output files than SNAPPY SNAPPY is faster than GZIP to run Example of performance with a source CUR of 6.3Gb: Using Parquet and GZIP, it will take approximate 11min 16sec, and produce 8.4Gb of output files Using Parquet and SNAPPY, it will take approximately 7min 8sec, and produce 12.2Gb of output files 3 - Execute the statement in Athena: 4 - Go into the S3 service dashboard 5 - Go to the output bucket and folder 6 - Verify the data has been populated into the S3 folders 7 - Verify the permissions are correct on the files - there should be multiple Grantees : 8 - Then delete the temp table from Athena by modifying the following code: (this will NOT delete the s3 data) DROP TABLE (database).temp_table","title":"2. Perform one off Fill of Member/Linked Data "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#3-create-athena-saved-queries-to-write-new-data","text":"Next we setup your recurring Athena queries. These will run each time a new CUR file is delivered, separate out the information for the sub accounts, and write it to the output S3 location. These queries will be very similar to the one above, except it will only extract data for the current month. You must write one query for the extraction of the data, which will create a temporary table, and then a second query to delete the table. As the system has been written for future expansion, you must adhere to the guidelines below when writing and naming statements (other wise you will need to change the code): The queries MUST start with: create_linked_ and delete_linked_ otherwise you'll need to modify the Lambda function. As Lambda looks for this string to identify these queries to automatically run when new files are delivered The output location must also end in the actual word subfolder as this will be re-written by the lambda function, to the current year and month The queries must include the component CAST(bill_billing_period_start_date as VARCHAR) like concat(substr(CAST(current_date as VARCHAR),1,7),'-01%') which ensures the query only gets data from the current month There is no need to include the columns year as year_1 and month as month_1 , as that was only used for partitioning 1 - Create the saved query in Athena named create_linked_(folder name) , the following sample code is the accompanying query for the previous query above: CREATE TABLE (database).temp_table WITH ( format = 'Parquet', parquet_compression = 'GZIP', external_location = 's3://(bucket)/(folder)/subfolder') AS SELECT * FROM \"(database)\".\"(table)\" where line_item_usage_account_id like '(some value)' and CAST(bill_billing_period_start_date as VARCHAR) like concat(substr(CAST(current_date as VARCHAR),1,7),'-01%') 2 - Create the accompanying delete statement named delete_linked_(folder name) to delete the temporary table: drop TABLE IF EXISTS (database).temp_table; 3 - Repeat the steps above for any additional create and delete queries as required.","title":"3. Create Athena Saved Queries to Write new Data "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#4-create-lambda-function-to-run-the-saved-queries","text":"This Lambda function ties everything together, it will remove all objects in the current months S3 folders, find the Athena queries to run, and then execute the saved Athena queries. First we will create the role with permissions for Lambda to use, then the Lambda function itself. 1 - Go to the IAM service dashboard 2 - Create a policy named LambdaSubAcctSplit 3 - Edit the following policy inline with security best practices, and add it to the policy: ./Code/SubAcctSplit_Role.md 4 - Create a Role for Lambda to call services 5 - Attach the LambdaSubAcctSplit policy 6 - Name the role LambdaSubAcctSplit 7 - Go into the Lambda service dashboard 8 - Create a function named SubAcctSplit , Author from scratch using the Python 3.7 Runtime and role LambdaSubAcctSplit : 9 - Copy the code into the editor from here: ./Code/Sub_Account_Split.md 10 - Edit the code as per the instructions at the top. 11 - Under Basic settings set the Timeout to 30seconds, and review this after the test at the end 12 - Change the Execution role to LambdaSubAcctSplit 13 - Save the function 14 - Test that the function by clicking on the Test button at the top, and make sure that it executes correctly: 15 - Go into the S3 Service dashboard, view the output folder and verify that there are files for the current month. Check the Last modified time stamp to ensure they were created at the time of the test. You have now setup the Lambda function which executes the queries. The final step is to trigger this Lambda function every time a new CUR file is delivered.","title":"4. Create Lambda function to run the Saved Queries "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#5-trigger-the-lambda-when-a-cur-is-delivered","text":"It is assumed that you have completed 300_Automated_CUR_Updates_and_Ingestion, so there is an existing Lambda function that is being executed when a new CUR file is delivered. We will add code into this setup to trigger the new Lambda function. 1 - Go to the CloudFormation service Dashboard 2 - Select the current stack which updates the Glue database 3 - Download the current template (crawler-cfn.yml file), and save this for later (if Teardown is required) 4 - Open the template up in a text editor of your choice 5 - A sample crawler file is below: ./Code/crawler-cfn.md 6 - Update the AWSCURCrawlerLambdaExecutor IAM role section, inside the PolicyName AWSCURCrawlerLambdaExecutor section: Add the following Action: 'lambda:InvokeFunction' Edit the following line , and add the following resource - 'arn:aws:lambda:<region>:<accountID>:function:SubAcctSplit' 7 - Make the following amendments to the AWSCURInitializer Lambda function section, inside the else statement after the glue section: var lambda = new AWS.Lambda(); var params = { FunctionName: 'SubAcctSplit' }; lambda.invoke(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); 8 - Save the new template file 9 - In the CloudFormation console update the stack 10 - Replace the current template with the new one, and upload your modified template 11 - After the stack has successfully updated, you can test the function 12 - Go to the S3 service dashboard, navigate to the source bucket and folder containing the current months original master/payer CUR file 13 - Download the CUR file, and delete the object from the bucket 14 - Re-upload the current CUR file back into its bucket 15 - Navigate to the output bucket and folder for the current month 16 - Check the Last modified time stamp on the object/s is/are the current time, and check that it has the correct Grantees in the permissions Setup is now complete for the payer account. When new CUR files are delivered, it will execute the Athena queries and extract the required data for the current month, and output it to the required S3 folder with the required permissions.","title":"5. Trigger the Lambda When a CUR is Delivered "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#6-sub-account-crawler-setup","text":"The final step is to setup the sub account to automatically scan the S3 folders each morning using a Glue Crawler, and update a local Athena database. 1 - Login to the sub account as an IAM user with the required permissions, and go into the Glue console . 2 - Add a Crawler with the following details: Include path : the S3 bucket in the account with the delivered CURs Exclude patterns : **.json, **.yml, **.sql, **.csv, **.gz, **.zip (1 per line) 3 - Create a new role for the crawler to use 4 - Create a daily schedule to update the tables each morning before you come into work 5 - Create a new database 6 - Review the crawler configuration and finish: 7 - Run the crawler, and check that it has added tables. 8 - Go into Athena and execute a preview query to verify access and the data. You have now given the sub account access to their specific CUR files as extracted from the Master/Payer CUR file. This will be automatically updated on any new versions delivered, or any new months delivered.","title":"6. Sub Account Crawler Setup "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#7-tear-down","text":"We will tear down this lab, removing any data, resources and configuration that it created. We will restore any modified code or resources to their original state before the lab.","title":"7. Tear Down "},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#71-sub-account","text":"1 - Log into the sub account as an IAM user with the required privileges 2 - Go to the Glue service dashboard 3 - Delete the created database and tables 4 - Delete the recurring Glue crawler","title":"7.1 Sub Account"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#72-masterpayer-account","text":"1 - Log into the master/payer account as an IAM user with the required privileges 2 - Go to the Cloudformation service dashboard, and select the CUR update stack 3 - Update the stack and use the original Template yml file 4 - Go to the Lambda service dashboard 5 - Delete the SubAcctSplit and S3LinkedPutACL Lambda functions 6 - Go to the IAM service dashboard 7 - Delete the LambdaSubAcctSplit and Lambda_Put_Linked_S3ACL roles 8 - Delete the LambdaSubAcctSplit and Lambda_S3Linked_PutACL policies 9 - Go to the Athena service dashboard 10 - Delete the create_linked_ and delete_linked_ Athena saved queries 11 - Delete any temp tables 12 - Go into the S3 service dashboard 13 - Delete the S3 output folder","title":"7.2 Master/Payer Account"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Lab_Guide.html#8-rate-this-lab","text":"","title":"8. Rate this lab"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/IAM_Athena.html","text":"IAM policy for access to Athena NOTE: This Policy is to be used as a starting point only. Ensure to follow security best practices and only provide the minimum required access. You will also need to modify the and fields before use. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"athena:StartQueryExecution\", \"glue:GetCrawler\", \"glue:GetDataCatalogEncryptionSettings\", \"glue:GetTableVersions\", \"glue:GetPartitions\", \"athena:GetQueryResults\", \"athena:ListWorkGroups\", \"athena:GetNamedQuery\", \"glue:GetDevEndpoint\", \"glue:GetSecurityConfiguration\", \"glue:GetResourcePolicy\", \"glue:GetTrigger\", \"glue:GetUserDefinedFunction\", \"athena:GetExecutionEngine\", \"glue:GetJobRun\", \"athena:GetExecutionEngines\", \"s3:HeadBucket\", \"glue:GetUserDefinedFunctions\", \"glue:GetClassifier\", \"s3:PutAccountPublicAccessBlock\", \"athena:GetQueryResultsStream\", \"glue:GetJobs\", \"glue:GetTables\", \"glue:GetTriggers\", \"athena:GetNamespace\", \"athena:GetQueryExecutions\", \"athena:GetCatalogs\", \"athena:ListNamedQueries\", \"athena:GetNamespaces\", \"glue:GetPartition\", \"glue:GetDevEndpoints\", \"athena:GetTables\", \"athena:GetTable\", \"athena:BatchGetNamedQuery\", \"athena:BatchGetQueryExecution\", \"glue:GetJob\", \"glue:GetConnections\", \"glue:GetCrawlers\", \"glue:GetClassifiers\", \"athena:ListQueryExecutions\", \"glue:GetCatalogImportStatus\", \"athena:GetWorkGroup\", \"glue:GetConnection\", \"glue:BatchGetPartition\", \"glue:GetSecurityConfigurations\", \"glue:GetDatabases\", \"athena:ListTagsForResource\", \"glue:GetTable\", \"glue:GetDatabase\", \"s3:GetAccountPublicAccessBlock\", \"glue:GetDataflowGraph\", \"s3:ListAllMyBuckets\", \"athena:GetQueryExecution\", \"glue:GetPlan\", \"glue:GetCrawlerMetrics\", \"glue:GetJobRuns\" ], \"Resource\": \"*\" }, { \"Sid\": \"VisualEditor1\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\", \"s3:GetObject\", \"s3:ListBucketMultipartUploads\", \"s3:AbortMultipartUpload\", \"s3:CreateBucket\", \"s3:ListBucket\", \"s3:GetBucketLocation\", \"s3:ListMultipartUploadParts\" ], \"Resource\": [ \"arn:aws:s3:::aws-athena-query-results-<Account ID>-us-east-1\", \"arn:aws:s3:::aws-athena-query-results-<Account ID>-us-east-1/*\" ] }, { \"Sid\": \"VisualEditor2\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucketByTags\", \"s3:GetLifecycleConfiguration\", \"s3:GetBucketTagging\", \"s3:GetInventoryConfiguration\", \"s3:GetObjectVersionTagging\", \"s3:ListBucketVersions\", \"s3:GetBucketLogging\", \"s3:ListBucket\", \"s3:GetAccelerateConfiguration\", \"s3:GetBucketPolicy\", \"s3:GetObjectVersionTorrent\", \"s3:GetObjectAcl\", \"s3:GetEncryptionConfiguration\", \"s3:GetBucketRequestPayment\", \"s3:GetObjectVersionAcl\", \"s3:GetObjectTagging\", \"s3:GetMetricsConfiguration\", \"s3:GetBucketPublicAccessBlock\", \"s3:GetBucketPolicyStatus\", \"s3:ListBucketMultipartUploads\", \"s3:GetBucketWebsite\", \"s3:GetBucketVersioning\", \"s3:GetBucketAcl\", \"s3:GetBucketNotification\", \"s3:GetReplicationConfiguration\", \"s3:ListMultipartUploadParts\", \"s3:GetObject\", \"s3:GetObjectTorrent\", \"s3:GetBucketCORS\", \"s3:GetAnalyticsConfiguration\", \"s3:GetObjectVersionForReplication\", \"s3:GetBucketLocation\", \"s3:GetObjectVersion\" ], \"Resource\": [ \"arn:aws:s3:::<S3 CUR Bucket>/*\", \"arn:aws:s3:::<S3 CUR Bucket>\" ] } ] }","title":"IAM Athena"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/S3LinkedPutACL.html","text":"Here is the Lambda function to re-write object ACLs. It is triggered by an S3 Event, reads the folder from the object - and then applies the required object ACL: FULL_CONTROL for the owner, READ for the sub account. Edit the following fields in the code below: folder1 : The name of the folder where new files will be placed Owner Account Name : The owner account name - the account email without the @companyname, they will get FULL_CONTROL permissions Owner Canonical ID : The owner canonical ID, to get the Canonical ID, refer to: https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html Sub Account Name : The sub account name - the account email without the @companyname, they will get READ permissions Sub Acct Canonical ID : The sub account canonical ID const AWS = require('aws-sdk'); const util = require('util'); // Permissions for the new objects // Key MUST match the top level folder // Format: <owner account name> - <Canonical ID> - <sub account name> - <canonical ID> // This will give owner full permission & sub account read only permission var permissions = new Array(); var permissions = { '<folder1>': ['<owner acct name>','<Owner Canonical ID>','<sub account name>','<Sub Acct Canonical ID>'], '<folder2>': ['<owner acct name>','<Owner Canonical ID>','<sub account name>','<Sub Acct Canonical ID>'] }; // Main Loop exports.handler = function(event, context, callback) { // If its an object delete, do nothing if (event.RequestType === 'Delete') { } else // Its an object put { // Get the source bucket from the S3 event var srcBucket = event.Records[0].s3.bucket.name; // Object key may have spaces or unicode non-ASCII characters, decode it var srcKey = decodeURIComponent(event.Records[0].s3.object.key.replace(/\\+/g, \" \")); // Gets the top level folder, which is the key for the permissions array var folderID = srcKey.split(\"/\")[0]; // Define the object permissions, using the permissions array var params = { Bucket: srcBucket, Key: srcKey, AccessControlPolicy: { 'Owner': { 'DisplayName': permissions[folderID][0], 'ID': permissions[folderID][1] }, 'Grants': [ { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': permissions[folderID][0], 'ID': permissions[folderID][1] }, 'Permission': 'FULL_CONTROL' }, { 'Grantee': { 'Type': 'CanonicalUser', 'DisplayName': permissions[folderID][2], 'ID': permissions[folderID][3] }, 'Permission': 'READ' }, ] } }; // get reference to S3 client var s3 = new AWS.S3(); // Put the ACL on the object s3.putObjectAcl(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); } };","title":"S3LinkedPutACL"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/S3_Bucket_Policy.html","text":"Bucket policy for member/linked account access to CUR files NOTE: Replace the Account ID [Sub-Account ID] with your own account ID, and the bucket name [S3 Bucket Name] with your bucket name. { \"Version\": \"2008-10-17\", \"Id\": \"Policy1335892530063\", \"Statement\": [ { \"Sid\": \"Stmt1335892150622\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::386209384616:root\" }, \"Action\": [ \"s3:GetBucketAcl\", \"s3:GetBucketPolicy\" ], \"Resource\": \"arn:aws:s3:::[S3 Bucket Name]\" }, { \"Sid\": \"Stmt1335892526596\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::386209384616:root\" }, \"Action\": \"s3:PutObject\", \"Resource\": \"arn:aws:s3:::[S3 Bucket Name]/*\" }, { \"Sid\": \"Stmt1546900919345\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::[Sub-Account ID]:root\" }, \"Action\": \"s3:ListBucket\", \"Resource\": \"arn:aws:s3:::[S3 Bucket Name]\" }, { \"Sid\": \"Stmt1546901049588\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::[Sub-Account ID]:root\" }, \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::[S3 Bucket Name]/*\" } ] }","title":"S3 Bucket Policy"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/SubAcctSplit_Role.html","text":"Review the policy below, and use it as a starting point to create your policy for the Lambda fuction. The following fields will need to be changed: Output bucket: The S3 bucket that will contain the output from the Athena queries Account ID: the master/payer account ID Source bucket: the location of the original CUR files in the master/payer { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"athena:StartQueryExecution\", \"s3:DeleteObjectVersion\", \"athena:GetQueryResults\", \"s3:ListBucket\", \"athena:GetNamedQuery\", \"logs:PutLogEvents\", \"athena:ListQueryExecutions\", \"athena:ListNamedQueries\", \"s3:PutObject\", \"s3:GetObject\", \"logs:CreateLogStream\", \"athena:GetQueryExecution\", \"s3:DeleteObject\" ], \"Resource\": [ \"arn:aws:s3:::(output bucket)/*\", \"arn:aws:logs:us-east-1:(account ID):log-group:/aws/lambda/SubAcctSplit:*\", \"arn:aws:athena:*:*:workgroup/*\" ] }, { \"Sid\": \"VisualEditor1\", \"Effect\": \"Allow\", \"Action\": \"s3:ListBucket\", \"Resource\": \"arn:aws:s3:::*\" }, { \"Sid\": \"VisualEditor2\", \"Effect\": \"Allow\", \"Action\": [ \"glue:GetDatabase\", \"glue:CreateTable\", \"glue:GetPartitions\", \"glue:DeleteTable\", \"glue:GetTable\" ], \"Resource\": \"*\" }, { \"Sid\": \"VisualEditor3\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetBucketLocation\", \"s3:GetObject\", \"s3:ListBucket\", \"s3:ListBucketMultipartUploads\", \"s3:ListMultipartUploadParts\", \"s3:AbortMultipartUpload\", \"s3:CreateBucket\", \"s3:PutObject\" ], \"Resource\": [ \"arn:aws:s3:::aws-athena-query-results-us-east-1-(account ID)/*\", \"arn:aws:s3:::aws-athena-query-results-us-east-1-(account ID)\" ] }, { \"Sid\": \"VisualEditor4\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:ListBucket\" ], \"Resource\": \"arn:aws:s3:::(source bucket)/*\" }, { \"Sid\": \"VisualEditor5\", \"Effect\": \"Allow\", \"Action\": \"logs:CreateLogGroup\", \"Resource\": \"arn:aws:logs:us-east-1:(account ID):*\" } ] }","title":"SubAcctSplit Role"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/Sub_Account_Split.html","text":"Below is the code for the lambda function. You will need to modify the following variable: athena_output : This is where Athena puts output data, this is typically the master/payer Account ID, which is the default folder for Athena output queries bucketname : This is the output bucket for the Athena queries You will need to modify the following arrays, the order is important - the first folder in the subfolder array, will be given the permissions of the first element of the S3ObjectPolicies array. subfolders : This contains the list of folders that the queries write to S3ObjectPolicies : This contains the S3 Object permissions ACL that will be written to objects in the corresponding folder. You will need to add the owners details (master/payer account) and the grantee (sub account) details. import boto3 import json import datetime import time # Get the current date, so you know which months folder you're working on now = datetime.datetime.now() # Variables to construct the s3 folder name # YES! you can do multiple subfolders if you have multiple queries to run, 1 subfolder per query currentmonth = '/year_1=' + str(now.year) + '/month_1=' + str(now.month) + '/' bucketname = '(output bucket)' #List of Subfolders & ACLs to apply to objects in them #There MUST be a 1:1 between subfolders & policies subfolders = ['<folder1>'] # Arrays to hold the Athena delete & create queries that we need to run delete_query_strings = [] create_query_strings = [] # Athena output folder athena_output = 's3://aws-athena-query-results-us-east-1-<account ID>/' # Main loop def lambda_handler(event, context): # Clear the current months S3 folder s3_clear_folders() # Get the athena queries to run get_athena_queries() # Make sure to delete any existing temp tables, so no wobbly's are thrown run_delete_athena_queries() # Create the athena tables, which will actually output data to S3 folders run_create_athena_queries() # Delete the array in case of another Lambda invocation create_query_strings.clear() # You could make another call to delete the tables, however you need to make sure # the creates are finished, which may take some time, consuming time($) in Lambda # run_delete_athena_queries() # Delete the array in case of another Lambda invocation delete_query_strings.clear() return { 'statusCode': 200, 'body': json.dumps('Finished!') } # Clear the S3 folders for the current month def s3_clear_folders(): # Get S3 client/object client = boto3.client('s3') # For each subfolder - in case you have multilpe subfolders, i.e. multilpe accounts/business units to split data out to for subfolder in subfolders: # List all objects in the current months bucket response = client.list_objects_v2( Bucket=bucketname, Prefix=subfolder + currentmonth ) # Get how many objects there are to delete, if any keys = response['KeyCount'] # Only try to delete if there's objects if (keys > 0): # Get the ojbects from the response s3objects = response['Contents'] # For each object, we're going to delete it # cycle through the list of objects for s3object in s3objects: # Get the object key objectkey = s3object['Key'] # Delete the object response = client.delete_object( Bucket=bucketname, Key=objectkey ) # Get the Athena saved queries to run # They need to be labelled 'create_linked' or 'delete_linked' def get_athena_queries(): # Get Athena client/object client = boto3.client('athena') # Get all the saved queries in Athena response = client.list_named_queries() # Get the named query IDs from the response named_query_IDs = response['NamedQueryIds'] # Go through all the query ID, to find the delete & create queries we need to run for query_ID in named_query_IDs: # Get all the details of a named query using its ID named_query = client.get_named_query( NamedQueryId=query_ID ) # Get the query string & query name of the query querystring = named_query['NamedQuery']['QueryString'] queryname = named_query['NamedQuery']['Name'] # If its a create query, add it to the list of create queries # We also replace the '/subfolder' string in the query with the folder structure for the current month if 'create_linked_' in queryname: # Get a unique ID for the temp table tableID = queryname.split('_')[2] # String replacements to make the tablename unique, and work with the current months data new_query1 = querystring.replace('/subfolder', currentmonth) new_query2 = new_query1.replace('temp_table', 'temp_'+tableID) # Add the create query string to the array create_query_strings.append(new_query2) # If its a delete query, add it to the list of delete queries to execute later if 'delete_linked_' in queryname: # Get a unique ID for the temp table tableID = queryname.split('_')[2] # String replacements to make the tablename unique, and work with the current months data new_query1 = querystring.replace('temp_table', 'temp_'+tableID) # Add the delete query string to the array delete_query_strings.append(new_query1) # Run the delete Athena queries to remove any temp tables def run_delete_athena_queries(): # Get Athena client/object client = boto3.client('athena') # Go through each of the delete query strings in the list for delete_query_string in delete_query_strings: # Execute the query string executionID = client.start_query_execution( QueryString=delete_query_string, ResultConfiguration={ 'OutputLocation': athena_output, 'EncryptionConfiguration': { 'EncryptionOption': 'SSE_S3', } } ) # Get the state of the delete execution response = client.get_query_execution( QueryExecutionId=executionID['QueryExecutionId'] )['QueryExecution']['Status']['State'] # A busy wait to make sure its finished before moving on # Tables must not exist before creation # If the function runs for a long time ($) you should implement step functions or a cost effective wait # This is a low \"cost of complexity\" solution while 'RUNNING' in response: # Busy wait to make sure it finishes time.sleep(1) # Get the current state of the query response = client.get_query_execution( QueryExecutionId=executionID['QueryExecutionId'] )['QueryExecution']['Status']['State'] # Run the Athena queries to create the table & populate the S3 data def run_create_athena_queries(): # Get Athena client/object client = boto3.client('athena') # Go through each of the create query strings in the list for create_query_string in create_query_strings: # Execute the query string executionID = client.start_query_execution( QueryString=create_query_string, ResultConfiguration={ 'OutputLocation': athena_output, 'EncryptionConfiguration': { 'EncryptionOption': 'SSE_S3', } } )","title":"Sub Account Split"},{"location":"Cost/Cost_and_Usage_Analysis/300_Splitting_Sharing_CUR_Access/Code/crawler-cfn.html","text":"Below is a sample crawler config file. It is suggested you modify your existing file, modifications are between '***' characters. Variables that need to be changed in the new code below: (region): The region that contains the Lambda function (accountID): The account that contains the Lambda function AWSTemplateFormatVersion: 2010-09-09 Resources: AWSCURDatabase: Type: 'AWS::Glue::Database' Properties: DatabaseInput: Name: '(Database Name)' CatalogId: !Ref AWS::AccountId AWSCURCrawlerComponentFunction: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - glue.amazonaws.com Action: - 'sts:AssumeRole' Path: / ManagedPolicyArns: - 'arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole' Policies: - PolicyName: AWSCURCrawlerComponentFunction PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' Resource: 'arn:aws:logs:*:*:*' - Effect: Allow Action: - 'glue:UpdateDatabase' - 'glue:UpdatePartition' - 'glue:CreateTable' - 'glue:UpdateTable' - 'glue:ImportCatalogToGlue' Resource: '*' - Effect: Allow Action: - 's3:GetObject' - 's3:PutObject' Resource: arn:aws:s3:::<bucketname>/<prefix>/<folder>/WorkshopCUR* AWSCURCrawlerLambdaExecutor: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - 'sts:AssumeRole' Path: / Policies: - PolicyName: AWSCURCrawlerLambdaExecutor PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' ***** - 'lambda:InvokeFunction' ***** Resource: ***** - 'arn:aws:logs:*:*:*' - 'arn:aws:lambda:<region>:<accountID>:function:SubAcctSplit' ***** - Effect: Allow Action: - 'glue:StartCrawler' Resource: '*' AWSCURCrawler: Type: 'AWS::Glue::Crawler' DependsOn: - AWSCURDatabase - AWSCURCrawlerComponentFunction Properties: Name: AWSCURCrawler-WorkshopCUR Description: A recurring crawler that keeps your CUR table in Athena up-to-date. Role: !GetAtt AWSCURCrawlerComponentFunction.Arn DatabaseName: !Ref AWSCURDatabase Targets: S3Targets: - Path: 's3://<bucket>/<prefix>/<folder>/WorkshopCUR' Exclusions: - '**.json' - '**.yml' - '**.sql' - '**.csv' - '**.gz' - '**.zip' SchemaChangePolicy: UpdateBehavior: UPDATE_IN_DATABASE DeleteBehavior: DELETE_FROM_DATABASE AWSCURInitializer: Type: 'AWS::Lambda::Function' DependsOn: AWSCURCrawler Properties: Code: ZipFile: > const AWS = require('aws-sdk'); const response = require('cfn-response'); exports.handler = function(event, context, callback) { if (event.RequestType === 'Delete') { response.send(event, context, response.SUCCESS); } else { const glue = new AWS.Glue(); glue.startCrawler({ Name: 'AWSCURCrawler-WorkshopCUR' }, function(err, data) { if (err) { const responseData = JSON.parse(this.httpResponse.body); if (responseData['__type'] == 'CrawlerRunningException') { callback(null, responseData.Message); } else { const responseString = JSON.stringify(responseData); if (event.ResponseURL) { response.send(event, context, response.FAILED,{ msg: responseString }); } else { callback(responseString); } } } else { if (event.ResponseURL) { response.send(event, context, response.SUCCESS); } else { callback(null, response.SUCCESS); } } }); ***** var lambda = new AWS.Lambda(); var params = { FunctionName: 'SubAcctSplit' }; lambda.invoke(params, function(err, data) { if (err) console.log(err, err.stack); // an error occurred else console.log(data); // successful response }); ***** } }; Handler: 'index.handler' Timeout: 30 Runtime: nodejs8.10 ReservedConcurrentExecutions: 1 Role: !GetAtt AWSCURCrawlerLambdaExecutor.Arn AWSStartCURCrawler: Type: 'Custom::AWSStartCURCrawler' Properties: ServiceToken: !GetAtt AWSCURInitializer.Arn AWSS3CUREventLambdaPermission: Type: AWS::Lambda::Permission Properties: Action: 'lambda:InvokeFunction' FunctionName: !GetAtt AWSCURInitializer.Arn Principal: 's3.amazonaws.com' SourceAccount: !Ref AWS::AccountId SourceArn: 'arn:aws:s3:::<bucket>' AWSS3CURLambdaExecutor: Type: 'AWS::IAM::Role' Properties: AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: Service: - lambda.amazonaws.com Action: - 'sts:AssumeRole' Path: / Policies: - PolicyName: AWSS3CURLambdaExecutor PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'logs:CreateLogGroup' - 'logs:CreateLogStream' - 'logs:PutLogEvents' Resource: 'arn:aws:logs:*:*:*' - Effect: Allow Action: - 's3:PutBucketNotification' Resource: 'arn:aws:s3:::<bucket>' AWSS3CURNotification: Type: 'AWS::Lambda::Function' DependsOn: - AWSCURInitializer - AWSS3CUREventLambdaPermission - AWSS3CURLambdaExecutor Properties: Code: ZipFile: > const AWS = require('aws-sdk'); const response = require('cfn-response'); exports.handler = function(event, context, callback) { const s3 = new AWS.S3(); const putConfigRequest = function(notificationConfiguration) { return new Promise(function(resolve, reject) { s3.putBucketNotificationConfiguration({ Bucket: event.ResourceProperties.BucketName, NotificationConfiguration: notificationConfiguration }, function(err, data) { if (err) reject({ msg: this.httpResponse.body.toString(), error: err, data: data }); else resolve(data); }); }); }; const newNotificationConfig = {}; if (event.RequestType !== 'Delete') { newNotificationConfig.LambdaFunctionConfigurations = [{ Events: [ 's3:ObjectCreated:*' ], LambdaFunctionArn: event.ResourceProperties.TargetLambdaArn || 'missing arn', Filter: { Key: { FilterRules: [ { Name: 'prefix', Value: event.ResourceProperties.ReportKey } ] } } }]; } putConfigRequest(newNotificationConfig).then(function(result) { response.send(event, context, response.SUCCESS, result); callback(null, result); }).catch(function(error) { response.send(event, context, response.FAILED, error); console.log(error); callback(error); }); }; Handler: 'index.handler' Timeout: 30 Runtime: nodejs8.10 ReservedConcurrentExecutions: 1 Role: !GetAtt AWSS3CURLambdaExecutor.Arn AWSPutS3CURNotification: Type: 'Custom::AWSPutS3CURNotification' Properties: ServiceToken: !GetAtt AWSS3CURNotification.Arn TargetLambdaArn: !GetAtt AWSCURInitializer.Arn BucketName: '<bucket>' ReportKey: '<prefix>/<folder>/WorkshopCUR' AWSCURReportStatusTable: Type: 'AWS::Glue::Table' DependsOn: AWSCURDatabase Properties: DatabaseName: athenacurcfn_workshop_c_u_r CatalogId: !Ref AWS::AccountId TableInput: Name: 'cost_and_usage_data_status' TableType: 'EXTERNAL_TABLE' StorageDescriptor: Columns: - Name: status Type: 'string' InputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OutputFormat: 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' SerdeInfo: SerializationLibrary: 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' Location: 's3://<bucket>/<prefix>/<folder>/cost_and_usage_data_status/'","title":"Crawler cfn"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/README.html","text":"Level 300: Automated Athena CUR Query and E-mail Delivery https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through deploying an automatic CUR query & E-mail delivery solution using Athena, Lambda, SES and CloudWatch. The Lambda function is triggered by a CloudWatch event, it then runs saved queries in Athena against your CUR file. The queries are grouped into a single report file (xlsx format), and sends report via SES. This solution provides automated reporting to your organization, to both consumers of cloud and financial teams. Goals Provide automated financial reports across your organization Prerequisites CUR is enabled and delivered into S3, with Athena integration. Recommend to complete 200_4_Cost_and_Usage_Analysis If your account is in the SES sandbox(default), verify your email addresses in SES to assure you can send or receive emails via verified mail addresses: https://docs.aws.amazon.com/ses/latest/DeveloperGuide/verify-email-addresses.html Permissions required Create IAM policies and roles Write and read to/from S3 Buckets Create and modify Lambda functions Create, save and execute Athena queries Verify e-mail address, send mail in SES Costs Variable, dependent on the amount of data scanned and report frequency Approximately <$5 a month for small to medium accounts Time to complete The lab should take approximately 15 minutes to complete License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/README.html#level-300-automated-athena-cur-query-and-e-mail-delivery","text":"https://wellarchitectedlabs.com","title":"Level 300: Automated Athena CUR Query and E-mail Delivery"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/README.html#introduction","text":"This hands-on lab will guide you through deploying an automatic CUR query & E-mail delivery solution using Athena, Lambda, SES and CloudWatch. The Lambda function is triggered by a CloudWatch event, it then runs saved queries in Athena against your CUR file. The queries are grouped into a single report file (xlsx format), and sends report via SES. This solution provides automated reporting to your organization, to both consumers of cloud and financial teams.","title":"Introduction"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/README.html#goals","text":"Provide automated financial reports across your organization","title":"Goals"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/README.html#prerequisites","text":"CUR is enabled and delivered into S3, with Athena integration. Recommend to complete 200_4_Cost_and_Usage_Analysis If your account is in the SES sandbox(default), verify your email addresses in SES to assure you can send or receive emails via verified mail addresses: https://docs.aws.amazon.com/ses/latest/DeveloperGuide/verify-email-addresses.html","title":"Prerequisites"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/README.html#permissions-required","text":"Create IAM policies and roles Write and read to/from S3 Buckets Create and modify Lambda functions Create, save and execute Athena queries Verify e-mail address, send mail in SES","title":"Permissions required"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/README.html#costs","text":"Variable, dependent on the amount of data scanned and report frequency Approximately <$5 a month for small to medium accounts","title":"Costs"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/README.html#time-to-complete","text":"The lab should take approximately 15 minutes to complete","title":"Time to complete"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/Lab_Guide.html","text":"Level 300: Automated Athena CUR Query and E-mail Delivery Authors Na Zhang, Sr. Technical Account Manager, AWS Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com Table of Contents Overview architecture Create S3 Bucket Create an IAM policy and role for Lambda function Configure parameters of function code and upload code to S3 Create and test a Lambda function Customize query strings and create scheduled CloudWatch event Tear down Rate this Lab 1. Overview architecture 2. Create S3 Bucket The first step is to create an S3 bucket which will hold the lambda code and also used for storage of the reports. NOTE : the bucket must be in the same region as the Lambda function, it is advised to use a single region for all resources within this lab. This bucket will store the reports and Athena CUR query results. These will not be deleted, to enable historical reporting, so delete these periodically if you do not require them. Login as an IAM user with the required permissions, go to the s3 dashboard and create an S3 bucket in the required region: 3. Create an IAM policy and role for Lambda function This step is used to create an IAM policy and a role that allows Lambda function to perform Athena CUR query and deliver processed CUR report via SES. Log into IAM console , click on Policies and click on Create Policy : Click on the JSON tab, modify the following policy, replacing the your-cur-query-results-bucket string. Make sure you add \"*\" at the end of the bucket name so the whole bucket is writable: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\" ], \"Resource\": [ \"arn:aws:s3:::your-cur-query-results-bucket*\" ] }, { \"Sid\": \"VisualEditor1\", \"Effect\": \"Allow\", \"Action\": [ \"athena:List*\", \"athena:*QueryExecution\", \"athena:Get*\", \"athena:BatchGet*\", \"glue:Get*\", \"glue:BatchGet*\", \"s3:Get*\", \"s3:List*\", \"SES:SendRawEmail\", \"SES:SendEmail\", \"logs:CreateLogStream\", \"logs:CreateLogGroup\", \"logs:PutLogEvents\" ], \"Resource\": \"*\" } ] } Copy the policy to JSON edit frame, ensure the bucket name has been changed, click Review policy : Configure the name Lambda_Auto_CUR_Delivery_Access , and click Create policy . Click on Roles , click Create Role : Choose Lambda as the service that will use this role, click Next Permissions : At Attach permissions policies page, search and choose Lambda_Auto_CUR_Delivery_Access policy created in the previous step. Click Next:Tags , click Next:Review . At Review page, configure a name Lambda_Auto_CUR_Delivery_Role , click Create role . This role will be used for lambda function execution. 4. Configure parameters of function code and upload code to S3 This step is used to edit parameters (CUR database name and table, SES sender and recipient etc) in the Lambda function code, which is then uploaded to S3 for Lambda execution. Download function code https://d3h9zoi3eqyz7s.cloudfront.net/Cost/AutoCURDelivery.zip to your local disk. This zip file includes: - auto_cur_delivery.py - Lambda function code config.yml - Configuration file package/ - All dependencies, libraries, including pandas, numpy, Xlrd, Openpyxl, Xlsxwriter, pyyaml Unzip config.yml from within AutoCURDelivery.zip , and open it into a text editor. Configure the following parameters in config.yml : CUR_Output_Location : Your S3 bucket created previously, i.e. S3://my-cur-bucket/out-put/ CUR_DB : CUR database and table name defined in Athena, i.e. athenacurcfn_my_athena_report.myathenareport CUR_Report_Name : Report filename that is sent with SES as an attachment, i.e. cost_utilization_report.xlsx Region : The region where SES service is called, i.e. us-east-1 Subject : SES mail subject, i.e. Cost and Utilization Report Sender : Your sender e-mail address, i.e. john@example.com Recipient : Your recipient e-mail addresses. If there are multiple recipients, separate them by comma, i.e. john@example.com,alice@example.com Keep other configuration unchanged and save config.yml . Add the updated config.yml back to AutoCURDelivery.zip . Upload AutoCURDelivery.zip to your S3 bucket. Make sure this S3 path is in the same region as Lambda function created in next step. NOTE this is a large 30+MB file, so it may take a little time. 5. Create a Lambda function We will now create a Lambda function which will run the code and produce the reports. NOTE : this Lambda function must be created in the same region as S3 bucket for CUR query results created earlier. Go to the Lambda console , click Create function . Select Author from scratch , configure the following parameters: Function name : Auto_CUR_Delivery Runtime : Python 3.7 Execution role : Use an existing role Existing role : Lambda_Auto_CUR_Delivery_Role click Create function . In the top right-hand corner of Lambda configuration page, click Select a test event drop-down box and choose Configure test events . Use the default event template Hello world, because this function does not need any input event parameters, set a event name AutoCURDeliveryTest , and click Create . In Function code section, configure the following: Code entry type : Upload a file from Amazon S3 Amazon S3 link URL : https://s3.amazonaws.com/bucket name/AutoCURDelivery.zip Handler : auto_cur_delivery.lambda_handler Scroll down to Basic settings section, set Memory to 512 MB, and timeout to 5 min. Keep other configurations as default, scroll to the very top and click Save . Click the Actions drop-down box and choose Publish new version . Set the Version description to v1, and click Publish . We have finished the configuration and we will now test it. Make sure AutoCURDeliveryTest event is selected, click Test . It takes a few seconds to execute Lambda function, and you'll see all logs after execution. Check your e-mail recipients, they should receive a mail for cost & utilization report with an excel file attached, similarly as below: By default, the cost & utilization report contains: Cost_By_Service - Cost in the recent three months split by service (e.g. current month is Jul, the recent three months are Jul, Jun and May, same as below) Data_Cost_By_Service - Data cost in the recent three months split by service MoM_Inter_AZ_DT(with graph) - Month over months inter-AZ data transfer usage and change in the recent three months MTD_S3_By_Bucket - Month to date S3 cost and usage type split by bucket name MTD_ELB_By_Name - Month to date ELB cost split by ELB name and region MTD_CF_By_Distribution - Month to date Cloudfront cost and usage split by distribution id Now you have completed this auto CUR delivery solution with default CUR query. In the next step we will add an additional query, and a CloudWatch scheduled event to trigger Lambda function as required. 6. Customize query strings and create scheduled CloudWatch event In you local path where AutoCURDelivery.zip is located. Unzip and re-open config.yml in a text editor. Find Body_Text , insert a description of new query MTD_Inter_AZ_DT . MTD_Inter_AZ_DT - Month to date inter-AZ data transfer split by resource ID Find the section Query_String_List , add following new query string at the bottom of file (note the indent should be same as other query strings), save config.yml . - MTD_Inter_AZ_DT: SELECT year ,month(line_item_usage_start_date) month ,line_item_product_code as Product_Name ,line_item_resource_id as Resource_Id ,line_item_usage_type as Usage_Type ,sum(line_item_usage_amount) as \"Inter_AZ_Data_Transfer(GB)\" ,sum(line_item_unblended_cost) as \"Cost($)\" FROM CUR_DB WHERE \"line_item_usage_type\" like '%Bytes%' AND \"line_item_usage_type\" like '%Regional%' AND year='CUR_YEAR' AND month='CUR_MONTH' GROUP BY 1,2,3,4,5 ORDER BY sum(\"line_item_unblended_cost\") desc The paramemters CUR_DB, CUR_MONTH, CUR_YEAR are replaced when function is running Add config.yml back into AutoCURDelivery.zip , and upload zip file to S3. Goto Lambda console, update function code path to above S3 path where new zip file is located, click Save . Perform another Test of the function. Check the cost & utilization report in the mail your recipient receives, there should be one more tab added in the excel file for month to date inter-az data transfer cost. We will now create a scheduled Cloudwatch event to trigger Lambda function periodically Go to the Cloudwatch dashboard , under Events click Rules Click Create rule . In Event Source , choose Schedule , use default fixed rate of 5 minutes . In Targets click Add Target and choose Lambda function in the drop-down box. Choose the function Auto_CUR_Delivery , click Configure details Configure a name 5_min_auto_cur_delivery , click Create rule . Wait for 5 minutes, your recipients should receive a cost & utilization report mail, and continually receive report mail every other 5 minutes. To stop event triggering, choose the rule 5_min_auto_cur_delivery , click Actions and select Disable . Now you have completed this lab to query CUR with customized query strings from Athena and send it via SES periodically. To explore more, you can define your own query strings in config.yml and configure CloudWatch event rule to the rate as required. 6. Tear down Delete IAM role Lambda_Auto_CUR_Delivery_Role and policy Lambda_Auto_CUR_Delivery_Access Delete Lambda function Auto_CUR_Delivery Delete CloudWatch event 5_min_auto_cur_delivery Delete SES configuration Delete S3 bucket for CUR query results storing 7. Rate this lab","title":"Lab Guide"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/Lab_Guide.html#level-300-automated-athena-cur-query-and-e-mail-delivery","text":"","title":"Level 300: Automated Athena CUR Query and E-mail Delivery"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/Lab_Guide.html#authors","text":"Na Zhang, Sr. Technical Account Manager, AWS","title":"Authors"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/Lab_Guide.html#feedback","text":"If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com","title":"Feedback"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/Lab_Guide.html#table-of-contents","text":"Overview architecture Create S3 Bucket Create an IAM policy and role for Lambda function Configure parameters of function code and upload code to S3 Create and test a Lambda function Customize query strings and create scheduled CloudWatch event Tear down Rate this Lab","title":"Table of Contents"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/Lab_Guide.html#1-overview-architecture","text":"","title":"1. Overview architecture"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/Lab_Guide.html#2-create-s3-bucket","text":"The first step is to create an S3 bucket which will hold the lambda code and also used for storage of the reports. NOTE : the bucket must be in the same region as the Lambda function, it is advised to use a single region for all resources within this lab. This bucket will store the reports and Athena CUR query results. These will not be deleted, to enable historical reporting, so delete these periodically if you do not require them. Login as an IAM user with the required permissions, go to the s3 dashboard and create an S3 bucket in the required region:","title":"2. Create S3 Bucket"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/Lab_Guide.html#3-create-an-iam-policy-and-role-for-lambda-function","text":"This step is used to create an IAM policy and a role that allows Lambda function to perform Athena CUR query and deliver processed CUR report via SES. Log into IAM console , click on Policies and click on Create Policy : Click on the JSON tab, modify the following policy, replacing the your-cur-query-results-bucket string. Make sure you add \"*\" at the end of the bucket name so the whole bucket is writable: { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"VisualEditor0\", \"Effect\": \"Allow\", \"Action\": [ \"s3:PutObject\" ], \"Resource\": [ \"arn:aws:s3:::your-cur-query-results-bucket*\" ] }, { \"Sid\": \"VisualEditor1\", \"Effect\": \"Allow\", \"Action\": [ \"athena:List*\", \"athena:*QueryExecution\", \"athena:Get*\", \"athena:BatchGet*\", \"glue:Get*\", \"glue:BatchGet*\", \"s3:Get*\", \"s3:List*\", \"SES:SendRawEmail\", \"SES:SendEmail\", \"logs:CreateLogStream\", \"logs:CreateLogGroup\", \"logs:PutLogEvents\" ], \"Resource\": \"*\" } ] } Copy the policy to JSON edit frame, ensure the bucket name has been changed, click Review policy : Configure the name Lambda_Auto_CUR_Delivery_Access , and click Create policy . Click on Roles , click Create Role : Choose Lambda as the service that will use this role, click Next Permissions : At Attach permissions policies page, search and choose Lambda_Auto_CUR_Delivery_Access policy created in the previous step. Click Next:Tags , click Next:Review . At Review page, configure a name Lambda_Auto_CUR_Delivery_Role , click Create role . This role will be used for lambda function execution.","title":"3. Create an IAM policy and role for Lambda function"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/Lab_Guide.html#4-configure-parameters-of-function-code-and-upload-code-to-s3","text":"This step is used to edit parameters (CUR database name and table, SES sender and recipient etc) in the Lambda function code, which is then uploaded to S3 for Lambda execution. Download function code https://d3h9zoi3eqyz7s.cloudfront.net/Cost/AutoCURDelivery.zip to your local disk. This zip file includes: - auto_cur_delivery.py - Lambda function code config.yml - Configuration file package/ - All dependencies, libraries, including pandas, numpy, Xlrd, Openpyxl, Xlsxwriter, pyyaml Unzip config.yml from within AutoCURDelivery.zip , and open it into a text editor. Configure the following parameters in config.yml : CUR_Output_Location : Your S3 bucket created previously, i.e. S3://my-cur-bucket/out-put/ CUR_DB : CUR database and table name defined in Athena, i.e. athenacurcfn_my_athena_report.myathenareport CUR_Report_Name : Report filename that is sent with SES as an attachment, i.e. cost_utilization_report.xlsx Region : The region where SES service is called, i.e. us-east-1 Subject : SES mail subject, i.e. Cost and Utilization Report Sender : Your sender e-mail address, i.e. john@example.com Recipient : Your recipient e-mail addresses. If there are multiple recipients, separate them by comma, i.e. john@example.com,alice@example.com Keep other configuration unchanged and save config.yml . Add the updated config.yml back to AutoCURDelivery.zip . Upload AutoCURDelivery.zip to your S3 bucket. Make sure this S3 path is in the same region as Lambda function created in next step. NOTE this is a large 30+MB file, so it may take a little time.","title":"4. Configure parameters of function code and upload code to S3"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/Lab_Guide.html#5-create-a-lambda-function","text":"We will now create a Lambda function which will run the code and produce the reports. NOTE : this Lambda function must be created in the same region as S3 bucket for CUR query results created earlier. Go to the Lambda console , click Create function . Select Author from scratch , configure the following parameters: Function name : Auto_CUR_Delivery Runtime : Python 3.7 Execution role : Use an existing role Existing role : Lambda_Auto_CUR_Delivery_Role click Create function . In the top right-hand corner of Lambda configuration page, click Select a test event drop-down box and choose Configure test events . Use the default event template Hello world, because this function does not need any input event parameters, set a event name AutoCURDeliveryTest , and click Create . In Function code section, configure the following: Code entry type : Upload a file from Amazon S3 Amazon S3 link URL : https://s3.amazonaws.com/bucket name/AutoCURDelivery.zip Handler : auto_cur_delivery.lambda_handler Scroll down to Basic settings section, set Memory to 512 MB, and timeout to 5 min. Keep other configurations as default, scroll to the very top and click Save . Click the Actions drop-down box and choose Publish new version . Set the Version description to v1, and click Publish . We have finished the configuration and we will now test it. Make sure AutoCURDeliveryTest event is selected, click Test . It takes a few seconds to execute Lambda function, and you'll see all logs after execution. Check your e-mail recipients, they should receive a mail for cost & utilization report with an excel file attached, similarly as below: By default, the cost & utilization report contains: Cost_By_Service - Cost in the recent three months split by service (e.g. current month is Jul, the recent three months are Jul, Jun and May, same as below) Data_Cost_By_Service - Data cost in the recent three months split by service MoM_Inter_AZ_DT(with graph) - Month over months inter-AZ data transfer usage and change in the recent three months MTD_S3_By_Bucket - Month to date S3 cost and usage type split by bucket name MTD_ELB_By_Name - Month to date ELB cost split by ELB name and region MTD_CF_By_Distribution - Month to date Cloudfront cost and usage split by distribution id Now you have completed this auto CUR delivery solution with default CUR query. In the next step we will add an additional query, and a CloudWatch scheduled event to trigger Lambda function as required.","title":"5. Create a Lambda function"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/Lab_Guide.html#6-customize-query-strings-and-create-scheduled-cloudwatch-event","text":"In you local path where AutoCURDelivery.zip is located. Unzip and re-open config.yml in a text editor. Find Body_Text , insert a description of new query MTD_Inter_AZ_DT . MTD_Inter_AZ_DT - Month to date inter-AZ data transfer split by resource ID Find the section Query_String_List , add following new query string at the bottom of file (note the indent should be same as other query strings), save config.yml . - MTD_Inter_AZ_DT: SELECT year ,month(line_item_usage_start_date) month ,line_item_product_code as Product_Name ,line_item_resource_id as Resource_Id ,line_item_usage_type as Usage_Type ,sum(line_item_usage_amount) as \"Inter_AZ_Data_Transfer(GB)\" ,sum(line_item_unblended_cost) as \"Cost($)\" FROM CUR_DB WHERE \"line_item_usage_type\" like '%Bytes%' AND \"line_item_usage_type\" like '%Regional%' AND year='CUR_YEAR' AND month='CUR_MONTH' GROUP BY 1,2,3,4,5 ORDER BY sum(\"line_item_unblended_cost\") desc The paramemters CUR_DB, CUR_MONTH, CUR_YEAR are replaced when function is running Add config.yml back into AutoCURDelivery.zip , and upload zip file to S3. Goto Lambda console, update function code path to above S3 path where new zip file is located, click Save . Perform another Test of the function. Check the cost & utilization report in the mail your recipient receives, there should be one more tab added in the excel file for month to date inter-az data transfer cost. We will now create a scheduled Cloudwatch event to trigger Lambda function periodically Go to the Cloudwatch dashboard , under Events click Rules Click Create rule . In Event Source , choose Schedule , use default fixed rate of 5 minutes . In Targets click Add Target and choose Lambda function in the drop-down box. Choose the function Auto_CUR_Delivery , click Configure details Configure a name 5_min_auto_cur_delivery , click Create rule . Wait for 5 minutes, your recipients should receive a cost & utilization report mail, and continually receive report mail every other 5 minutes. To stop event triggering, choose the rule 5_min_auto_cur_delivery , click Actions and select Disable . Now you have completed this lab to query CUR with customized query strings from Athena and send it via SES periodically. To explore more, you can define your own query strings in config.yml and configure CloudWatch event rule to the rate as required.","title":"6. Customize query strings and create scheduled CloudWatch event"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/Lab_Guide.html#6-tear-down","text":"Delete IAM role Lambda_Auto_CUR_Delivery_Role and policy Lambda_Auto_CUR_Delivery_Access Delete Lambda function Auto_CUR_Delivery Delete CloudWatch event 5_min_auto_cur_delivery Delete SES configuration Delete S3 bucket for CUR query results storing","title":"6. Tear down"},{"location":"Cost/Expenditure_Awareness/300_Automated_CUR_Query_and_Email_Delivery/Lab_Guide.html#7-rate-this-lab","text":"","title":"7. Rate this lab"},{"location":"Operations/README.html","text":"AWS Well-Architected Operational Excellence Labs https://wellarchitectedlabs.com Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. For more information about Operational Excellence on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected Operational Excellence whitepaper. Labs Operations Fundamentals Level 100: Introduction to Inventory and Patch Management License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Operations/README.html#aws-well-architected-operational-excellence-labs","text":"https://wellarchitectedlabs.com","title":"AWS Well-Architected Operational Excellence Labs"},{"location":"Operations/README.html#introduction","text":"This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. For more information about Operational Excellence on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected Operational Excellence whitepaper.","title":"Introduction"},{"location":"Operations/README.html#labs","text":"","title":"Labs"},{"location":"Operations/README.html#operations-fundamentals","text":"Level 100: Introduction to Inventory and Patch Management","title":"Operations Fundamentals"},{"location":"Operations/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html","text":"Level 100: Inventory and Patch Management https://wellarchitectedlabs.com Introduction In this lab you will apply the concepts of Infrastructure as Code and Operations as Code to the following activities: Deployment of Infrastructure Inventory Management Patch Management Goals: Automated deployment of infrastructure Dynamic management of resources Automated patch management Prerequisites: An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM. Important You will be billed for any applicable AWS resources used in this lab that are not covered in the AWS Free Tier . At the end of the lab guide there is an additional section on how to remove all the resources you have created. Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#level-100-inventory-and-patch-management","text":"https://wellarchitectedlabs.com","title":"Level 100: Inventory and Patch Management"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#introduction","text":"In this lab you will apply the concepts of Infrastructure as Code and Operations as Code to the following activities: Deployment of Infrastructure Inventory Management Patch Management","title":"Introduction"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#goals","text":"Automated deployment of infrastructure Dynamic management of resources Automated patch management","title":"Goals:"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM. Important You will be billed for any applicable AWS resources used in this lab that are not covered in the AWS Free Tier . At the end of the lab guide there is an additional section on how to remove all the resources you have created.","title":"Prerequisites:"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html","text":"Level 100: Inventory and Patch Management: Lab Guide In the cloud, you can apply the same engineering discipline that you use for application code to your entire environment. You can define your entire workload (applications, infrastructure, etc.) as code and update it with code. You can script your operations procedures and automate their execution by triggering them in response to events. By performing operations as code, you limit human error and enable consistent execution of operations activities. In this lab you will apply the concepts of Infrastructure as Code and Operations as Code to the following activities: * Deployment of Infrastructure * Inventory Management * Patch Management Included in the lab guide are bonus sections that can be completed if you have time or later if interested. * Creating Maintenance Windows and Scheduling Automated Operations Activities * Create and Subscribe to a Simple Notification Service Topic Important You will be billed for any applicable AWS resources used in this lab that are not covered in the AWS Free Tier . At the end of the lab guide there is an additional section on how to remove all the resources you have created. * Removing Lab Resources 1. Setup Requirements You will need the following to be able to perform this lab: * Your own device for console access * An AWS account that you are able to use for testing, that is not used for production or other purposes * An available region within your account with capacity to add 2 additional VPCs User and Group Management When you create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user. It is accessed by signing in with the email address and password that you used to create the account. We strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user. Securely store the root user credentials and use them to perform only a few account and service management tasks. To view the tasks that require you to sign in as the root user, see AWS Tasks That Require Root User . IAM Users & Groups As a best practice, do not use the AWS account root user for any task where it's not required. Instead, create a new IAM user for each person that requires administrator access. Then grant administrator access by placing the users into an \"Administrators\" group to which the AdministratorAccess managed policy is attached. Use administrators group members to manage permissions and policy for the AWS account. Limit use of the root user to only those actions that require it . 1.1 Create Administrator IAM User and Group To create an administrator user for yourself and add the user to an administrators group: Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ . In the IAM navigation pane, choose Users and then choose Add user . In Set user details for User name , type a user name for the administrator account you are creating. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 64 characters in length. In Select AWS access type for Access type , select the check box next to AWS Management Console access , select Custom password , and then type your new password in the text box. If you're creating the user for someone other than yourself, you can leave Require password reset selected to force the user to create a new password when first signing in. Clear the box next to Require password reset and then choose Next: Permissions . In set permissions for user ensure Add user to group is selected. Under Add user to group choose Create group . In the Create group dialog box, type a Group name for the new group, such as Administrators. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 128 characters in length. In the policy list, select the check box next to AdministratorAccess and then choose Create group . Back at Add user to group , in the list of groups, ensure the check box for your new group is selected. Choose Refresh if necessary to see the group in the list. choose Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, choose Create user . At the confirmation screen you do not need to download the user credentials for programmatic access at this time. You can create new credentials at any time. You can use this same process to create more groups and users and to give your users access to your AWS account resources. To learn about using policies that restrict user permissions to specific AWS resources, see Access Management and Example Policies . To add additional users to the group after it's created, see Adding and Removing Users in an IAM Group . 1.2 Log in to the AWS Management Console using your administrator account You can now use this administrator user instead of your root user for this AWS account. Choose the link https\\://\\<yourAccountNumber>.signin.aws.amazon.com/console and log in with your administrator user credentials. Select the region you will use for the lab from the the list in the upper right corner. Verify that you have 2 available VPCs (3 or less in use) in the selected region by navigating to the VPC Console (https://console.aws.amazon.com/vpc/) and in the Resources section reviewing the number of VPCs. 1.3 Create an EC2 Key Pair Amazon EC2 uses public-key cryptography to encrypt and decrypt login information. Public-key cryptography uses a public key to encrypt a piece of data, such as a password, then the recipient uses the private key to decrypt the data. The public and private keys are known as a key pair. To log in to the Amazon Linux instances we will create in this lab, you must create a key pair, specify the name of the key pair when you launch the instance, and provide the private key when you connect to the instance. Use your administrator account to access the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . In the IAM navigation pane under Network & Security , choose Key Pairs and then choose Create Key Pair . In the Create Key Pair dialog box, type a Key pair name such as OELabIPM and then choose Create . Save the keyPairName.pem file for optional later use accessing the EC2 instances created in this lab. 2. Deploy an Environment Using Infrastructure as Code Tagging We will make extensive use of tagging throughout the lab. The CloudFormation template for the lab includes the definition of multiple tags against a variety of resources. AWS enables you to assign metadata to your AWS resources in the form of tags . Each tag is a simple label consisting of a customer-defined key and an optional value that can make it easier to manage, search for, and filter resources. Although there are no inherent types of tags, commonly adopted categories of tags include technical tags (e.g., Environment, Workload, InstanceRole, and Name), tags for automation (e.g., Patch Group, and SSMManaged), business tags (e.g., Owner), and security tags (e.g., Confidentiality). Apply the following best practices when using tags: * Use a standardized, case-sensitive format for tags, and implement it consistently across all resource types * Consider tag dimensions that support the following: * Managing resource access control with IAM * Cost tracking * Automation * AWS console organization * Implement automated tools to help manage resource tags. The Resource Groups Tagging API enables programmatic control of tags, making it easier to automatically manage, search, and filter tags and resources. * Err on the side of using too many tags rather than too few tags. * Develop a tagging strategy . Note It is easy to modify tags to accommodate changing business requirements; however, consider the consequences of future changes, especially in relation to tag-based access control, automation, or upstream billing reports. Important Patch Group is a reserved tag key used by Systems Manager Patch Manager that is case sensitive with a space between the two words. Management Tools: CloudFormation AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications. You create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances) and AWS CloudFormation provisions and configures those resources for you. AWS CloudFormation enables you to use a template file to create and delete a collection of resources as a single unit (a stack). There is no additional charge for AWS CloudFormation . You pay for AWS resources (such as Amazon EC2 instances, Elastic Load Balancing load balancers, etc.) created using AWS CloudFormation in the same manner as if you created the resources manually. You only pay for what you use as you use it. There are no minimum fees and no required upfront commitments. 2.1 Deploy the Lab Infrastructure To deploy the lab infrastructure: Download the CloudFormation script for this lab from /Code . Use your administrator account to access the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Choose Create Stack . On the Select Template page, select Specify an Amazon S3 template URL and enter the URL for the downloaded location. AWS CloudFormation Designer AWS CloudFormation Designer is a graphic tool for creating, viewing, and modifying AWS CloudFormation templates. With Designer you can diagram your template resources using a drag-and-drop interface. You can edit their details using the integrated JSON and YAML editor. AWS CloudFormation Designer can help you see the relationship between template resources. On the Select Template page, next to Specify an Amazon S3 template URL , choose the link to View/Edit template in Designer . Briefly review the graphical representation of the environment we are about to create, including the template in the JSON and YAML formats. You can use this feature to convert between JSON and YAML formats. Choose the Create Stack icon (a cloud with an arrow) to return to the Select Template page . On the Select Template page, choose Next . A CloudFormation template is a JSON or YAML formatted text file that describes your AWS infrastructure containing both optional and required sections . In the next steps, we will provide a name for our stack and parameters that will be passed into the template to help define the resources that will be implemented. In the Specify Details section, define a Stack name , such as OELabStack1 . In the Parameters section: Leave InstanceProfile blank as we have not yet defined an instance profile. Leave InstanceTypeApp and InstanceTypeWeb as the default free-tier-eligible t2.micro value. Select the EC2 KeyName you defined earlier from the list. In a browser window, go to https://checkip.amazonaws.com/ to get your IP. Enter your IP address in SSHLocation in CIDR notation (i.e., ending in /32). Define the Workload Name as Test . Choose Next . On the Options page under Tags , define a Key of Owner , with Value set to the username you choose for your administrator. You may define additional keys as needed. The CloudFormation template creates all the example tags given in the discussion on tagging above. Leave all other sections unmodified. Scroll to the bottom of the page and choose Next . On the Review page, review your choices and then choose Create . On the CloudFormation console page Check the box next to your Stack Name to see its details. If your Stack Name is not displayed, click the refresh button (circular arrow) in the top right until it appears. If the details are not displayed, choose the refresh button until details appear. Choose the Events tab for your selected workload to see the activity log from the creation of your CloudFormation stack. When the Status of your stack displays CREATE_COMPLETE in the filter list, you have just created a representation of a typical lift and shift 2-tier application migrated to the cloud. Navigate to the EC2 console to view the deployed systems: Choose Instances . Select a server and review the details under its Description and Tag tabs. (Optional) choose Security Groups and select the Security Group whose name begins with the name of your stack. Examine the inbound rules. (Optional) navigate to the VPC console and examine the configuration of the VPC you just created. The impact of Infrastructure as Code With infrastructure as code, if you can deploy one environment, you can deploy any number of copies of that environment. In this example we have created a Test environment. Later, we will repeat these steps to deploy a Prod environment. The ability to dynamically deploy temporary environments on-demand enables parallel experimentation, development, and testing efforts. It allows duplication of environments to recreate and analyze errors, as well as cut-over deployment of production systems using blue-green methodologies. These practices contribute to reduced risk, increased operations effectiveness, and efficiency. 3. Inventory Management using Operations as Code Management Tools: Systems Manager AWS Systems Manager is a collection of features that enable IT Operations that we will explore throughout this lab. There are set up tasks and pre-requisites that must be satisfied prior to using Systems Manager to manage your EC2 instances or on-premises systems in hybrid environments . * You must use a supported operating system * Supported operating systems include versions of Windows, Amazon Linux, Ubuntu Server, RHEL, and CentOS * The SSM Agent must be installed * The SSM Agent for Windows also requires PowerShell 3.0 or later to run some SSM documents * Your EC2 instances must have outbound internet access * You must access Systems Manager in a supported region * Systems Manager requires IAM roles * for instances that will process commands * for users executing commands SSM Agent is installed by default on: * Amazon Linux base AMIs dated 2017.09 and later * Windows Server 2016 instances * Instances created from Windows Server 2003-2012 R2 AMIs published in November 2016 or later There is no additional charge for AWS Systems Manager . You only pay for your underlying AWS resources managed or created by AWS Systems Manager (e.g., Amazon EC2 instances or Amazon CloudWatch metrics). You only pay for what you use as you use it. There are no minimum fees and no upfront commitments. 3.1 Setting up Systems Manager Use your administrator account to access the Systems Manager console at https://console.aws.amazon.com/systems-manager/ . Choose Managed Instances from the navigation bar. If you have not satisfied the pre-requisites for Systems Manager, you will arrive at the AWS Systems Manager Managed Instances page. As a user with AdministratorAccess permissions, you already have User Access to Systems Manager . The Amazon Linux AMIs used to create the instances in your environment are dated 2017.09. They are supported operating systems and have the SSM Agent installed by default. If you are in a supported region the remaining step is to configure the IAM role for instances that will process commands. Create an Instance Profile for Systems Manager managed instances: Navigate to the IAM console In the navigation pane, choose Roles . Then choose Create role . In the Select type of trusted entity section, verify that the default AWS service is selected. In the Choose the service that will use this role section, scroll past the first reference to EC2 ( EC2 Allows EC2 instances to call AWS services on your behalf ) and choose EC2 from within the field of services. This will open the Select your use case section further down the page. In the Select your use case section, choose EC2 Role for Simple Systems Manager to select it. Then choose Next: Permissions . Under Attached permissions policy , verify that AmazonEC2RoleforSSM is listed, and then choose Next: Review . In the Review section: Enter a Role name , such as ManagedInstancesRole . Accept the default in the Role description . Choose Create role . Apply this role to the instances you wish to manage with Systems Manager: Navigate to the EC2 Console and choose Instances . Select the first instance and then choose Actions , Instance Settings , and Attach/Replace IAM Role . Under Attach/Replace IAM Role , select ManagedInstancesRole from the drop down list and choose Apply . After you receive confirmation of success, choose Close . Repeat this process, assigning ManagedInstancesRole to each of the 3 remaining instances. Return to the Systems Manager console and choose Managed Instances from the navigation bar. Periodically choose Managed Instances until your instances begin to appear in the list. Over the next couple of minutes your instances will populate into the list as managed instances. Note If desired, you can use a more restrictive permission set to grant access to Systems Manager. 3.2 Create a Second CloudFormation Stack Create a second CloudFormation stack using the procedure in 2.1 with the following changes: In the Specify Details section, define a Stack name, such as OELabStack2 . Specify the InstanceProfile using the ManagedInstancesRole you defined. Define the Workload Name as Prod . Systems Manager: Inventory You can use AWS Systems Manager Inventory to collect operating system (OS), application, and instance metadata from your Amazon EC2 instances and your on-premises servers or virtual machines (VMs) in your hybrid environment. You can query the metadata to quickly understand which instances are running the software and configurations required by your software policy, and which instances need to be updated. 3.3 Using Systems Manager Inventory to Track Your Instances Under Instances & Nodes in the AWS Systems Manager navigation bar, choose Inventory . Scroll down in the window to the Corresponding managed instances section. Inventory currently contains only the instance data available from the EC2 Choose the InstanceID of one of your systems. Examine each of the available tabs of data under the Instance ID heading. Inventory collection must be specifically configured and the data types to be collected must be specified Choose Inventory in the navigation bar. Choose Setup Inventory in the top right corner of the window In the Setup Inventory screen, define targets for inventory: Under Specify targets by , select Specifying a tag Under Tags specify Environment for the key and OELabIPM for the value Note You can select all managed instances in this account, ensuring that all managed instances will be inventoried. You can constrain inventoried instances to those with specific tags, such as Environment or Workload. Or you can manually select specific instances for inventory. Schedule the frequency with which inventory is collected. The default and minimum period is 30 minutes For Collect inventory data every , accept the default 30 Minute(s) Under parameters, specify what information to collect with the inventory process Review the options and select the defaults (Optional) If desired, you may specify an S3 bucket to receive the inventory execution logs (you will need to create a destination bucket for the logs prior to proceeding): Check the box next to Sync inventory execution logs to an S3 bucket under the Advanced options. Provide an S3 bucket name. (Optional) Provide an S3 bucket prefix. Choose Setup Inventory at the bottom of the page (it can take up to 10 minutes to deploy a new inventory policy to an instance). To create a new inventory policy, from Inventory , choose Setup inventory . To edit an existing policy, from State Manager in the left navigation menu, select the association and choose Edit . Note You can create multiple Inventory specifications. They will each be stored as associations within Systems Manager State Manager . Systems Manager: State Manager In State Manager, an association is the result of binding configuration information that defines the state you want your instances to be in to the instances themselves. This information specifies when and how you want instance-related operations to run that ensure your Amazon EC2 and hybrid infrastructure is in an intended or consistent state. An association defines the state you want to apply to a set of targets. An association includes three components and one optional set of components: * A document that defines the state * Target(s) * A schedule * (Optional) Runtime parameters. When you performed the Setup Inventory actions, you created an association in State Manager. 3.4 Review Association Status Under Actions in the navigation bar, select State Manager . At this point, the Status may show that the inventory activity has not yet completed. Choose the single Association id that is the result of your Setup Inventory action. Examine each of the available tabs of data under the Association ID heading. Choose Edit . Enter a name under Name - optional to provide a more user friendly label to the association, such as InventoryAllInstances (white space is not permitted in an Association Name ). Inventory is accomplished through the following: * The activities defined in the AWS-GatherSoftwareInventory command document. * The parameters provided in the Parameters section are passed to the document at execution. * The targets are defined in the Targets section. Important In this example there is a single target, the wildcard. The wildcard matches all instances making them all targets. * The schedule for this activity is defined under Specify schedule and Specify with to use a CRON/Rate expression on a 30 minute interval. * There is the option to specify Output options . Note If you change the command document, the Parameters section will change to be appropriate to the new command document. Navigate to Managed Instances under Instances and Nodes in the navigation bar. An Association Status has been established for the inventoried instances under management. Choose one of the Instance ID links to go to the inventory of the instance. The Inventory tab is now populated and you can track associations and their last activity under the Associations tab. Navigate to Compliance under Instances & Nodes in the navigation bar. Here you can view the overall compliance status of your managed instances in the Compliance Summary and the individual compliance status of systems in the Corresponding managed instances section below. Note The inventory activity can take up to 10 minutes to complete. While waiting for the inventory activity to complete, you can proceed with the next section. Systems Manager: Compliance You can use AWS Systems Manager Configuration Compliance to scan your fleet of managed instances for patch compliance and configuration inconsistencies. You can collect and aggregate data from multiple AWS accounts and Regions, and then drill down into specific resources that aren\u2019t compliant. By default, Configuration Compliance displays compliance data about Systems Manager Patch Manager patching and Systems Manager State Manager associations. You can also customize the service and create your own compliance types based on your IT or business requirements. You can also port data to Amazon Athena and Amazon QuickSight to generate fleet-wide reports. 4. Patch Management Systems Manager: Patch Manager AWS Systems Manager Patch Manager automates the process of patching managed instances with security related updates. Note For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), and Amazon Linux. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches. You can target instances individually or in large groups by using Amazon EC2 tags. Warning * AWS does not test patches for Windows or Linux before making them available in Patch Manager . * If any updates are installed by Patch Manager the patched instance is rebooted . * Always test patches thoroughly before deploying to production environments . Patch Baselines Patch Manager uses patch baselines , which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. Later in this lab we will schedule patching to occur on a regular basis using a Systems Manager Maintenance Window task. Patch Manager integrates with AWS Identity and Access Management (IAM), AWS CloudTrail, and Amazon CloudWatch Events to provide a secure patching experience that includes event notifications and the ability to audit usage. Warning The operating systems supported by Patch Manager may vary from those supported by the SSM Agent. 4.1 Create a Patch Baseline Under Instances and Nodes in the AWS Systems Manager navigation bar, choose Patch Manager . Click the View predefined patch baselines link under the Configure patching button on the upper right. Choose Create patch baseline . On the Create patch baseline page in the Provide patch baseline details section: Enter a Name for your custom patch baseline, such as AmazonLinuxSecAndNonSecBaseline . Optionally enter a description, such as Amazon Linux patch baseline including security and non-security patches . Select Amazon Linux from the list. In the Approval rules section: Examine the options in the lists and ensure that Product , Classification , and Severity have values of All . Leave the Auto approval delay at its default of 0 days . Change the value of Compliance reporting - optional to Critical . Choose Add another rule . In the new rule, change the value of Compliance reporting - optional to Medium . Check the box under Include non-security updates to include all Amazon Linux updates when patching. If an approved patch is reported as missing, the option you choose in Compliance reporting , such as Critical or Medium , determines the severity of the compliance violation reported in System Manager Compliance . In the Patch exceptions section in the Rejected patches - optional text box, enter system-release.* This will reject patches to new Amazon Linux releases that may advance you beyond the Patch Manager supported operating systems prior to your testing new releases. For Linux operating systems, you can optionally define an alternative patch source repository . Choose the X in the Patch sources area to remove the empty patch source definition. Choose Create patch baseline and you will go to the Patch Baselines page where the AWS provided default patch baselines, and your custom baseline, are displayed. Patch Groups A patch group is an optional method to organize instances for patching. For example, you can create patch groups for different operating systems (Linux or Windows), different environments (Development, Test, and Production), or different server functions (web servers, file servers, databases). Patch groups can help you avoid deploying patches to the wrong set of instances. They can also help you avoid deploying patches before they have been adequately tested. You create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group must be defined with the tag key: Patch Group (tag keys are case sensitive). You can specify any value (for example, web servers ) but the key must be Patch Group . Note An instance can only be in one patch group. After you create a patch group and tag instances, you can register the patch group with a patch baseline. By registering the patch group with a patch baseline, you ensure that the correct patches are installed during the patching execution. When the system applies a patch baseline to an instance, the service checks if a patch group is defined for the instance. * If the instance is assigned to a patch group, the system checks to see which patch baseline is registered to that group. * If a patch baseline is found for that group, the system applies that patch baseline. * If an instance isn't assigned to a patch group, the system automatically uses the currently configured default patch baseline. 4.2 Assign a Patch Group Choose the Baseline ID of your newly created baseline to enter the details screen. Choose Actions in the top right of the window and select Modify patch groups . In the Modify patch groups window under Patch groups , enter Critical , choose Add , and then choose Close to be returned to the Patch Baseline details screen. AWS-RunPatchBaseline AWS-RunPatchBaseline is a command document that enables you to control patch approvals using patch baselines. It reports patch compliance information that you can view using the Systems Manager Compliance tools. For example,you can view which instances are missing patches and what those patches are. For Linux operating systems, compliance information is provided for patches from both the default source repository configured on an instance and from any alternative source repositories you specify in a custom patch baseline. AWS-RunPatchBaseline supports both Windows and Linux operating systems. AWS Systems Manager: Document An AWS Systems Manager document defines the actions that Systems Manager performs on your managed instances. Systems Manager includes many pre-configured documents that you can use by specifying parameters at runtime, including 'AWS-RunPatchBaseline'. These documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. All AWS provided Automation and Run Command documents can be viewed in AWS Systems Manager Documents . You can create your own documents or launch existing scripts using provided documents to implement custom operations as code activities. 4.3 Examine AWS-RunPatchBaseline in Documents To examine AWS-RunPatchBaseline in Documents: In the AWS Systems Manager navigation bar under Shared Resources , choose Documents . Click in the search box , select Document name prefix , and then Equal . Type AWS-Run into the text field and press Enter on your keyboard to start the search. Select AWS-RunPatchBaseline and choose View details . Review the content of each tab in the details page of the document. AWS Systems Manager: Run Command AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. Run Command enables you to automate common administrative tasks and perform ad hoc configuration changes at scale. You can use Run Command from the AWS Management Console, the AWS Command Line Interface, AWS Tools for Windows PowerShell, or the AWS SDKs. 4.4 Scan Your Instances with AWS-RunPatchBaseline via Run Command Under Instances and Nodes in the AWS Systems Manager navigation bar, choose Run Command . In the Run Command dashboard, you will see previously executed commands including the execution of AWS-RefreshAssociation, which was performed when you set up inventory. (Optional) choose a Command ID from the list and examine the record of the command execution. Choose Run Command in the top right of the window. In the Run a command window, under Command document : Choose the search icon and select Platform types , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. In the Command parameters section, leave the Operation value as the default Scan . In the Targets section: Under Specify targets by , choose Specifying a tag to reveal the Tags sub-section. Under Enter a tag key , enter Workload , and under Enter a tag value , enter Test and click Add . The remaining Run Command features enable you to: * Specify Rate control , limiting Concurrency to a specific number of targets or a calculated percentage of systems, or to specify an Error threshold by count or percentage of systems after which the command execution will end. * Specify Output options to record the entire output to a preconfigured S3 bucket and optional S3 key prefix . Note Only the last 2500 characters of a command document's output are displayed in the console. * Specify SNS notifications to a specified SNS Topic on all events or on a specific event type for either the entire command or on a per-instance basis. This requires Amazon SNS to be preconfigured. * View the command as it would appear if executed within the AWS Command Line Interface. Choose Run to execute the command and return to its details page. Scroll down to Targets and outputs to view the status of the individual targets that were selected through your tag key and value pair. Refresh your page to update the status. Choose an Instance ID from the targets list to view the Output from command execution on that instance. Choose Step 1 - Output to view the first 2500 characters of the command output from Step 1 of the command, and choose Step 1 - Output again to conceal it. Choose Step 2 - Output to view the first 2500 characters of the command output from Step 2 of the command. The execution step for PatchWindows was skipped as it did not apply to your Amazon Linux instance. Choose Step 1 - Output again to conceal it. 4.5 Review Initial Patch Compliance Under Instances & Nodes in the the AWS Systems Manager navigation bar, choose Compliance . On the Compliance page in the Compliance resources summary , you will now see that there are 4 systems that have critical severity compliance issues. In the Resources list, you will see the individual compliance status and details. 4.6 Patch Your Instances with AWS-RunPatchBaseline via Run Command Under Instances and Nodes in the AWS Systems Manager navigation bar, choose Run Command . Choose Run Command in the top right of the window. In the Run a command window, under Command document : Choose the search icon, select Platform types , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. In the Targets section: Under Specify targets by , choose Specifying a tag to reveal the Tags sub-section. Under Enter a tag key , enter Workload and under Enter a tag value enter Test . In the Command parameters section, change the Operation value to Install . In the Targets section, choose Specify a tag using Workload and Test . Note You could have choosen Manually selecting instances and used the check box at the top of the list to select all instances displayed, or selected them individually. Note there are multiple pages of instances. If manually selecting instances, individual selections must be made on each page. In the Rate control section: For Concurrency , ensure that targets is selected and specify the value as 1 . Tip Limiting concurrency will stagger the application of patches and the reboot cycle, however, to ensure that your instances are not rebooting at the same time, create separate tags to define target groups and schedule the application of patches at separate times. For Error threshold , ensure that error is selected and specify the value as 1 . Choose Run to execute the command and to go to its details page. Refresh the page to view updated status and proceed when the execution is successful. Warning Remember, if any updates are installed by Patch Manager, the patched instance is rebooted. 4.7 Review Patch Compliance After Patching Under Instances & Nodes in the the AWS Systems Manager navigation bar, choose Compliance . The Compliance resources summary will now show that there are 4 systems that have satisfied critical severity patch compliance. In the optional Scheduling Automated Operations Activities section of this lab you can set up Systems Manager Maintenance Windows and schedule the automated application of patches. The Impact of Operations as Code In a traditional environment, you would have had to set up the systems and software to perform these activities. You would require a server to execute your scripts. You would need to manage authentication credentials across all of your systems. Operations as code reduces the resources, time, risk, and complexity of performing operations tasks and ensures consistent execution. You can take operations as code and automate operations activities by using scheduling and event triggers. Through integration at the infrastructure level you avoid \"swivel chair\" processes that require multiple interfaces and systems to complete a single operations activity. Bonus Content: Creating Maintenance Windows and Scheduling Automated Operations Activities AWS Systems Manager: Maintenance Windows AWS Systems Manager Maintenance Windows let you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system (OS), updating drivers, or installing software. Each Maintenance Window has a schedule, a duration, a set of registered targets, and a set of registered tasks. With Maintenance Windows, you can perform tasks like the following: Installing applications, updating patches, installing or updating SSM Agent, or executing PowerShell commands and Linux shell scripts by using a Systems Manager Run Command task Building Amazon Machine Images (AMIs), boot-strapping software, and configuring instances by using Systems Manager Automation Executing AWS Lambda functions that trigger additional actions such as scanning your instances for patch updates Running AWS Step Function state machines to perform tasks such as removing an instance from an Elastic Load Balancing environment, patching the instance, and then adding the instance back to the Elastic Load Balancing environment Note To register Step Function tasks you must use the AWS CLI. 5.1 Setting up Maintenance Windows Create the role that allows Systems Manager to tasks in Maintenance Windows on your behalf: Navigate to the IAM console . In the navigation pane, choose Roles , and then choose Create role . In the Select type of trusted entity section, verify that the default AWS service is selected. In the Choose the service that will use this role section, choose EC2 . This allows EC2 instances to call AWS services on your behalf. Choose Next: Permissions . Under Attached permissions policy : Search for AmazonSSMMaintenanceWindowRole . Check the box next to AmazonSSMMaintenanceWindowRole in the list. Choose Next: Review . In the Review section: Enter a Role name , such as SSMMaintenanceWindowRole . Enter a Role description , such as Role for Amazon SSMMaintenanceWindow . Choose Create role . Upon success you will be returned to the Roles screen. To enable the service to run tasks on your behalf, we need to edit the trust relationship for this role: Choose the role you just created to enter its Summary page. Choose the Trust relationships tab. Choose Edit trust relationship . Delete the current policy, and then copy and paste the following policy into the Policy Document field: { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Sid\":\"\", \"Effect\":\"Allow\", \"Principal\":{ \"Service\":[ \"ec2.amazonaws.com\", \"ssm.amazonaws.com\", \"sns.amazonaws.com\" ] }, \"Action\":\"sts:AssumeRole\" } ] } Choose Update Trust Policy . You will be returned to the now updated Summary page for your role. Copy the Role ARN to your clipboard by choosing the double document icon at the end of the ARN. When you register a task with a Maintenance Window, you specify the role you created, which the service will assume when it runs tasks on your behalf. To register the task, you must assign the IAM PassRole policy to your IAM user account. The policy in the following procedure provides the minimum permissions required to register tasks with a Maintenance Window. To create the IAM PassRole policy for your Administrators IAM user group: In the IAM console navigation pane, choose Policies , and then choose Create policy . On the Create policy page, in the Select a service area , next to Service choose Choose a service , and then choose IAM . In the Actions section, search for PassRole and check the box next to it when it appears in the list. In the Resources section, choose \"You choose actions that require the role resource type.\", and then choose Add ARN to restrict access. The Add ARN(s) window will open. In the Add ARN(s) window, in the Specify ARN for role field , delete the existing entry, paste in the role ARN you created in the previous procedure, and then choose Add to return to the Create policy window. Choose Review policy . On the Review Policy page, type a name in the Name box, such as SSMMaintenanceWindowPassRole and then choose Create policy . You will be returned to the Policies page. To assign the IAM PassRole policy to your Administrators IAM user group: In the IAM console navigation pane, choose Groups , and then choose your Administrators group to reach its Summary page. Under the permissions tab, choose Attach Policy . On the Attach Policy page, search for SSMMaintenanceWindowPassRole, check the box next to it in the list, and choose Attach Policy . You will be returned to the Summary page for the group. Creating Maintenance Windows To create a Maintenance Window , you must do the following: Create the window and define its schedule and duration. Assign targets for the window. Assign tasks to run during the window. After you complete these steps, the Maintenance Window runs according to the schedule you defined and runs the tasks on the targets you specified. After a task is finished, Systems Manager logs the details of the execution. 5.2 Create a Patch Maintenance Window First, you must create the window and define its schedule and duration: Open the AWS Systems Manager console . In the navigation pane, choose Maintenance Windows and then choose Create a Maintenance Window . In the Provide maintenance window details section: In the Name field, type a descriptive name to help you identify this Maintenance Window, such as PatchTestWorkloadWebServers . (Optional) you may enter a description in the Description field. Choose Allow unregistered targets if you want to allow a Maintenance Window task to run on managed instances, even if you have not registered those instances as targets. Note If you choose Allow unregistered targets , then you can choose the unregistered instances (by instance ID) when you register a task with the Maintenance Window. If you don't, then you must choose previously registered targets when you register a task with the Maintenance Window. Specify a schedule for the Maintenance Window by using one of the scheduling options: Under Specify with , accept the default Cron schedule builder . Under Window starts , choose the third option, specify Every Day at , and select a time, such as 02:00 . In the Duration field, type the number of hours the Maintenance Window should run, such as '3' hours . In the Stop initiating tasks field, type the number of hours before the end of the Maintenance Window that the system should stop scheduling new tasks to run, such as 1 hour before the window closes . Allow enough time for initiate activities to complete before the close of the maintenance window. (Optionally) to have the maintenance window execute more rapidly while engaged with the lab: Under Window starts , choose Every 30 minutes to have the tasks execute on every hour and every half hour. Set the Duration to the minimum 1 hours. Set the Stop initiation tasks to the minimum 0 hours. Choose Create maintenance window . The system returns you to the Maintenance Window page. The state of the Maintenance Window you just created is Enabled . 5.3 Assigning Targets to Your Patch Maintenance Window After you create a Maintenance Window, you assign targets where the tasks will run. On the Maintenance windows page, choose the Window ID of your maintenance window to enter its Details page. Choose Actions in the top right of the window and select Register targets . On the Register target page under Maintenance window target details : In the Target Name field, enter a name for the targets, such as TestWebServers . (Optional) Enter a description in the Description field. (Optional) Specify a name or work alias in the Owner information field. Note : Owner information is included in any CloudWatch Events that are raised while running tasks for these targets in this Maintenance Window. In the Targets section, under Select Targets by : Choose the default Specifying tags to target instances by using Amazon EC2 tags that were previously assigned to the instances. Under Tags , enter 'Workload' as the key and Test as the value. The option to add and additional tag key/value pair will appear. Add a second key/value pair using InstanceRole as the key and WebServer as the value. Choose Register target at the bottom of the page to return to the maintenance window details page. If you want to assign more targets to this window, choose the Targets tab, and then choose Register target to register new targets. With this option, you can choose a different means of targeting. For example, if you previously targeted instances by instance ID, you can register new targets and target instances by specifying Amazon EC2 tags. 5.4 Assigning Tasks to Your Patch Maintenance Window After you assign targets, you assign tasks to perform during the window: From the details page of your maintenance window, choose Actions in the top right of the window and select Register Run command task . On the Register Run command task page: In the Name field, enter a name for the task, such as PatchTestWorkloadWebServers . (Optional) Enter a description in the Description field. In the Command document section: Choose the search icon, select Platform , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. Leave the Task priority at the default value of 1 (1 is the highest priority). Tasks in a Maintenance Window are scheduled in priority order, with tasks that have the same priority scheduled in parallel. In the Targets section: For Target by , select Selecting registered target groups . Select the group you created from the list. In the Rate control section: For Concurrency , leave the default targets selected and specify 1 . For Error threshold , leave the default errors selected and specify 1 . In the Role section, specify the role you defined with the AmazonSSMMaintenanceWindowRole. It will be SSMMaintenanceWindowRole if you followed the suggestion in the instructions above. In Output options , leave Enable writing to S3 clear. (Optionally) Specify Output options to record the entire output to a preconfigured S3 bucket and optional S3 key prefix Note Only the last 2500 characters of a command document's output are displayed in the console. To capture the complete output define and S3 bucket to receive the logs. In SNS notifications , leave Enable SNS notifications clear. (Optional) Specify SNS notifications to a preconfigured SNS Topic on all events or a specific event type for either the entire command or on a per-instance basis. In the Parameters section, under Operation , select Install . Choose Register Run command task to complete the task definition and return to the details page. 5.5 Review Maintenance Window Execution After allowing enough time for your maintenance window to complete: Navigte to the AWS Systems Manager console . Choose Maintenance Windows , and then select the Window ID for your new maintenance window. On the Maintenance window ID details page, choose History . Select a Windows execution ID and choose View details . On the Command ID details page, scroll down to the Targets and outputs section, select an Instance ID , and choose View output . Choose Step 1 - Output and review the output. Choose Step 2 - Output and review the output. You have now configured a maintenance window, assigned targets, assigned tasks, and validated successful execution. The same procedures can be used to schedule the execution of any AWS Systems Manager Document . Bonus Content: Creating a Simple Notification Service Topic Amazon Simple Notification Service (Amazon SNS) coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. In Amazon SNS, there are two types of clients: publishers and subscribers. These are also referred to as producers and consumers. Publishers communicate asynchronously with subscribers by producing and sending a message to a topic, which is a logical access point and communication channel. Subscribers (i.e., web servers, email addresses, Amazon SQS queues, AWS Lambda functions) consume or receive the message or notification over one of the supported protocols (i.e., Amazon SQS, HTTP/S, email, SMS, Lambda) when they are subscribed to the topic. 6.1 Create and Subscribe to an SNS Topic To create and subscribe to an SNS topic: Navigate to the SNS console at https://console.aws.amazon.com/sns/ . Choose Create topic . In the Create new topic window: In the Topic name field, enter AdminAlert . In the Display name field, enter AdminAlert . Choose Create topic . On the Topic details: AdminAlert page, choose Create subscription . In the Create subscription window: Select Email from the Protocol list. Enter your email address in the Endpoint field. Choose Create subscription . You will receive an email request for confirmation. Your Subscription ID will remain PendingConfirmation until you confirm your subscription by clicking through the link to Confirm subscription in the email. Refresh the page after confirming your subscription to view the populated Subscription ARN . You can now use this SNS topic to send notifications to your Administrator user. 7 Removing Lab Resources Note When the lab is complete, remove the resources you created. Otherwise you will be charged for any resources that are not covered in the AWS Free Tier. 7.1 Remove resources created with CloudFormation Navigate to the CloudFormation dashboard at https://console.aws.amazon.com/cloudformation/ : Select your first stack. Choose Actions and choose delete stack . Select your second stack. Choose Actions and choose delete stack . Navigate to Systems Manager console at https://console.aws.amazon.com/systems-manager/ : Choose State Manager . Select the association you created. Choose Delete . If you created an S3 bucket to store detailed output, delete the bucket and associated data: Navigate to the S3 console https://s3.console.aws.amazon.com/s3/ . Select the bucket. Choose Delete and provide the bucket name to confirm deletion. If you created the optional SNS Topic , delete the SNS topic: Navigate to the SNS console https://console.aws.amazon.com/sns/ . Select your AdminAlert SNS topic from the list. Choose Actions and select Delete topics . If you created a Maintenance Window , delete the Maintenance Window: Navigate to the Systems Manager console at https://console.aws.amazon.com/systems-manager/ . Choose Maintenance Windows . Select the maintenance window you created. Choose Delete . In the Delete maintenance window window, choose Delete . If you do not intend to continue to use the Administrator account you created, delete the account: Navigate to the IAM console at https://console.aws.amazon.com/iam/ . Choose Users . Select your user from the list. Choose Delete user . Select the check box next to \"One or more of these users have recently accessed AWS. Deleting them could affect running systems. Check the box to confirm that you want to delete these users.\". Choose Yes, delete . When next you navigate within the console you will be returned to the account login page. If you do intend to continue to use the Administrator account you created, we strongly suggest you enable MFA . Thank you for using this lab.","title":"Lab Guide"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#level-100-inventory-and-patch-management-lab-guide","text":"In the cloud, you can apply the same engineering discipline that you use for application code to your entire environment. You can define your entire workload (applications, infrastructure, etc.) as code and update it with code. You can script your operations procedures and automate their execution by triggering them in response to events. By performing operations as code, you limit human error and enable consistent execution of operations activities. In this lab you will apply the concepts of Infrastructure as Code and Operations as Code to the following activities: * Deployment of Infrastructure * Inventory Management * Patch Management Included in the lab guide are bonus sections that can be completed if you have time or later if interested. * Creating Maintenance Windows and Scheduling Automated Operations Activities * Create and Subscribe to a Simple Notification Service Topic Important You will be billed for any applicable AWS resources used in this lab that are not covered in the AWS Free Tier . At the end of the lab guide there is an additional section on how to remove all the resources you have created. * Removing Lab Resources","title":"Level 100: Inventory and Patch Management: Lab Guide"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#1-setup","text":"","title":"1. Setup"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#requirements","text":"You will need the following to be able to perform this lab: * Your own device for console access * An AWS account that you are able to use for testing, that is not used for production or other purposes * An available region within your account with capacity to add 2 additional VPCs","title":"Requirements"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#user-and-group-management","text":"When you create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user. It is accessed by signing in with the email address and password that you used to create the account. We strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user. Securely store the root user credentials and use them to perform only a few account and service management tasks. To view the tasks that require you to sign in as the root user, see AWS Tasks That Require Root User .","title":"User and Group Management"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#iam-users-groups","text":"As a best practice, do not use the AWS account root user for any task where it's not required. Instead, create a new IAM user for each person that requires administrator access. Then grant administrator access by placing the users into an \"Administrators\" group to which the AdministratorAccess managed policy is attached. Use administrators group members to manage permissions and policy for the AWS account. Limit use of the root user to only those actions that require it .","title":"IAM Users &amp; Groups"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#11-create-administrator-iam-user-and-group","text":"To create an administrator user for yourself and add the user to an administrators group: Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ . In the IAM navigation pane, choose Users and then choose Add user . In Set user details for User name , type a user name for the administrator account you are creating. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 64 characters in length. In Select AWS access type for Access type , select the check box next to AWS Management Console access , select Custom password , and then type your new password in the text box. If you're creating the user for someone other than yourself, you can leave Require password reset selected to force the user to create a new password when first signing in. Clear the box next to Require password reset and then choose Next: Permissions . In set permissions for user ensure Add user to group is selected. Under Add user to group choose Create group . In the Create group dialog box, type a Group name for the new group, such as Administrators. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 128 characters in length. In the policy list, select the check box next to AdministratorAccess and then choose Create group . Back at Add user to group , in the list of groups, ensure the check box for your new group is selected. Choose Refresh if necessary to see the group in the list. choose Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, choose Create user . At the confirmation screen you do not need to download the user credentials for programmatic access at this time. You can create new credentials at any time. You can use this same process to create more groups and users and to give your users access to your AWS account resources. To learn about using policies that restrict user permissions to specific AWS resources, see Access Management and Example Policies . To add additional users to the group after it's created, see Adding and Removing Users in an IAM Group .","title":"1.1 Create Administrator IAM User and Group"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#12-log-in-to-the-aws-management-console-using-your-administrator-account","text":"You can now use this administrator user instead of your root user for this AWS account. Choose the link https\\://\\<yourAccountNumber>.signin.aws.amazon.com/console and log in with your administrator user credentials. Select the region you will use for the lab from the the list in the upper right corner. Verify that you have 2 available VPCs (3 or less in use) in the selected region by navigating to the VPC Console (https://console.aws.amazon.com/vpc/) and in the Resources section reviewing the number of VPCs.","title":"1.2 Log in to the AWS Management Console using your administrator account"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#13-create-an-ec2-key-pair","text":"Amazon EC2 uses public-key cryptography to encrypt and decrypt login information. Public-key cryptography uses a public key to encrypt a piece of data, such as a password, then the recipient uses the private key to decrypt the data. The public and private keys are known as a key pair. To log in to the Amazon Linux instances we will create in this lab, you must create a key pair, specify the name of the key pair when you launch the instance, and provide the private key when you connect to the instance. Use your administrator account to access the Amazon EC2 console at https://console.aws.amazon.com/ec2/ . In the IAM navigation pane under Network & Security , choose Key Pairs and then choose Create Key Pair . In the Create Key Pair dialog box, type a Key pair name such as OELabIPM and then choose Create . Save the keyPairName.pem file for optional later use accessing the EC2 instances created in this lab.","title":"1.3 Create an EC2 Key Pair"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#2-deploy-an-environment-using-infrastructure-as-code","text":"","title":"2. Deploy an Environment Using Infrastructure as Code"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#tagging","text":"We will make extensive use of tagging throughout the lab. The CloudFormation template for the lab includes the definition of multiple tags against a variety of resources. AWS enables you to assign metadata to your AWS resources in the form of tags . Each tag is a simple label consisting of a customer-defined key and an optional value that can make it easier to manage, search for, and filter resources. Although there are no inherent types of tags, commonly adopted categories of tags include technical tags (e.g., Environment, Workload, InstanceRole, and Name), tags for automation (e.g., Patch Group, and SSMManaged), business tags (e.g., Owner), and security tags (e.g., Confidentiality). Apply the following best practices when using tags: * Use a standardized, case-sensitive format for tags, and implement it consistently across all resource types * Consider tag dimensions that support the following: * Managing resource access control with IAM * Cost tracking * Automation * AWS console organization * Implement automated tools to help manage resource tags. The Resource Groups Tagging API enables programmatic control of tags, making it easier to automatically manage, search, and filter tags and resources. * Err on the side of using too many tags rather than too few tags. * Develop a tagging strategy . Note It is easy to modify tags to accommodate changing business requirements; however, consider the consequences of future changes, especially in relation to tag-based access control, automation, or upstream billing reports. Important Patch Group is a reserved tag key used by Systems Manager Patch Manager that is case sensitive with a space between the two words.","title":"Tagging"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#management-tools-cloudformation","text":"AWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing those resources and more time focusing on your applications. You create a template that describes all the AWS resources that you want (like Amazon EC2 instances or Amazon RDS DB instances) and AWS CloudFormation provisions and configures those resources for you. AWS CloudFormation enables you to use a template file to create and delete a collection of resources as a single unit (a stack). There is no additional charge for AWS CloudFormation . You pay for AWS resources (such as Amazon EC2 instances, Elastic Load Balancing load balancers, etc.) created using AWS CloudFormation in the same manner as if you created the resources manually. You only pay for what you use as you use it. There are no minimum fees and no required upfront commitments.","title":"Management Tools: CloudFormation"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#21-deploy-the-lab-infrastructure","text":"To deploy the lab infrastructure: Download the CloudFormation script for this lab from /Code . Use your administrator account to access the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Choose Create Stack . On the Select Template page, select Specify an Amazon S3 template URL and enter the URL for the downloaded location. AWS CloudFormation Designer AWS CloudFormation Designer is a graphic tool for creating, viewing, and modifying AWS CloudFormation templates. With Designer you can diagram your template resources using a drag-and-drop interface. You can edit their details using the integrated JSON and YAML editor. AWS CloudFormation Designer can help you see the relationship between template resources. On the Select Template page, next to Specify an Amazon S3 template URL , choose the link to View/Edit template in Designer . Briefly review the graphical representation of the environment we are about to create, including the template in the JSON and YAML formats. You can use this feature to convert between JSON and YAML formats. Choose the Create Stack icon (a cloud with an arrow) to return to the Select Template page . On the Select Template page, choose Next . A CloudFormation template is a JSON or YAML formatted text file that describes your AWS infrastructure containing both optional and required sections . In the next steps, we will provide a name for our stack and parameters that will be passed into the template to help define the resources that will be implemented. In the Specify Details section, define a Stack name , such as OELabStack1 . In the Parameters section: Leave InstanceProfile blank as we have not yet defined an instance profile. Leave InstanceTypeApp and InstanceTypeWeb as the default free-tier-eligible t2.micro value. Select the EC2 KeyName you defined earlier from the list. In a browser window, go to https://checkip.amazonaws.com/ to get your IP. Enter your IP address in SSHLocation in CIDR notation (i.e., ending in /32). Define the Workload Name as Test . Choose Next . On the Options page under Tags , define a Key of Owner , with Value set to the username you choose for your administrator. You may define additional keys as needed. The CloudFormation template creates all the example tags given in the discussion on tagging above. Leave all other sections unmodified. Scroll to the bottom of the page and choose Next . On the Review page, review your choices and then choose Create . On the CloudFormation console page Check the box next to your Stack Name to see its details. If your Stack Name is not displayed, click the refresh button (circular arrow) in the top right until it appears. If the details are not displayed, choose the refresh button until details appear. Choose the Events tab for your selected workload to see the activity log from the creation of your CloudFormation stack. When the Status of your stack displays CREATE_COMPLETE in the filter list, you have just created a representation of a typical lift and shift 2-tier application migrated to the cloud. Navigate to the EC2 console to view the deployed systems: Choose Instances . Select a server and review the details under its Description and Tag tabs. (Optional) choose Security Groups and select the Security Group whose name begins with the name of your stack. Examine the inbound rules. (Optional) navigate to the VPC console and examine the configuration of the VPC you just created.","title":"2.1 Deploy the Lab Infrastructure"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#the-impact-of-infrastructure-as-code","text":"With infrastructure as code, if you can deploy one environment, you can deploy any number of copies of that environment. In this example we have created a Test environment. Later, we will repeat these steps to deploy a Prod environment. The ability to dynamically deploy temporary environments on-demand enables parallel experimentation, development, and testing efforts. It allows duplication of environments to recreate and analyze errors, as well as cut-over deployment of production systems using blue-green methodologies. These practices contribute to reduced risk, increased operations effectiveness, and efficiency.","title":"The impact of Infrastructure as Code"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#3-inventory-management-using-operations-as-code","text":"","title":"3. Inventory Management using Operations as Code"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#management-tools-systems-manager","text":"AWS Systems Manager is a collection of features that enable IT Operations that we will explore throughout this lab. There are set up tasks and pre-requisites that must be satisfied prior to using Systems Manager to manage your EC2 instances or on-premises systems in hybrid environments . * You must use a supported operating system * Supported operating systems include versions of Windows, Amazon Linux, Ubuntu Server, RHEL, and CentOS * The SSM Agent must be installed * The SSM Agent for Windows also requires PowerShell 3.0 or later to run some SSM documents * Your EC2 instances must have outbound internet access * You must access Systems Manager in a supported region * Systems Manager requires IAM roles * for instances that will process commands * for users executing commands SSM Agent is installed by default on: * Amazon Linux base AMIs dated 2017.09 and later * Windows Server 2016 instances * Instances created from Windows Server 2003-2012 R2 AMIs published in November 2016 or later There is no additional charge for AWS Systems Manager . You only pay for your underlying AWS resources managed or created by AWS Systems Manager (e.g., Amazon EC2 instances or Amazon CloudWatch metrics). You only pay for what you use as you use it. There are no minimum fees and no upfront commitments.","title":"Management Tools: Systems Manager"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#31-setting-up-systems-manager","text":"Use your administrator account to access the Systems Manager console at https://console.aws.amazon.com/systems-manager/ . Choose Managed Instances from the navigation bar. If you have not satisfied the pre-requisites for Systems Manager, you will arrive at the AWS Systems Manager Managed Instances page. As a user with AdministratorAccess permissions, you already have User Access to Systems Manager . The Amazon Linux AMIs used to create the instances in your environment are dated 2017.09. They are supported operating systems and have the SSM Agent installed by default. If you are in a supported region the remaining step is to configure the IAM role for instances that will process commands. Create an Instance Profile for Systems Manager managed instances: Navigate to the IAM console In the navigation pane, choose Roles . Then choose Create role . In the Select type of trusted entity section, verify that the default AWS service is selected. In the Choose the service that will use this role section, scroll past the first reference to EC2 ( EC2 Allows EC2 instances to call AWS services on your behalf ) and choose EC2 from within the field of services. This will open the Select your use case section further down the page. In the Select your use case section, choose EC2 Role for Simple Systems Manager to select it. Then choose Next: Permissions . Under Attached permissions policy , verify that AmazonEC2RoleforSSM is listed, and then choose Next: Review . In the Review section: Enter a Role name , such as ManagedInstancesRole . Accept the default in the Role description . Choose Create role . Apply this role to the instances you wish to manage with Systems Manager: Navigate to the EC2 Console and choose Instances . Select the first instance and then choose Actions , Instance Settings , and Attach/Replace IAM Role . Under Attach/Replace IAM Role , select ManagedInstancesRole from the drop down list and choose Apply . After you receive confirmation of success, choose Close . Repeat this process, assigning ManagedInstancesRole to each of the 3 remaining instances. Return to the Systems Manager console and choose Managed Instances from the navigation bar. Periodically choose Managed Instances until your instances begin to appear in the list. Over the next couple of minutes your instances will populate into the list as managed instances. Note If desired, you can use a more restrictive permission set to grant access to Systems Manager.","title":"3.1 Setting up Systems Manager"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#32-create-a-second-cloudformation-stack","text":"Create a second CloudFormation stack using the procedure in 2.1 with the following changes: In the Specify Details section, define a Stack name, such as OELabStack2 . Specify the InstanceProfile using the ManagedInstancesRole you defined. Define the Workload Name as Prod .","title":"3.2 Create a Second CloudFormation Stack"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#systems-manager-inventory","text":"You can use AWS Systems Manager Inventory to collect operating system (OS), application, and instance metadata from your Amazon EC2 instances and your on-premises servers or virtual machines (VMs) in your hybrid environment. You can query the metadata to quickly understand which instances are running the software and configurations required by your software policy, and which instances need to be updated.","title":"Systems Manager: Inventory"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#33-using-systems-manager-inventory-to-track-your-instances","text":"Under Instances & Nodes in the AWS Systems Manager navigation bar, choose Inventory . Scroll down in the window to the Corresponding managed instances section. Inventory currently contains only the instance data available from the EC2 Choose the InstanceID of one of your systems. Examine each of the available tabs of data under the Instance ID heading. Inventory collection must be specifically configured and the data types to be collected must be specified Choose Inventory in the navigation bar. Choose Setup Inventory in the top right corner of the window In the Setup Inventory screen, define targets for inventory: Under Specify targets by , select Specifying a tag Under Tags specify Environment for the key and OELabIPM for the value Note You can select all managed instances in this account, ensuring that all managed instances will be inventoried. You can constrain inventoried instances to those with specific tags, such as Environment or Workload. Or you can manually select specific instances for inventory. Schedule the frequency with which inventory is collected. The default and minimum period is 30 minutes For Collect inventory data every , accept the default 30 Minute(s) Under parameters, specify what information to collect with the inventory process Review the options and select the defaults (Optional) If desired, you may specify an S3 bucket to receive the inventory execution logs (you will need to create a destination bucket for the logs prior to proceeding): Check the box next to Sync inventory execution logs to an S3 bucket under the Advanced options. Provide an S3 bucket name. (Optional) Provide an S3 bucket prefix. Choose Setup Inventory at the bottom of the page (it can take up to 10 minutes to deploy a new inventory policy to an instance). To create a new inventory policy, from Inventory , choose Setup inventory . To edit an existing policy, from State Manager in the left navigation menu, select the association and choose Edit . Note You can create multiple Inventory specifications. They will each be stored as associations within Systems Manager State Manager .","title":"3.3 Using Systems Manager Inventory to Track Your Instances"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#systems-manager-state-manager","text":"In State Manager, an association is the result of binding configuration information that defines the state you want your instances to be in to the instances themselves. This information specifies when and how you want instance-related operations to run that ensure your Amazon EC2 and hybrid infrastructure is in an intended or consistent state. An association defines the state you want to apply to a set of targets. An association includes three components and one optional set of components: * A document that defines the state * Target(s) * A schedule * (Optional) Runtime parameters. When you performed the Setup Inventory actions, you created an association in State Manager.","title":"Systems Manager: State Manager"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#34-review-association-status","text":"Under Actions in the navigation bar, select State Manager . At this point, the Status may show that the inventory activity has not yet completed. Choose the single Association id that is the result of your Setup Inventory action. Examine each of the available tabs of data under the Association ID heading. Choose Edit . Enter a name under Name - optional to provide a more user friendly label to the association, such as InventoryAllInstances (white space is not permitted in an Association Name ). Inventory is accomplished through the following: * The activities defined in the AWS-GatherSoftwareInventory command document. * The parameters provided in the Parameters section are passed to the document at execution. * The targets are defined in the Targets section. Important In this example there is a single target, the wildcard. The wildcard matches all instances making them all targets. * The schedule for this activity is defined under Specify schedule and Specify with to use a CRON/Rate expression on a 30 minute interval. * There is the option to specify Output options . Note If you change the command document, the Parameters section will change to be appropriate to the new command document. Navigate to Managed Instances under Instances and Nodes in the navigation bar. An Association Status has been established for the inventoried instances under management. Choose one of the Instance ID links to go to the inventory of the instance. The Inventory tab is now populated and you can track associations and their last activity under the Associations tab. Navigate to Compliance under Instances & Nodes in the navigation bar. Here you can view the overall compliance status of your managed instances in the Compliance Summary and the individual compliance status of systems in the Corresponding managed instances section below. Note The inventory activity can take up to 10 minutes to complete. While waiting for the inventory activity to complete, you can proceed with the next section.","title":"3.4 Review Association Status"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#systems-manager-compliance","text":"You can use AWS Systems Manager Configuration Compliance to scan your fleet of managed instances for patch compliance and configuration inconsistencies. You can collect and aggregate data from multiple AWS accounts and Regions, and then drill down into specific resources that aren\u2019t compliant. By default, Configuration Compliance displays compliance data about Systems Manager Patch Manager patching and Systems Manager State Manager associations. You can also customize the service and create your own compliance types based on your IT or business requirements. You can also port data to Amazon Athena and Amazon QuickSight to generate fleet-wide reports.","title":"Systems Manager: Compliance"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#4-patch-management","text":"","title":"4. Patch Management"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#systems-manager-patch-manager","text":"AWS Systems Manager Patch Manager automates the process of patching managed instances with security related updates. Note For Linux-based instances, you can also install patches for non-security updates. You can patch fleets of Amazon EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows, Ubuntu Server, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), and Amazon Linux. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches. You can target instances individually or in large groups by using Amazon EC2 tags. Warning * AWS does not test patches for Windows or Linux before making them available in Patch Manager . * If any updates are installed by Patch Manager the patched instance is rebooted . * Always test patches thoroughly before deploying to production environments .","title":"Systems Manager: Patch Manager"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#patch-baselines","text":"Patch Manager uses patch baselines , which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. Later in this lab we will schedule patching to occur on a regular basis using a Systems Manager Maintenance Window task. Patch Manager integrates with AWS Identity and Access Management (IAM), AWS CloudTrail, and Amazon CloudWatch Events to provide a secure patching experience that includes event notifications and the ability to audit usage. Warning The operating systems supported by Patch Manager may vary from those supported by the SSM Agent.","title":"Patch Baselines"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#41-create-a-patch-baseline","text":"Under Instances and Nodes in the AWS Systems Manager navigation bar, choose Patch Manager . Click the View predefined patch baselines link under the Configure patching button on the upper right. Choose Create patch baseline . On the Create patch baseline page in the Provide patch baseline details section: Enter a Name for your custom patch baseline, such as AmazonLinuxSecAndNonSecBaseline . Optionally enter a description, such as Amazon Linux patch baseline including security and non-security patches . Select Amazon Linux from the list. In the Approval rules section: Examine the options in the lists and ensure that Product , Classification , and Severity have values of All . Leave the Auto approval delay at its default of 0 days . Change the value of Compliance reporting - optional to Critical . Choose Add another rule . In the new rule, change the value of Compliance reporting - optional to Medium . Check the box under Include non-security updates to include all Amazon Linux updates when patching. If an approved patch is reported as missing, the option you choose in Compliance reporting , such as Critical or Medium , determines the severity of the compliance violation reported in System Manager Compliance . In the Patch exceptions section in the Rejected patches - optional text box, enter system-release.* This will reject patches to new Amazon Linux releases that may advance you beyond the Patch Manager supported operating systems prior to your testing new releases. For Linux operating systems, you can optionally define an alternative patch source repository . Choose the X in the Patch sources area to remove the empty patch source definition. Choose Create patch baseline and you will go to the Patch Baselines page where the AWS provided default patch baselines, and your custom baseline, are displayed.","title":"4.1 Create a Patch Baseline"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#patch-groups","text":"A patch group is an optional method to organize instances for patching. For example, you can create patch groups for different operating systems (Linux or Windows), different environments (Development, Test, and Production), or different server functions (web servers, file servers, databases). Patch groups can help you avoid deploying patches to the wrong set of instances. They can also help you avoid deploying patches before they have been adequately tested. You create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group must be defined with the tag key: Patch Group (tag keys are case sensitive). You can specify any value (for example, web servers ) but the key must be Patch Group . Note An instance can only be in one patch group. After you create a patch group and tag instances, you can register the patch group with a patch baseline. By registering the patch group with a patch baseline, you ensure that the correct patches are installed during the patching execution. When the system applies a patch baseline to an instance, the service checks if a patch group is defined for the instance. * If the instance is assigned to a patch group, the system checks to see which patch baseline is registered to that group. * If a patch baseline is found for that group, the system applies that patch baseline. * If an instance isn't assigned to a patch group, the system automatically uses the currently configured default patch baseline.","title":"Patch Groups"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#42-assign-a-patch-group","text":"Choose the Baseline ID of your newly created baseline to enter the details screen. Choose Actions in the top right of the window and select Modify patch groups . In the Modify patch groups window under Patch groups , enter Critical , choose Add , and then choose Close to be returned to the Patch Baseline details screen.","title":"4.2 Assign a Patch Group"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#aws-runpatchbaseline","text":"AWS-RunPatchBaseline is a command document that enables you to control patch approvals using patch baselines. It reports patch compliance information that you can view using the Systems Manager Compliance tools. For example,you can view which instances are missing patches and what those patches are. For Linux operating systems, compliance information is provided for patches from both the default source repository configured on an instance and from any alternative source repositories you specify in a custom patch baseline. AWS-RunPatchBaseline supports both Windows and Linux operating systems.","title":"AWS-RunPatchBaseline"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#aws-systems-manager-document","text":"An AWS Systems Manager document defines the actions that Systems Manager performs on your managed instances. Systems Manager includes many pre-configured documents that you can use by specifying parameters at runtime, including 'AWS-RunPatchBaseline'. These documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify. All AWS provided Automation and Run Command documents can be viewed in AWS Systems Manager Documents . You can create your own documents or launch existing scripts using provided documents to implement custom operations as code activities.","title":"AWS Systems Manager: Document"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#43-examine-aws-runpatchbaseline-in-documents","text":"To examine AWS-RunPatchBaseline in Documents: In the AWS Systems Manager navigation bar under Shared Resources , choose Documents . Click in the search box , select Document name prefix , and then Equal . Type AWS-Run into the text field and press Enter on your keyboard to start the search. Select AWS-RunPatchBaseline and choose View details . Review the content of each tab in the details page of the document.","title":"4.3 Examine AWS-RunPatchBaseline in Documents"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#aws-systems-manager-run-command","text":"AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. Run Command enables you to automate common administrative tasks and perform ad hoc configuration changes at scale. You can use Run Command from the AWS Management Console, the AWS Command Line Interface, AWS Tools for Windows PowerShell, or the AWS SDKs.","title":"AWS Systems Manager: Run Command"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#44-scan-your-instances-with-aws-runpatchbaseline-via-run-command","text":"Under Instances and Nodes in the AWS Systems Manager navigation bar, choose Run Command . In the Run Command dashboard, you will see previously executed commands including the execution of AWS-RefreshAssociation, which was performed when you set up inventory. (Optional) choose a Command ID from the list and examine the record of the command execution. Choose Run Command in the top right of the window. In the Run a command window, under Command document : Choose the search icon and select Platform types , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. In the Command parameters section, leave the Operation value as the default Scan . In the Targets section: Under Specify targets by , choose Specifying a tag to reveal the Tags sub-section. Under Enter a tag key , enter Workload , and under Enter a tag value , enter Test and click Add . The remaining Run Command features enable you to: * Specify Rate control , limiting Concurrency to a specific number of targets or a calculated percentage of systems, or to specify an Error threshold by count or percentage of systems after which the command execution will end. * Specify Output options to record the entire output to a preconfigured S3 bucket and optional S3 key prefix . Note Only the last 2500 characters of a command document's output are displayed in the console. * Specify SNS notifications to a specified SNS Topic on all events or on a specific event type for either the entire command or on a per-instance basis. This requires Amazon SNS to be preconfigured. * View the command as it would appear if executed within the AWS Command Line Interface. Choose Run to execute the command and return to its details page. Scroll down to Targets and outputs to view the status of the individual targets that were selected through your tag key and value pair. Refresh your page to update the status. Choose an Instance ID from the targets list to view the Output from command execution on that instance. Choose Step 1 - Output to view the first 2500 characters of the command output from Step 1 of the command, and choose Step 1 - Output again to conceal it. Choose Step 2 - Output to view the first 2500 characters of the command output from Step 2 of the command. The execution step for PatchWindows was skipped as it did not apply to your Amazon Linux instance. Choose Step 1 - Output again to conceal it.","title":"4.4 Scan Your Instances with AWS-RunPatchBaseline via Run Command"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#45-review-initial-patch-compliance","text":"Under Instances & Nodes in the the AWS Systems Manager navigation bar, choose Compliance . On the Compliance page in the Compliance resources summary , you will now see that there are 4 systems that have critical severity compliance issues. In the Resources list, you will see the individual compliance status and details.","title":"4.5 Review Initial Patch Compliance"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#46-patch-your-instances-with-aws-runpatchbaseline-via-run-command","text":"Under Instances and Nodes in the AWS Systems Manager navigation bar, choose Run Command . Choose Run Command in the top right of the window. In the Run a command window, under Command document : Choose the search icon, select Platform types , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. In the Targets section: Under Specify targets by , choose Specifying a tag to reveal the Tags sub-section. Under Enter a tag key , enter Workload and under Enter a tag value enter Test . In the Command parameters section, change the Operation value to Install . In the Targets section, choose Specify a tag using Workload and Test . Note You could have choosen Manually selecting instances and used the check box at the top of the list to select all instances displayed, or selected them individually. Note there are multiple pages of instances. If manually selecting instances, individual selections must be made on each page. In the Rate control section: For Concurrency , ensure that targets is selected and specify the value as 1 . Tip Limiting concurrency will stagger the application of patches and the reboot cycle, however, to ensure that your instances are not rebooting at the same time, create separate tags to define target groups and schedule the application of patches at separate times. For Error threshold , ensure that error is selected and specify the value as 1 . Choose Run to execute the command and to go to its details page. Refresh the page to view updated status and proceed when the execution is successful. Warning Remember, if any updates are installed by Patch Manager, the patched instance is rebooted.","title":"4.6 Patch Your Instances with AWS-RunPatchBaseline via Run Command"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#47-review-patch-compliance-after-patching","text":"Under Instances & Nodes in the the AWS Systems Manager navigation bar, choose Compliance . The Compliance resources summary will now show that there are 4 systems that have satisfied critical severity patch compliance. In the optional Scheduling Automated Operations Activities section of this lab you can set up Systems Manager Maintenance Windows and schedule the automated application of patches.","title":"4.7 Review Patch Compliance After Patching"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#the-impact-of-operations-as-code","text":"In a traditional environment, you would have had to set up the systems and software to perform these activities. You would require a server to execute your scripts. You would need to manage authentication credentials across all of your systems. Operations as code reduces the resources, time, risk, and complexity of performing operations tasks and ensures consistent execution. You can take operations as code and automate operations activities by using scheduling and event triggers. Through integration at the infrastructure level you avoid \"swivel chair\" processes that require multiple interfaces and systems to complete a single operations activity.","title":"The Impact of Operations as Code"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#bonus-content-creating-maintenance-windows-and-scheduling-automated-operations-activities","text":"","title":"Bonus Content: Creating Maintenance Windows and Scheduling Automated Operations Activities"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#aws-systems-manager-maintenance-windows","text":"AWS Systems Manager Maintenance Windows let you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system (OS), updating drivers, or installing software. Each Maintenance Window has a schedule, a duration, a set of registered targets, and a set of registered tasks. With Maintenance Windows, you can perform tasks like the following: Installing applications, updating patches, installing or updating SSM Agent, or executing PowerShell commands and Linux shell scripts by using a Systems Manager Run Command task Building Amazon Machine Images (AMIs), boot-strapping software, and configuring instances by using Systems Manager Automation Executing AWS Lambda functions that trigger additional actions such as scanning your instances for patch updates Running AWS Step Function state machines to perform tasks such as removing an instance from an Elastic Load Balancing environment, patching the instance, and then adding the instance back to the Elastic Load Balancing environment Note To register Step Function tasks you must use the AWS CLI.","title":"AWS Systems Manager: Maintenance Windows"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#51-setting-up-maintenance-windows","text":"Create the role that allows Systems Manager to tasks in Maintenance Windows on your behalf: Navigate to the IAM console . In the navigation pane, choose Roles , and then choose Create role . In the Select type of trusted entity section, verify that the default AWS service is selected. In the Choose the service that will use this role section, choose EC2 . This allows EC2 instances to call AWS services on your behalf. Choose Next: Permissions . Under Attached permissions policy : Search for AmazonSSMMaintenanceWindowRole . Check the box next to AmazonSSMMaintenanceWindowRole in the list. Choose Next: Review . In the Review section: Enter a Role name , such as SSMMaintenanceWindowRole . Enter a Role description , such as Role for Amazon SSMMaintenanceWindow . Choose Create role . Upon success you will be returned to the Roles screen. To enable the service to run tasks on your behalf, we need to edit the trust relationship for this role: Choose the role you just created to enter its Summary page. Choose the Trust relationships tab. Choose Edit trust relationship . Delete the current policy, and then copy and paste the following policy into the Policy Document field: { \"Version\":\"2012-10-17\", \"Statement\":[ { \"Sid\":\"\", \"Effect\":\"Allow\", \"Principal\":{ \"Service\":[ \"ec2.amazonaws.com\", \"ssm.amazonaws.com\", \"sns.amazonaws.com\" ] }, \"Action\":\"sts:AssumeRole\" } ] } Choose Update Trust Policy . You will be returned to the now updated Summary page for your role. Copy the Role ARN to your clipboard by choosing the double document icon at the end of the ARN. When you register a task with a Maintenance Window, you specify the role you created, which the service will assume when it runs tasks on your behalf. To register the task, you must assign the IAM PassRole policy to your IAM user account. The policy in the following procedure provides the minimum permissions required to register tasks with a Maintenance Window. To create the IAM PassRole policy for your Administrators IAM user group: In the IAM console navigation pane, choose Policies , and then choose Create policy . On the Create policy page, in the Select a service area , next to Service choose Choose a service , and then choose IAM . In the Actions section, search for PassRole and check the box next to it when it appears in the list. In the Resources section, choose \"You choose actions that require the role resource type.\", and then choose Add ARN to restrict access. The Add ARN(s) window will open. In the Add ARN(s) window, in the Specify ARN for role field , delete the existing entry, paste in the role ARN you created in the previous procedure, and then choose Add to return to the Create policy window. Choose Review policy . On the Review Policy page, type a name in the Name box, such as SSMMaintenanceWindowPassRole and then choose Create policy . You will be returned to the Policies page. To assign the IAM PassRole policy to your Administrators IAM user group: In the IAM console navigation pane, choose Groups , and then choose your Administrators group to reach its Summary page. Under the permissions tab, choose Attach Policy . On the Attach Policy page, search for SSMMaintenanceWindowPassRole, check the box next to it in the list, and choose Attach Policy . You will be returned to the Summary page for the group.","title":"5.1 Setting up Maintenance Windows"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#creating-maintenance-windows","text":"To create a Maintenance Window , you must do the following: Create the window and define its schedule and duration. Assign targets for the window. Assign tasks to run during the window. After you complete these steps, the Maintenance Window runs according to the schedule you defined and runs the tasks on the targets you specified. After a task is finished, Systems Manager logs the details of the execution.","title":"Creating Maintenance Windows"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#52-create-a-patch-maintenance-window","text":"First, you must create the window and define its schedule and duration: Open the AWS Systems Manager console . In the navigation pane, choose Maintenance Windows and then choose Create a Maintenance Window . In the Provide maintenance window details section: In the Name field, type a descriptive name to help you identify this Maintenance Window, such as PatchTestWorkloadWebServers . (Optional) you may enter a description in the Description field. Choose Allow unregistered targets if you want to allow a Maintenance Window task to run on managed instances, even if you have not registered those instances as targets. Note If you choose Allow unregistered targets , then you can choose the unregistered instances (by instance ID) when you register a task with the Maintenance Window. If you don't, then you must choose previously registered targets when you register a task with the Maintenance Window. Specify a schedule for the Maintenance Window by using one of the scheduling options: Under Specify with , accept the default Cron schedule builder . Under Window starts , choose the third option, specify Every Day at , and select a time, such as 02:00 . In the Duration field, type the number of hours the Maintenance Window should run, such as '3' hours . In the Stop initiating tasks field, type the number of hours before the end of the Maintenance Window that the system should stop scheduling new tasks to run, such as 1 hour before the window closes . Allow enough time for initiate activities to complete before the close of the maintenance window. (Optionally) to have the maintenance window execute more rapidly while engaged with the lab: Under Window starts , choose Every 30 minutes to have the tasks execute on every hour and every half hour. Set the Duration to the minimum 1 hours. Set the Stop initiation tasks to the minimum 0 hours. Choose Create maintenance window . The system returns you to the Maintenance Window page. The state of the Maintenance Window you just created is Enabled .","title":"5.2 Create a Patch Maintenance Window"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#53-assigning-targets-to-your-patch-maintenance-window","text":"After you create a Maintenance Window, you assign targets where the tasks will run. On the Maintenance windows page, choose the Window ID of your maintenance window to enter its Details page. Choose Actions in the top right of the window and select Register targets . On the Register target page under Maintenance window target details : In the Target Name field, enter a name for the targets, such as TestWebServers . (Optional) Enter a description in the Description field. (Optional) Specify a name or work alias in the Owner information field. Note : Owner information is included in any CloudWatch Events that are raised while running tasks for these targets in this Maintenance Window. In the Targets section, under Select Targets by : Choose the default Specifying tags to target instances by using Amazon EC2 tags that were previously assigned to the instances. Under Tags , enter 'Workload' as the key and Test as the value. The option to add and additional tag key/value pair will appear. Add a second key/value pair using InstanceRole as the key and WebServer as the value. Choose Register target at the bottom of the page to return to the maintenance window details page. If you want to assign more targets to this window, choose the Targets tab, and then choose Register target to register new targets. With this option, you can choose a different means of targeting. For example, if you previously targeted instances by instance ID, you can register new targets and target instances by specifying Amazon EC2 tags.","title":"5.3 Assigning Targets to Your Patch Maintenance Window"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#54-assigning-tasks-to-your-patch-maintenance-window","text":"After you assign targets, you assign tasks to perform during the window: From the details page of your maintenance window, choose Actions in the top right of the window and select Register Run command task . On the Register Run command task page: In the Name field, enter a name for the task, such as PatchTestWorkloadWebServers . (Optional) Enter a description in the Description field. In the Command document section: Choose the search icon, select Platform , and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list. Leave the Task priority at the default value of 1 (1 is the highest priority). Tasks in a Maintenance Window are scheduled in priority order, with tasks that have the same priority scheduled in parallel. In the Targets section: For Target by , select Selecting registered target groups . Select the group you created from the list. In the Rate control section: For Concurrency , leave the default targets selected and specify 1 . For Error threshold , leave the default errors selected and specify 1 . In the Role section, specify the role you defined with the AmazonSSMMaintenanceWindowRole. It will be SSMMaintenanceWindowRole if you followed the suggestion in the instructions above. In Output options , leave Enable writing to S3 clear. (Optionally) Specify Output options to record the entire output to a preconfigured S3 bucket and optional S3 key prefix Note Only the last 2500 characters of a command document's output are displayed in the console. To capture the complete output define and S3 bucket to receive the logs. In SNS notifications , leave Enable SNS notifications clear. (Optional) Specify SNS notifications to a preconfigured SNS Topic on all events or a specific event type for either the entire command or on a per-instance basis. In the Parameters section, under Operation , select Install . Choose Register Run command task to complete the task definition and return to the details page.","title":"5.4 Assigning Tasks to Your Patch Maintenance Window"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#55-review-maintenance-window-execution","text":"After allowing enough time for your maintenance window to complete: Navigte to the AWS Systems Manager console . Choose Maintenance Windows , and then select the Window ID for your new maintenance window. On the Maintenance window ID details page, choose History . Select a Windows execution ID and choose View details . On the Command ID details page, scroll down to the Targets and outputs section, select an Instance ID , and choose View output . Choose Step 1 - Output and review the output. Choose Step 2 - Output and review the output. You have now configured a maintenance window, assigned targets, assigned tasks, and validated successful execution. The same procedures can be used to schedule the execution of any AWS Systems Manager Document .","title":"5.5 Review Maintenance Window Execution"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#bonus-content-creating-a-simple-notification-service-topic","text":"Amazon Simple Notification Service (Amazon SNS) coordinates and manages the delivery or sending of messages to subscribing endpoints or clients. In Amazon SNS, there are two types of clients: publishers and subscribers. These are also referred to as producers and consumers. Publishers communicate asynchronously with subscribers by producing and sending a message to a topic, which is a logical access point and communication channel. Subscribers (i.e., web servers, email addresses, Amazon SQS queues, AWS Lambda functions) consume or receive the message or notification over one of the supported protocols (i.e., Amazon SQS, HTTP/S, email, SMS, Lambda) when they are subscribed to the topic.","title":"Bonus Content: Creating a Simple Notification Service Topic"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#61-create-and-subscribe-to-an-sns-topic","text":"To create and subscribe to an SNS topic: Navigate to the SNS console at https://console.aws.amazon.com/sns/ . Choose Create topic . In the Create new topic window: In the Topic name field, enter AdminAlert . In the Display name field, enter AdminAlert . Choose Create topic . On the Topic details: AdminAlert page, choose Create subscription . In the Create subscription window: Select Email from the Protocol list. Enter your email address in the Endpoint field. Choose Create subscription . You will receive an email request for confirmation. Your Subscription ID will remain PendingConfirmation until you confirm your subscription by clicking through the link to Confirm subscription in the email. Refresh the page after confirming your subscription to view the populated Subscription ARN . You can now use this SNS topic to send notifications to your Administrator user.","title":"6.1 Create and Subscribe to an SNS Topic"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#7-removing-lab-resources","text":"Note When the lab is complete, remove the resources you created. Otherwise you will be charged for any resources that are not covered in the AWS Free Tier.","title":"7 Removing Lab Resources"},{"location":"Operations/100_Inventory_and_Patch_Mgmt/Lab_Guide.html#71-remove-resources-created-with-cloudformation","text":"Navigate to the CloudFormation dashboard at https://console.aws.amazon.com/cloudformation/ : Select your first stack. Choose Actions and choose delete stack . Select your second stack. Choose Actions and choose delete stack . Navigate to Systems Manager console at https://console.aws.amazon.com/systems-manager/ : Choose State Manager . Select the association you created. Choose Delete . If you created an S3 bucket to store detailed output, delete the bucket and associated data: Navigate to the S3 console https://s3.console.aws.amazon.com/s3/ . Select the bucket. Choose Delete and provide the bucket name to confirm deletion. If you created the optional SNS Topic , delete the SNS topic: Navigate to the SNS console https://console.aws.amazon.com/sns/ . Select your AdminAlert SNS topic from the list. Choose Actions and select Delete topics . If you created a Maintenance Window , delete the Maintenance Window: Navigate to the Systems Manager console at https://console.aws.amazon.com/systems-manager/ . Choose Maintenance Windows . Select the maintenance window you created. Choose Delete . In the Delete maintenance window window, choose Delete . If you do not intend to continue to use the Administrator account you created, delete the account: Navigate to the IAM console at https://console.aws.amazon.com/iam/ . Choose Users . Select your user from the list. Choose Delete user . Select the check box next to \"One or more of these users have recently accessed AWS. Deleting them could affect running systems. Check the box to confirm that you want to delete these users.\". Choose Yes, delete . When next you navigate within the console you will be returned to the account login page. If you do intend to continue to use the Administrator account you created, we strongly suggest you enable MFA . Thank you for using this lab.","title":"7.1 Remove resources created with CloudFormation"},{"location":"Performance/README.html","text":"AWS Well-Architected Performance Efficiency Labs Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about performance efficiency read the AWS Well-Architected Performance Efficiency whitepaper or online https://wa.aws.amazon.com/ . Labs Level 100: Monitoring with CloudWatch Dashboards License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Performance/README.html#aws-well-architected-performance-efficiency-labs","text":"","title":"AWS Well-Architected Performance Efficiency Labs"},{"location":"Performance/README.html#introduction","text":"This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about performance efficiency read the AWS Well-Architected Performance Efficiency whitepaper or online https://wa.aws.amazon.com/ .","title":"Introduction"},{"location":"Performance/README.html#labs","text":"Level 100: Monitoring with CloudWatch Dashboards","title":"Labs"},{"location":"Performance/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/README.html","text":"Level 100: Monitoring with CloudWatch Dashboards Introduction This hands-on lab will guide you through configuring an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of all AWS resources. This enables you to quickly get started with monitoring, explore account and resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues. You can find more best practices by reading the Performance Efficiency Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Monitor resources to ensure they are performing as expected Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Monitoring with CloudWatch Dashboards"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/README.html#level-100-monitoring-with-cloudwatch-dashboards","text":"","title":"Level 100: Monitoring with CloudWatch Dashboards"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/README.html#introduction","text":"This hands-on lab will guide you through configuring an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of all AWS resources. This enables you to quickly get started with monitoring, explore account and resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues. You can find more best practices by reading the Performance Efficiency Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/README.html#goals","text":"Monitor resources to ensure they are performing as expected","title":"Goals"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/Lab_Guide.html","text":"Level 100: Monitoring with CloudWatch Dashboards Authors Ben Potter, Security Lead, Well-Architected 1. View Amazon CloudWatch Automatic Dashboards Amazon CloudWatch Automatic Dashboards allow you to easily monitor all AWS Resources, and is quick to get started. Explore account and resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues. Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region from the top menu bar. The upper left shows a list of AWS services you use in your account, along with the state of alarms in those services. The upper right shows two or four alarms in your account, depending on how many AWS services you use. The alarms shown are those in the ALARM state or those that most recently changed state. These upper areas enable you to assess the health of your AWS services, by seeing the alarm states in every service and the alarms that most recently changed state. This helps you monitor and quickly diagnose issues. Below these areas is the custom dashboard that you have created and named CloudWatch-Default, if any. This is a convenient way for you to add metrics about your own custom services or applications to the overview page, or to bring forward additional key metrics from AWS services that you most want to monitor. If you use six or more AWS services, below the default dashboard is a link to the automatic cross-service dashboard. The cross-service dashboard automatically displays key metrics from every AWS service you use, without requiring you to choose what metrics to monitor or create custom dashboards. You can also use it to drill down to any AWS service and see even more key metrics for that service. If you use fewer than six AWS services, the cross-service dashboard is shown automatically on this page. References & useful resources See Key Metrics From All AWS Services Focus on Metrics and Alarms in a Single AWS Service Focus on Metrics and Alarms in a Resource Group License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 100: Monitoring with CloudWatch Dashboards"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/Lab_Guide.html#level-100-monitoring-with-cloudwatch-dashboards","text":"","title":"Level 100: Monitoring with CloudWatch Dashboards"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/Lab_Guide.html#1-view-amazon-cloudwatch-automatic-dashboards","text":"Amazon CloudWatch Automatic Dashboards allow you to easily monitor all AWS Resources, and is quick to get started. Explore account and resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues. Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region from the top menu bar. The upper left shows a list of AWS services you use in your account, along with the state of alarms in those services. The upper right shows two or four alarms in your account, depending on how many AWS services you use. The alarms shown are those in the ALARM state or those that most recently changed state. These upper areas enable you to assess the health of your AWS services, by seeing the alarm states in every service and the alarms that most recently changed state. This helps you monitor and quickly diagnose issues. Below these areas is the custom dashboard that you have created and named CloudWatch-Default, if any. This is a convenient way for you to add metrics about your own custom services or applications to the overview page, or to bring forward additional key metrics from AWS services that you most want to monitor. If you use six or more AWS services, below the default dashboard is a link to the automatic cross-service dashboard. The cross-service dashboard automatically displays key metrics from every AWS service you use, without requiring you to choose what metrics to monitor or create custom dashboards. You can also use it to drill down to any AWS service and see even more key metrics for that service. If you use fewer than six AWS services, the cross-service dashboard is shown automatically on this page.","title":"1. View Amazon CloudWatch Automatic Dashboards "},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/Lab_Guide.html#references-useful-resources","text":"See Key Metrics From All AWS Services Focus on Metrics and Alarms in a Single AWS Service Focus on Metrics and Alarms in a Resource Group","title":"References &amp; useful resources"},{"location":"Performance/100_Monitoring_with_CloudWatch_Dashboards/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Reliability/README.html","text":"AWS Well-Architected Reliability Labs https://wellarchitectedlabs.com Introduction This repository contains documentation and code in the format of hands-on-labs to help you learn how to learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about Reliability, read the AWS Well-Architected Reliability whitepaper . Labs Level 200: Testing for Resiliency of EC2 Level 200: Testing Backup and Restore of Data Level 300: Testing for Resiliency of EC2, RDS, and S3 Level 300: Implementing Health Checks and Managing Dependencies to Improve Reliability License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Reliability/README.html#aws-well-architected-reliability-labs","text":"https://wellarchitectedlabs.com","title":"AWS Well-Architected Reliability Labs"},{"location":"Reliability/README.html#introduction","text":"This repository contains documentation and code in the format of hands-on-labs to help you learn how to learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about Reliability, read the AWS Well-Architected Reliability whitepaper .","title":"Introduction"},{"location":"Reliability/README.html#labs","text":"Level 200: Testing for Resiliency of EC2 Level 200: Testing Backup and Restore of Data Level 300: Testing for Resiliency of EC2, RDS, and S3 Level 300: Implementing Health Checks and Managing Dependencies to Improve Reliability","title":"Labs"},{"location":"Reliability/README.html#license","text":"","title":"License"},{"location":"Reliability/README.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/README.html#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/README.html","text":"Level 200: Backup and Restore of Data https://wellarchitectedlabs.com Introduction In this lab, you will become familiar with creating periodic backups of AWS resources such as EBS Volumes, RDS Databases, DynamoDB Tables, and EFS File Systems. You will learn how to restore data from backups, and how to automate the entire process. It is not sufficient to just create backups of data sources, you must also test these backups to ensure they can be used to recover data. A backup is useless if you are unable to restore your data from it. Testing the restore process after each backup will ensure you are aware of any issues that might arise during a restore down the line. In this lab, you will create an EBS Volume as a data source. You will then create a strategy to backup these data sources periodically using AWS Backup, and finally, automate the testing of the restore process as well as cleanup of resources using AWS Lambda. The skills you learn will help you define a backup and restore plan in alignment with the AWS Well-Architected Framework Goals: Create a Backup Strategy to ensure mission-critical data is being backed up regularly Test restoring from backups to ensure there are no data recovery issues Learn how to automate this process Prerequisites: An AWS Account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account that has Administrator privileges. Launch the CloudFormation Stack to provision resources that will act as data sources. NOTE : You will be billed for any applicable AWS resources used as part of this lab, that are not covered in the AWS Free Tier. Start the Lab! License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/README.html#level-200-backup-and-restore-of-data","text":"https://wellarchitectedlabs.com","title":"Level 200: Backup and Restore of Data"},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/README.html#introduction","text":"In this lab, you will become familiar with creating periodic backups of AWS resources such as EBS Volumes, RDS Databases, DynamoDB Tables, and EFS File Systems. You will learn how to restore data from backups, and how to automate the entire process. It is not sufficient to just create backups of data sources, you must also test these backups to ensure they can be used to recover data. A backup is useless if you are unable to restore your data from it. Testing the restore process after each backup will ensure you are aware of any issues that might arise during a restore down the line. In this lab, you will create an EBS Volume as a data source. You will then create a strategy to backup these data sources periodically using AWS Backup, and finally, automate the testing of the restore process as well as cleanup of resources using AWS Lambda. The skills you learn will help you define a backup and restore plan in alignment with the AWS Well-Architected Framework","title":"Introduction"},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/README.html#goals","text":"Create a Backup Strategy to ensure mission-critical data is being backed up regularly Test restoring from backups to ensure there are no data recovery issues Learn how to automate this process","title":"Goals:"},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/README.html#prerequisites","text":"An AWS Account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account that has Administrator privileges. Launch the CloudFormation Stack to provision resources that will act as data sources. NOTE : You will be billed for any applicable AWS resources used as part of this lab, that are not covered in the AWS Free Tier.","title":"Prerequisites:"},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/README.html#license","text":"","title":"License"},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/README.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/README.html#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/Lab_Guide.html","text":"Level 200: Testing Backup and Restore of Data Authors Mahanth Jayadeva, Solutions Architect, Well-Architected Table of Contents Deploy the Infrastructure Create Backup Plan Enable Notifications Test Restore Tear Down For many organizations, the data that they possess is one of the most valuable assets they have. Backing up data frequently is of vital importance for the long lasting success of any organization. However, a backup of data is only valuable if data can be recovered/restored from the backup. In the cloud, backing up data and testing the restore is easier compared to on-premises datacenters. Automating this process with appropriate notification systems will ensure that an organization's data is backed up frequently, the backups are tested to ensure expected recovery, and the appropriate people are notified in case of failures. 1. Setup You will use AWS CloudFormation to provision some resources needed for this lab. As part of this lab, the CloudFormation stack that you provision will create an EBS Volume, an SNS Topic, and a Lambda Function. This lab will only work in us-east-1. Click here to deploy the stack. Under PARAMETERS , specify an email address that you have access to for NotificationEmail. Check the box I acknowledge that AWS CloudFormation might create IAM resources. Click CREATE / CREATE STACK. MANUAL STEPS Use your administrator account to access the CloudFormation console - https://console.aws.amazon.com/cloudformation/ . Click on CREATE STACK . Under PREREQUISITE - PREPARE TEMPLATE , select the option TEMPLATE IS READY . Under SPECIFY TEMPLATE , select the option AMAZON S3 URL , enter the link - https://c3pio.s3.amazonaws.com/backup-lab/backup-lab.yaml and click NEXT . Enter a STACK NAME such as WA - BACKUP-LAB . For NotificationEmail, specify an email address that you have access to. Leave default values for the rest of the fields and click NEXT . No changes are needed on the CONFIGURE STACK OPTIONS page, click NEXT . Review the details of the stack, scroll down to CAPABILITIES, and check the box next to I acknowledge that AWS CloudFormation might create IAM resources. Click CREATE STACK . Note: Once stack creation starts, monitor the email address you entered. You should receive an email from SNS with the subject AWS Notification - Subscription Confirmation. Click on the link Confirm subscription to confirm the subscription of your email to the SNS Topic. The stack takes about 3 minutes to create all the resources. Periodically refresh the page until you see that the STACK STATUS is in CREATE_COMPLETE . Once the stack is in CREATE_COMPLETE , visit the OUTPUTS section for the stack and note down the KEY and VALUE for each of the outputs. This information will be used later in the lab. 2. Create a Backup Strategy A well thought out backup strategy is key to an organization's success and is determined by a variety of factors. The biggest factors influencing a backup strategy is the Recovery Time Objective (RTO) and Recovery Point Objective (RPO) set for the workload. RTO and RPO are determined based on the criticality of the workload to the business, the SLAs that have been agreed upon, and the cost associated with achieving the RTO and RPO. RTO and RPO should be specific to each workload and not set for the entire organization/infrastructure. In this lab, you will create a backup strategy by leveraging AWS Backup, a fully managed backup service that can automatically backup data from EBS Volumes, RDS Databases, DynamoDB Tables, EFS File Systems, EC2 Instances, and Storage Gateways. Sign in to the AWS Backup console - https://us-east-1.console.aws.amazon.com/backup/home?region=us-east-1#backupplan . Choose CREATE BACKUP PLAN . Select the option to BUILD A NEW PLAN . Specify a Backup plan name such as BACKUP-LAB . Under BACKUP RULE CONFIGURATION , enter a RULE NAME such as BACKUP-LAB-RULE . To set a SCHEDULE for the backup, you can specify the FREQUENCY at which backups are taken. You can enter frequency as every 12 hours, Daily, Weekly, or Monthly. Alternatively, you can specify a custom CRON EXPRESSION for your backup frequency. For this exercise, select the FREQUENCY as DAILY. Once frequency has been established, you will need to specify a backup window - a period of time during which data is being backed up from your data sources. Keep in mind that creating backups could cause you data sources to be temporarily unavailable depending on the underlying configuration of those resources. It is best to create backups during scheduled downtimes/maintenance windows when user impact is minimal. For this exercise, select Use backup window defaults - *recommended. * The default backup window is set to start at 5 AM UTC time and last 8 hours. If you need to schedule backups at a different time, you can customize the backup window. You can set lifecycle policies for your backups to transition them to cold storage or to expire them after a period of time to reduce costs and operational overhead. This is currently supported for backups of EFS only. For this exercise, set the values for Transition to cold storage and Expire both to NEVER. Under BACKUP VAULT , click on CREATE NEW BACKUP VAULT . It is recommended to use different Backup Vaults for different workloads. On the pop-up screen, specify a BACKUP VAULT NAME such as BACKUP-LAB-VAULT. You can choose to encrypt your backups for additional security by specifying a KMS key. You can choose the default key created and managed by AWS Backup or specify your own custom key. For this exercise, select the default key (default) aws/backup . Click CREATE BACKUP VAULT . Additionally, you can choose to have your backups automatically copied to a different AWS Region. You can add tags to your recovery points to help identify them. Click CREATE PLAN . Once the backup plan and the backup rule has been created, you can specify resources to back up. You can select individual resources to be backed up, or specify a tag (key-value) associated with the resource. AWS Backup will execute backup jobs on all resources that match the tags specified. Click on BACKUP PLANS from the menu on the left side of the screen. Select the backup plan BACKUP-LAB that you just created. Scroll down to the section titled RESOURCE ASSIGNMENTS and click on ASSIGN RESOURCES . Specify a RESOURCE ASSIGNMENT NAME such as BACKUP-RESOURCES to help identify the resources that are being backed up. Leave the DEFAULT ROLE selected for IAM ROLE . If a role does not already exist, the AWS Backup service will create one with the necessary permissions. Under ASSIGN RESOURCES , you can specify resources to be backed up individually by specifying the RESOURCE TYPE and RESOURCE ID , or select TAGS and enter the TAG KEY and the TAG VALUE . For this lab, select TAGS as the value for ASSIGN BY , and enter workload as the KEY and myapp as the VALUE . This tag and value was created by the CloudFormation stack. Remember that tags are case sensitive and ensure that the values you enter are all in lower case. Click on ASSIGN RESOURCES . You have successfully created a backup plan for your data sources, and all supported resources with the tags WORKLOAD-MYAPP will be backed up automatically, at the frequency specified. In case of a disaster, these backups can be used to recover data to ensure business continuity. Since the entire process is automated, it will save considerable operational overhead for your Operations teams. 3. Enable Notifications In the cloud, setting up notifications to be aware of events within your workload is easily achieved. AWS Backup leverages AWS SNS to send notifications related to backup activities that are occurring. This will allow visibility into backup job statuses, restore job statuses, or any failures that may have occurred, allowing your Operations teams to respond appropriately. Open a terminal where you have access to the AWS CLI. Ensure that the CLI is up to date and that you have AWS Administrator Permissions to run AWS CLI commands. https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html Edit the following AWS CLI command and include the ARN of the SNS TOPIC that you created. Replace with the ARN of the SNS TOPIC obtained from the outputs section of the CloudFormation Stack. Note that the backup vault name is case sensitive. aws backup put-backup-vault-notifications --region us-east-1 --backup-vault-name BACKUP-LAB-VAULT --backup-vault-events BACKUP_JOB_COMPLETED RESTORE_JOB_COMPLETED --sns-topic-arn <YOUR SNS TOPIC ARN> Once edited, run the above command, it will enable notifications with messages published to the SNS TOPIC every time a backup or restore job is completed. This will ensure the Operations team is aware of any failures with backing up or restoring data. You can verify that notifications have been enabled by running the following command. The output will include a section called SNSTopicArn followed by the ARN of the SNS Topic that was created as part of the lab. aws backup get-backup-vault-notifications --backup-vault-name BACKUP-LAB-VAULT --region us-east-1 You have now successfully enabled notifications for the backup vault BACKUP-LAB-VAULT, ensuring that the Operations team is aware of all backup and restore activities involving this vault, and any failures associated with those activities. 4. Create a Restore Process A backup of a data source is useful only if data can be restored from it. If backups aren't tested, you might find yourself in a situation where your workload has been impacted by an event and you need to recover data from your backups, but the backups are faulty and restoring data is no longer feasible, or exceeds your RTO. To avoid such situations, backups taken should always be tested to ensure they can be used to recover data. In this lab, you will leverage AWS Lambda to automatically test all backups created to ensure recovery is successful, and clean up any resources that were created during the restore test process to save on cost. This will ensure you are aware of any faulty backups that might be unusable to recover data from. Automating this process with notifications enabled will ensure there is minimal operational overhead and that the Operations teams are aware of backup and restore statuses. Testing Recovery For the purpose of this lab, we will simulate the action performed by AWS Backup when creating backups of data sources by creating an on-demand backup to see if the backup is successful. Once the backup is completed, you will receive a notification stating that the backup job has completed and the lambda function will get invoked. The Lambda function will make API calls to start restoring data from the backup that was created. This will help ascertain that the backup is good. Once the restore process has been completed, you will receive another notification confirming this, and the lambda function will get invoked again to clean up new resources that were created as part of the restore. Once the cleanup has been completed, you will receive one last notification confirming cleanup. Use your administrator account to access the AWS Backup console - https://us-east-1.console.aws.amazon.com/backup/home?region=us-east-1#home Click on CREATE AN ON-DEMAND BACKUP in the middle of the screen. Under RESOURCE TYPE , select EBS . Paste in the Volume ID obtained from the Output of the CloudFormation Stack. Under BACKUP WINDOW , ensure that the CREATE BACKUP NOW option is selected. Under EXPIRE , select the option DAYS AFTER CREATION and enter 1 for the value for this lab. This will ensure that the backup is deleted after 1 day. Under Backup Vault , select the BACKUP-LAB-VAULT . Leave the default IAM role selected. Click CREATE ON-DEMAND BACKUP . Click on JOBS from the menu on the left and select BACKUP JOBS . You should see a new backup job started with the status of RUNNING . Click on the RESTORE JOBS tab, there shouldn't be any restore jobs running. Periodically refresh the console until the STATUS changes to COMPLETED . It should take about 5-10 minutes to complete. While waiting for the job to finish and the notification to go out, you can review the lambda function code here to understand what the lambda function is doing. After the job is completed, click on the JOB ID and view the DETAILS . You should see the Recovery Point ARN that was created, the RESOURCE ID for which the backup was created, and the RESOURCE TYPE for which the backup was created. Monitor your email to see if you receive a Notification from AWS Backup . Compare details in the email to what you see on the AWS Console, they should match. It takes about 10 mins for the email to show up once the backup job has completed. Go back to JOBS and switch to the RESTORE JOBS tab. You should see a RESTORE JOB running. The lambda function that was created as part of this lab has requested a restore job to ensure that the backup can be used to recover from. Periodically refresh the console until the STATUS changes to COMPLETED . It should take about 5-10 minutes to complete. After the job is completed, click on the JOB ID and view the DETAILS . You should see the Recovery Point ARN from which the restore was tested, the RESOURCE ID of the newly created EBS Volume, and the RESOURCE TYPE for which the restore was created. Note down the RESOURCE ID of the newly created EBS Volume and verify that it exists from the EC2 Console - https://console.aws.amazon.com/ec2/home?region=us-east-1#Volumes:sort=size Monitor your email to see if you have received a Notification from AWS Backup confirming the restore job was successful. Compare details in the email to what you see on the AWS Console, they should match. It takes about 10 mins for the email to show up once the restore job has completed. Once it is established that the restore was successful, it is time to delete the new resource that was created to prevent unnecessary spend. This process is also automated using AWS Lambda. Monitor your email to see if you have received a Restore Test Status notification confirming the deletion of the newly created resource. Check the EC2 Console to verify that the new EBS Volume has been deleted - https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#Volumes:sort=size Use your administrator account to access the AWS CloudWatch console - https://console.aws.amazon.com/cloudwatch/home?region=us-east-1 Click on LOGS from the menu on the left side. For filter, paste the following string. /aws/lambda/RestoreTestFunction Click on the LOG STREAM and view the output of the Lambda function's execution. Review of Best Practices Implemented Identify all data that needs to be backed up and perform backups or reproduce the data from sources: Back up important data using Amazon S3, Amazon EBS snapshots, or third-party software. Alternatively, if the data can be reproduced from sources to meet RPO, you may not require a backup. Perform data backup automatically or reproduce the data from sources automatically: Automate backups or the reproduction from sources using AWS features (for example, snapshots of Amazon RDS and Amazon EBS, versions on Amazon S3, etc.), AWS Marketplace solutions, or third-party solutions. Perform periodic recovery of the data to verify backup integrity and processes: Validate that your backup process implementation meets Recovery Time Objective and Recovery Point Objective through a recovery test. 5. Lab Cleanup The following instructions will remove the resources that you have created in this lab. Cleaning up AWS Backup Resources Sign in to the AWS Management Console and navigate to the AWS Backup console - https://us-east-1.console.aws.amazon.com/backup/home?region=us-east-1#home Click on BACKUP VAULTS from the menu on the left side, and select BACKUP-LAB-VAULT . Under the section BACKUPS , delete all the RECOVERY POINTS . Once all the RECOVERY POINTS have been deleted, delete the Backup Vault by clicking on DELETE on the top right hand corner. Click on BACKUP PLANS from the menu on the left side, and select BACKUP-LAB . Scroll down to the section RESOURCE ASSIGNMENTS , and delete the resource assignment. Delete the BACKUP PLAN by clicking on DELETE on the upper right corner of the screen. Cleaning up the CloudFormation Stack Sign in to the AWS Management Console and navigate to the AWS CloudFormation console - https://console.aws.amazon.com/cloudformation/ Select the stack WA-BACKUP-LAB , and delete the stack. Cleaning up the CloudWatch Logs Sign in to the AWS Management Console, and open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/ . Click Logs in the left navigation. Click the radio button on the left of the /aws/lambda/RestoreTestFunction . Click the Actions Button then click Delete Log Group . Verify the log group name then click Yes, Delete . Thank you for using this lab.","title":"Lab Guide"},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/Lab_Guide.html#level-200-testing-backup-and-restore-of-data","text":"","title":"Level 200: Testing Backup and Restore of Data"},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/Lab_Guide.html#authors","text":"Mahanth Jayadeva, Solutions Architect, Well-Architected","title":"Authors"},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/Lab_Guide.html#table-of-contents","text":"Deploy the Infrastructure Create Backup Plan Enable Notifications Test Restore Tear Down For many organizations, the data that they possess is one of the most valuable assets they have. Backing up data frequently is of vital importance for the long lasting success of any organization. However, a backup of data is only valuable if data can be recovered/restored from the backup. In the cloud, backing up data and testing the restore is easier compared to on-premises datacenters. Automating this process with appropriate notification systems will ensure that an organization's data is backed up frequently, the backups are tested to ensure expected recovery, and the appropriate people are notified in case of failures.","title":"Table of Contents"},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/Lab_Guide.html#1-setup","text":"You will use AWS CloudFormation to provision some resources needed for this lab. As part of this lab, the CloudFormation stack that you provision will create an EBS Volume, an SNS Topic, and a Lambda Function. This lab will only work in us-east-1. Click here to deploy the stack. Under PARAMETERS , specify an email address that you have access to for NotificationEmail. Check the box I acknowledge that AWS CloudFormation might create IAM resources. Click CREATE / CREATE STACK. MANUAL STEPS Use your administrator account to access the CloudFormation console - https://console.aws.amazon.com/cloudformation/ . Click on CREATE STACK . Under PREREQUISITE - PREPARE TEMPLATE , select the option TEMPLATE IS READY . Under SPECIFY TEMPLATE , select the option AMAZON S3 URL , enter the link - https://c3pio.s3.amazonaws.com/backup-lab/backup-lab.yaml and click NEXT . Enter a STACK NAME such as WA - BACKUP-LAB . For NotificationEmail, specify an email address that you have access to. Leave default values for the rest of the fields and click NEXT . No changes are needed on the CONFIGURE STACK OPTIONS page, click NEXT . Review the details of the stack, scroll down to CAPABILITIES, and check the box next to I acknowledge that AWS CloudFormation might create IAM resources. Click CREATE STACK . Note: Once stack creation starts, monitor the email address you entered. You should receive an email from SNS with the subject AWS Notification - Subscription Confirmation. Click on the link Confirm subscription to confirm the subscription of your email to the SNS Topic. The stack takes about 3 minutes to create all the resources. Periodically refresh the page until you see that the STACK STATUS is in CREATE_COMPLETE . Once the stack is in CREATE_COMPLETE , visit the OUTPUTS section for the stack and note down the KEY and VALUE for each of the outputs. This information will be used later in the lab.","title":"1. Setup "},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/Lab_Guide.html#2-create-a-backup-strategy","text":"A well thought out backup strategy is key to an organization's success and is determined by a variety of factors. The biggest factors influencing a backup strategy is the Recovery Time Objective (RTO) and Recovery Point Objective (RPO) set for the workload. RTO and RPO are determined based on the criticality of the workload to the business, the SLAs that have been agreed upon, and the cost associated with achieving the RTO and RPO. RTO and RPO should be specific to each workload and not set for the entire organization/infrastructure. In this lab, you will create a backup strategy by leveraging AWS Backup, a fully managed backup service that can automatically backup data from EBS Volumes, RDS Databases, DynamoDB Tables, EFS File Systems, EC2 Instances, and Storage Gateways. Sign in to the AWS Backup console - https://us-east-1.console.aws.amazon.com/backup/home?region=us-east-1#backupplan . Choose CREATE BACKUP PLAN . Select the option to BUILD A NEW PLAN . Specify a Backup plan name such as BACKUP-LAB . Under BACKUP RULE CONFIGURATION , enter a RULE NAME such as BACKUP-LAB-RULE . To set a SCHEDULE for the backup, you can specify the FREQUENCY at which backups are taken. You can enter frequency as every 12 hours, Daily, Weekly, or Monthly. Alternatively, you can specify a custom CRON EXPRESSION for your backup frequency. For this exercise, select the FREQUENCY as DAILY. Once frequency has been established, you will need to specify a backup window - a period of time during which data is being backed up from your data sources. Keep in mind that creating backups could cause you data sources to be temporarily unavailable depending on the underlying configuration of those resources. It is best to create backups during scheduled downtimes/maintenance windows when user impact is minimal. For this exercise, select Use backup window defaults - *recommended. * The default backup window is set to start at 5 AM UTC time and last 8 hours. If you need to schedule backups at a different time, you can customize the backup window. You can set lifecycle policies for your backups to transition them to cold storage or to expire them after a period of time to reduce costs and operational overhead. This is currently supported for backups of EFS only. For this exercise, set the values for Transition to cold storage and Expire both to NEVER. Under BACKUP VAULT , click on CREATE NEW BACKUP VAULT . It is recommended to use different Backup Vaults for different workloads. On the pop-up screen, specify a BACKUP VAULT NAME such as BACKUP-LAB-VAULT. You can choose to encrypt your backups for additional security by specifying a KMS key. You can choose the default key created and managed by AWS Backup or specify your own custom key. For this exercise, select the default key (default) aws/backup . Click CREATE BACKUP VAULT . Additionally, you can choose to have your backups automatically copied to a different AWS Region. You can add tags to your recovery points to help identify them. Click CREATE PLAN . Once the backup plan and the backup rule has been created, you can specify resources to back up. You can select individual resources to be backed up, or specify a tag (key-value) associated with the resource. AWS Backup will execute backup jobs on all resources that match the tags specified. Click on BACKUP PLANS from the menu on the left side of the screen. Select the backup plan BACKUP-LAB that you just created. Scroll down to the section titled RESOURCE ASSIGNMENTS and click on ASSIGN RESOURCES . Specify a RESOURCE ASSIGNMENT NAME such as BACKUP-RESOURCES to help identify the resources that are being backed up. Leave the DEFAULT ROLE selected for IAM ROLE . If a role does not already exist, the AWS Backup service will create one with the necessary permissions. Under ASSIGN RESOURCES , you can specify resources to be backed up individually by specifying the RESOURCE TYPE and RESOURCE ID , or select TAGS and enter the TAG KEY and the TAG VALUE . For this lab, select TAGS as the value for ASSIGN BY , and enter workload as the KEY and myapp as the VALUE . This tag and value was created by the CloudFormation stack. Remember that tags are case sensitive and ensure that the values you enter are all in lower case. Click on ASSIGN RESOURCES . You have successfully created a backup plan for your data sources, and all supported resources with the tags WORKLOAD-MYAPP will be backed up automatically, at the frequency specified. In case of a disaster, these backups can be used to recover data to ensure business continuity. Since the entire process is automated, it will save considerable operational overhead for your Operations teams.","title":"2. Create a Backup Strategy "},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/Lab_Guide.html#3-enable-notifications","text":"In the cloud, setting up notifications to be aware of events within your workload is easily achieved. AWS Backup leverages AWS SNS to send notifications related to backup activities that are occurring. This will allow visibility into backup job statuses, restore job statuses, or any failures that may have occurred, allowing your Operations teams to respond appropriately. Open a terminal where you have access to the AWS CLI. Ensure that the CLI is up to date and that you have AWS Administrator Permissions to run AWS CLI commands. https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html Edit the following AWS CLI command and include the ARN of the SNS TOPIC that you created. Replace with the ARN of the SNS TOPIC obtained from the outputs section of the CloudFormation Stack. Note that the backup vault name is case sensitive. aws backup put-backup-vault-notifications --region us-east-1 --backup-vault-name BACKUP-LAB-VAULT --backup-vault-events BACKUP_JOB_COMPLETED RESTORE_JOB_COMPLETED --sns-topic-arn <YOUR SNS TOPIC ARN> Once edited, run the above command, it will enable notifications with messages published to the SNS TOPIC every time a backup or restore job is completed. This will ensure the Operations team is aware of any failures with backing up or restoring data. You can verify that notifications have been enabled by running the following command. The output will include a section called SNSTopicArn followed by the ARN of the SNS Topic that was created as part of the lab. aws backup get-backup-vault-notifications --backup-vault-name BACKUP-LAB-VAULT --region us-east-1 You have now successfully enabled notifications for the backup vault BACKUP-LAB-VAULT, ensuring that the Operations team is aware of all backup and restore activities involving this vault, and any failures associated with those activities.","title":"3. Enable Notifications "},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/Lab_Guide.html#4-create-a-restore-process","text":"A backup of a data source is useful only if data can be restored from it. If backups aren't tested, you might find yourself in a situation where your workload has been impacted by an event and you need to recover data from your backups, but the backups are faulty and restoring data is no longer feasible, or exceeds your RTO. To avoid such situations, backups taken should always be tested to ensure they can be used to recover data. In this lab, you will leverage AWS Lambda to automatically test all backups created to ensure recovery is successful, and clean up any resources that were created during the restore test process to save on cost. This will ensure you are aware of any faulty backups that might be unusable to recover data from. Automating this process with notifications enabled will ensure there is minimal operational overhead and that the Operations teams are aware of backup and restore statuses.","title":"4. Create a Restore Process "},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/Lab_Guide.html#testing-recovery","text":"For the purpose of this lab, we will simulate the action performed by AWS Backup when creating backups of data sources by creating an on-demand backup to see if the backup is successful. Once the backup is completed, you will receive a notification stating that the backup job has completed and the lambda function will get invoked. The Lambda function will make API calls to start restoring data from the backup that was created. This will help ascertain that the backup is good. Once the restore process has been completed, you will receive another notification confirming this, and the lambda function will get invoked again to clean up new resources that were created as part of the restore. Once the cleanup has been completed, you will receive one last notification confirming cleanup. Use your administrator account to access the AWS Backup console - https://us-east-1.console.aws.amazon.com/backup/home?region=us-east-1#home Click on CREATE AN ON-DEMAND BACKUP in the middle of the screen. Under RESOURCE TYPE , select EBS . Paste in the Volume ID obtained from the Output of the CloudFormation Stack. Under BACKUP WINDOW , ensure that the CREATE BACKUP NOW option is selected. Under EXPIRE , select the option DAYS AFTER CREATION and enter 1 for the value for this lab. This will ensure that the backup is deleted after 1 day. Under Backup Vault , select the BACKUP-LAB-VAULT . Leave the default IAM role selected. Click CREATE ON-DEMAND BACKUP . Click on JOBS from the menu on the left and select BACKUP JOBS . You should see a new backup job started with the status of RUNNING . Click on the RESTORE JOBS tab, there shouldn't be any restore jobs running. Periodically refresh the console until the STATUS changes to COMPLETED . It should take about 5-10 minutes to complete. While waiting for the job to finish and the notification to go out, you can review the lambda function code here to understand what the lambda function is doing. After the job is completed, click on the JOB ID and view the DETAILS . You should see the Recovery Point ARN that was created, the RESOURCE ID for which the backup was created, and the RESOURCE TYPE for which the backup was created. Monitor your email to see if you receive a Notification from AWS Backup . Compare details in the email to what you see on the AWS Console, they should match. It takes about 10 mins for the email to show up once the backup job has completed. Go back to JOBS and switch to the RESTORE JOBS tab. You should see a RESTORE JOB running. The lambda function that was created as part of this lab has requested a restore job to ensure that the backup can be used to recover from. Periodically refresh the console until the STATUS changes to COMPLETED . It should take about 5-10 minutes to complete. After the job is completed, click on the JOB ID and view the DETAILS . You should see the Recovery Point ARN from which the restore was tested, the RESOURCE ID of the newly created EBS Volume, and the RESOURCE TYPE for which the restore was created. Note down the RESOURCE ID of the newly created EBS Volume and verify that it exists from the EC2 Console - https://console.aws.amazon.com/ec2/home?region=us-east-1#Volumes:sort=size Monitor your email to see if you have received a Notification from AWS Backup confirming the restore job was successful. Compare details in the email to what you see on the AWS Console, they should match. It takes about 10 mins for the email to show up once the restore job has completed. Once it is established that the restore was successful, it is time to delete the new resource that was created to prevent unnecessary spend. This process is also automated using AWS Lambda. Monitor your email to see if you have received a Restore Test Status notification confirming the deletion of the newly created resource. Check the EC2 Console to verify that the new EBS Volume has been deleted - https://console.aws.amazon.com/ec2/v2/home?region=us-east-1#Volumes:sort=size Use your administrator account to access the AWS CloudWatch console - https://console.aws.amazon.com/cloudwatch/home?region=us-east-1 Click on LOGS from the menu on the left side. For filter, paste the following string. /aws/lambda/RestoreTestFunction Click on the LOG STREAM and view the output of the Lambda function's execution.","title":"Testing Recovery"},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/Lab_Guide.html#review-of-best-practices-implemented","text":"Identify all data that needs to be backed up and perform backups or reproduce the data from sources: Back up important data using Amazon S3, Amazon EBS snapshots, or third-party software. Alternatively, if the data can be reproduced from sources to meet RPO, you may not require a backup. Perform data backup automatically or reproduce the data from sources automatically: Automate backups or the reproduction from sources using AWS features (for example, snapshots of Amazon RDS and Amazon EBS, versions on Amazon S3, etc.), AWS Marketplace solutions, or third-party solutions. Perform periodic recovery of the data to verify backup integrity and processes: Validate that your backup process implementation meets Recovery Time Objective and Recovery Point Objective through a recovery test.","title":"Review of Best Practices Implemented"},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/Lab_Guide.html#5-lab-cleanup","text":"The following instructions will remove the resources that you have created in this lab.","title":"5. Lab Cleanup "},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/Lab_Guide.html#cleaning-up-aws-backup-resources","text":"Sign in to the AWS Management Console and navigate to the AWS Backup console - https://us-east-1.console.aws.amazon.com/backup/home?region=us-east-1#home Click on BACKUP VAULTS from the menu on the left side, and select BACKUP-LAB-VAULT . Under the section BACKUPS , delete all the RECOVERY POINTS . Once all the RECOVERY POINTS have been deleted, delete the Backup Vault by clicking on DELETE on the top right hand corner. Click on BACKUP PLANS from the menu on the left side, and select BACKUP-LAB . Scroll down to the section RESOURCE ASSIGNMENTS , and delete the resource assignment. Delete the BACKUP PLAN by clicking on DELETE on the upper right corner of the screen.","title":"Cleaning up AWS Backup Resources"},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/Lab_Guide.html#cleaning-up-the-cloudformation-stack","text":"Sign in to the AWS Management Console and navigate to the AWS CloudFormation console - https://console.aws.amazon.com/cloudformation/ Select the stack WA-BACKUP-LAB , and delete the stack.","title":"Cleaning up the CloudFormation Stack"},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/Lab_Guide.html#cleaning-up-the-cloudwatch-logs","text":"Sign in to the AWS Management Console, and open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/ . Click Logs in the left navigation. Click the radio button on the left of the /aws/lambda/RestoreTestFunction . Click the Actions Button then click Delete Log Group . Verify the log group name then click Yes, Delete .","title":"Cleaning up the CloudWatch Logs"},{"location":"Reliability/200_Testing_Backup_and_Restore_of_Data/Lab_Guide.html#thank-you-for-using-this-lab","text":"","title":"Thank you for using this lab."},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html","text":"Level 200: Testing for Resiliency of EC2 https://wellarchitectedlabs.com Introduction The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). One primary capability that AWS provides is the ability to test your systems at a production scale, under load. It is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner. In this lab, you will deploy a 2-tier resource, with a reverse proxy (Application Load Balancer), and Web Application on Amazon Elastic Compute Cloud (EC2). The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework Goals: Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR) Prequisites: An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling lanch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#level-200-testing-for-resiliency-of-ec2","text":"https://wellarchitectedlabs.com","title":"Level 200: Testing for Resiliency of EC2"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#introduction","text":"The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). One primary capability that AWS provides is the ability to test your systems at a production scale, under load. It is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner. In this lab, you will deploy a 2-tier resource, with a reverse proxy (Application Load Balancer), and Web Application on Amazon Elastic Compute Cloud (EC2). The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework","title":"Introduction"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#goals","text":"Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR)","title":"Goals:"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#prequisites","text":"An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling lanch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prequisites:"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#license","text":"","title":"License"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/README.html#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html","text":"Level 200: Testing for Resiliency of EC2 Authors Rodney Lester, Reliability Lead, Well-Architected Table of Contents Deploy the Infrastructure Configure Execution Environment Test Resiliency Using Failure Injection Tear Down 1. Deploy the Infrastructure It is a prerequisite to this lab is that you have deployed the static web application stack. If you have already run the following two labs (and have not torn down the resources) then you have already deployed the necessary infrastructure. Proceed to next step Configure Execution Environment Security: Level 200: Automated Deployment of VPC Security: Level 200: Automated Deployment of EC2 Web Application If you have not already deployed the necessary infrastructure, then follow these steps: 1.1 Deploy the VPC infrastructure If you are comfortable deploying a CloudFormation stack, then use the express steps listed here. If you need guidance in how to deploy a CloudFormation stack, then follow the directions for the Automated Deployment of VPC lab, and then return here for the next step: 1.2 Deploy the EC2s and Static WebApp infrastructure Express Steps (Deploy the VPC infrastructure) Download the vpc-alb-app-db.yaml CloudFormation template Choose the AWS region you wish to use - if possible we recommend using us-east-2 (Ohio) In your chosen region, create a CloudFormation stack uploading this CloudFormation Template Name the stack WebApp1-VPC (case sensitive) Leave all CloudFormation Parameters at their default values click Next until the last page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack 1.2 Deploy the EC2s and Static WebApp infrastructure If you are comfortable deploying a CloudFormation stack, then use the express steps listed here. If you need guidance in how to deploy a CloudFormation stack, then follow the directions for the Automated Deployment of EC2 Web Application Follow directions for the create a static web application option Then return here for the next step: Website URL Express Steps (Deploy the EC2s and Static WebApp infrastructure) Download the staticwebapp.yaml CloudFormation template In your chosen region, create a CloudFormation stack uploading this CloudFormation Template Name the stack WebApp1-Static (case sensitive) Leave all CloudFormation Parameters at their default values click Next until the last page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack Website URL In the WebApp1-Static stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created Save this URL - you will need it later 2. Configure Execution Environment Failure injection is a means of testing resiliency by which a specific failure type is simulated on a service and its response is assessed. You have a choice of environments from which to execute the failure injections for this lab. Bash scripts are a good choice and can be used from a Linux command line. If you prefer Python, Java, Powershell, or C# instructions for these are also provided. 2.1 Setup AWS credentials and configuration Your execution environment needs to be configured to enable access to the AWS account you are using for the workshop. This includes Credentials AWS access key AWS secret access key AWS session token (used in some cases) Configuration Region: us-east-2 (or region where you deployed your WebApp) Default output: JSON Note: us-east-2 is the Ohio region If you already know how to configure these, please do so now and proceed to the next step 2.2 Set up the bash environment If you need help then follow the instructions in either Option A or Option B below Option A AWS CLI This option uses the AWS CLI. If you do not have this installed, or do not want to install it, then use Option B To see if the AWS CLI is installed: $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.1 or higher is fine If you instead got command not found then either install the AWS CLI or use Option B Run aws configure and provide the following values: $ aws configure AWS Access Key ID [*************xxxx]: <Your AWS Access Key ID> AWS Secret Access Key [**************xxxx]: <Your AWS Secret Access Key> Default region name: [us-east-2]: us-east-2 (or your chosen region) Default output format [None]: json Option B Manually creating credential files If you already did Option A , then skip this create a .aws directory under your home directory mkdir ~/.aws Change directory to there cd ~/.aws Use a text editor (vim, emacs, notepad) to create a text file (no extension) named credentials . In this file you should have the following text. [default] aws_access_key_id = <Your access key> aws_secret_access_key = <Your secret key> Create a text file (no extension) named config . In this file you should have the following text: [default] region = us-east-2 (or your chosen region) output = json 2.2 Set up the bash environment Using bash is an effective way to execute the failure injection tests for this workshop. The bash scripts make use of the AWS CLI. If you will be using bash, then follow the directions in this section. If you cannot use bash, then skip to the next section . Prerequisites awscli AWS CLI installed $ aws --version aws-cli/1.16.249 Python/3.6.8... Version 1.1 or higher is fine If you instead got command not found then see instructions here to install awscli jq command-line JSON processor installed. $ jq --version jq-1.5-1-a5b5cbe Version 1.4 or higher is fine If you instead got command not found then see instructions here to install jq Download the fail_instance.sh script from the resiliency bash scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script: bash/fail_instance.sh Set the script to be executable. chmod u+x fail_instance.sh 2.3 Set up the programming language environment (for Python, Java, C#, or PowerShell) If you will be using bash and executed the steps in the previous section, then you can skip this and go to the section: Test Resiliency Using Failure Injection If you will be using Python, Java, C#, or PowerShell for this workshop, click here for instructions on setting up your environment 3. Test Resiliency Using Failure Injection Failure injection (also known as chaos testing ) is an effective and essential method to validate and understand the resiliency of your workload and is a recommended practice of the AWS Well-Architected Reliability Pillar . Here you will initiate various failure scenarios and assess how your system reacts. Preparation Before testing, please prepare the following: Region must be the one you selected when you deployed your WebApp We will be using the AWS Console to assess the impact of our testing Throughout this lab, make sure you are in the correct region. For example the following screen shot shows the desired region assuming your WebApp was deployed to Ohio region Get VPC ID A VPC (Amazon Virtual Private Cloud) is a logically isolated section of the AWS Cloud where you have deployed the resources for your service For these tests you will need to know the VPC ID of the VPC you created as part of deploying the service Navigate to the VPC management console: https://console.aws.amazon.com/vpc In the left pane, click Your VPCs 1 - Tick the checkbox next to WebApp1-VPC 2 - Copy the VPC ID Save the VPC ID - you will use later whenever <vpc-id> is indicated in a command Get familiar with the service website Point a web browser at the URL you saved from earlier * If you do not recall this, then in the WebApp1-Static stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created) Note the instance_id (begins with i- ) - this is the EC2 instance serving this request Refresh the website several times watching these values Note the values change. You have deployed two web servers per each of three Availability Zones. The AWS Elastic Load Balancer (ELB) sends your request to any of these three healthy instances. 3.1 EC2 failure injection This failure injection will simulate a critical problem with one of the three web servers used by your service. Navigate to the EC2 console at http://console.aws.amazon.com/ec2 and click Instances in the left pane. There are three EC2 instances with a name beginning with WebApp1 . For these EC2 instances note: Each has a unique Instance ID There is two instances per each Availability Zone All instances are healthy Open up two more console in separate tabs/windows. From the left pane, open Target Groups and Auto Scaling Groups in separate tabs. You now have three console views open To fail one of the EC2 instances, use the VPC ID as the command line argument replacing <vpc-id> in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for) Language Command Bash ./fail_instance.sh <vpc-id> Python python fail_instance.py <vpc-id> Java java -jar app-resiliency-1.0.jar EC2 <vpc-id> C# .\\AppResiliency EC2 <vpc-id> PowerShell .\\fail_instance.ps1 <vpc-id> The specific output will vary based on the command used, but will include a reference to the ID of the EC2 instance and an indicator of success. Here is the output for the Bash command. Note the CurrentState is shutting-down $ ./fail_instance.sh vpc-04f8541d10ed81c80 Terminating i-0710435abc631eab3 { \"TerminatingInstances\": [ { \"CurrentState\": { \"Code\": 32, \"Name\": \"shutting-down\" }, \"InstanceId\": \"i-0710435abc631eab3\", \"PreviousState\": { \"Code\": 16, \"Name\": \"running\" } } ] } Go to the EC2 Instances console which you already have open (or click here to open a new one ) Refresh it. ( Note : it is usually more efficient to use the refresh button in the console, than to refresh the browser) Observe the status of the instance reported by the script. In the screen cap below it is shutting down as reported by the script and will ultimately transition to terminated . 3.2 System response to EC2 instance failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long. 3.2.1 System availability Refresh the service website several times. Note the following: Website remains available The remaining two EC2 instances are handling all the requests (as per the displayed instance_id) 3.2.2 Load balancing Load balancing ensures service requests are not routed to unhealthy resources, such as the failed EC2 instance. Go to the Target Groups console you already have open (or click here to open a new one ) If there is more than one target group, select the one with whose name begins with WebAp Click on the Targets tab and observe: Status of the instances in the group. The load balancer will only send traffic to healthy instances. When the auto scaling launches a new instance, it is automatically added to the load balancer target group. In the screen cap below the unhealthy instance is the newly added one. The load balancer will not send traffic to it until it is completed initializing. It will ultimately transition to healthy and then start receiving traffic. Note the new instance was started in the same Availability Zone as the failed one. Amazon EC2 Auto Scaling automatically maintains balance across all of the Availability Zones that you specify. From the same console, now click on the Monitoring tab and view metrics such as Unhealthy hosts and Healthy hosts 3.2.3 Auto scaling Autos scaling ensures we have the capacity necessary to meet customer demand. The auto scaling for this service is a simple configuration that ensures at least three EC2 instances are running. More complex configurations in response to CPU or network load are also possible using AWS. Go to the Auto Scaling Groups console you already have open (or click here to open a new one ) If there is more than one auto scaling group, select the one with the name that starts with WebApp1 Click on the Activity History tab and observe: The screen cap below shows that instances were successfully started at 17:25 At 19:29 the instance targeted by the script was put in draining state and a new instance ending in ...62640 was started, but was still initializing. The new instance will ultimately transition to Successful status Draining allows existing, in-flight requests made to an instance to complete, but it will not send any new requests to the instance. Learn more : After the lab see this blog post for more information on draining . Learn more : After the lab see Auto Scaling Groups to learn more how auto scaling groups are setup and how they distribute instances, and Dynamic Scaling for Amazon EC2 Auto Scaling for more details on setting up auto scaling that responds to demand 3.2.4 EC2 failure injection - conclusion Deploying multiple servers and Elastic Load Balancing enables a service suffer the loss of a server with no availability disruptions as user traffic is automatically routed to the healthy servers. Amazon Auto Scaling ensures unhealthy hosts are removed and replaced with healthy ones to maintain high availability. Availability Zones ( AZ s) are isolated sets of resources within a region, each with redundant power, networking, and connectivity, housed in separate facilities. Each Availability Zone is isolated, but the Availability Zones in a Region are connected through low-latency links. AWS provides you with the flexibility to place instances and store data across multiple Availability Zones within each AWS Region for high resiliency. Learn more : After the lab see this whitepaper on regions and availability zones 4. Tear down this lab The following instructions will remove the resources that you have created in this lab. If you deployed the CloudFormation stacks as part of the prerequisites for this lab, then delete these stacks to remove all the AWS resources. If you need help with how to delete CloudFormation stacks then follow these instructions to tear down those resources: Delete the WebApp resources Wait for this stack deletion to complete Delete the VPC resources Otherwise, there were no additional new resources added as part of this lab. References & useful resources AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#level-200-testing-for-resiliency-of-ec2","text":"","title":"Level 200: Testing for Resiliency of EC2"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#authors","text":"Rodney Lester, Reliability Lead, Well-Architected","title":"Authors"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#table-of-contents","text":"Deploy the Infrastructure Configure Execution Environment Test Resiliency Using Failure Injection Tear Down","title":"Table of Contents"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#1-deploy-the-infrastructure","text":"It is a prerequisite to this lab is that you have deployed the static web application stack. If you have already run the following two labs (and have not torn down the resources) then you have already deployed the necessary infrastructure. Proceed to next step Configure Execution Environment Security: Level 200: Automated Deployment of VPC Security: Level 200: Automated Deployment of EC2 Web Application If you have not already deployed the necessary infrastructure, then follow these steps:","title":"1. Deploy the Infrastructure "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#11-deploy-the-vpc-infrastructure","text":"If you are comfortable deploying a CloudFormation stack, then use the express steps listed here. If you need guidance in how to deploy a CloudFormation stack, then follow the directions for the Automated Deployment of VPC lab, and then return here for the next step: 1.2 Deploy the EC2s and Static WebApp infrastructure","title":"1.1 Deploy the VPC infrastructure"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#express-steps-deploy-the-vpc-infrastructure","text":"Download the vpc-alb-app-db.yaml CloudFormation template Choose the AWS region you wish to use - if possible we recommend using us-east-2 (Ohio) In your chosen region, create a CloudFormation stack uploading this CloudFormation Template Name the stack WebApp1-VPC (case sensitive) Leave all CloudFormation Parameters at their default values click Next until the last page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack","title":"Express Steps (Deploy the VPC infrastructure)"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#12-deploy-the-ec2s-and-static-webapp-infrastructure","text":"If you are comfortable deploying a CloudFormation stack, then use the express steps listed here. If you need guidance in how to deploy a CloudFormation stack, then follow the directions for the Automated Deployment of EC2 Web Application Follow directions for the create a static web application option Then return here for the next step: Website URL","title":"1.2 Deploy the EC2s and Static WebApp infrastructure"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#express-steps-deploy-the-ec2s-and-static-webapp-infrastructure","text":"Download the staticwebapp.yaml CloudFormation template In your chosen region, create a CloudFormation stack uploading this CloudFormation Template Name the stack WebApp1-Static (case sensitive) Leave all CloudFormation Parameters at their default values click Next until the last page check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack","title":"Express Steps (Deploy the EC2s and Static WebApp infrastructure)"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#website-url","text":"In the WebApp1-Static stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created Save this URL - you will need it later","title":"Website URL"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#2-configure-execution-environment","text":"Failure injection is a means of testing resiliency by which a specific failure type is simulated on a service and its response is assessed. You have a choice of environments from which to execute the failure injections for this lab. Bash scripts are a good choice and can be used from a Linux command line. If you prefer Python, Java, Powershell, or C# instructions for these are also provided.","title":"2. Configure Execution Environment "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#21-setup-aws-credentials-and-configuration","text":"Your execution environment needs to be configured to enable access to the AWS account you are using for the workshop. This includes Credentials AWS access key AWS secret access key AWS session token (used in some cases) Configuration Region: us-east-2 (or region where you deployed your WebApp) Default output: JSON Note: us-east-2 is the Ohio region If you already know how to configure these, please do so now and proceed to the next step 2.2 Set up the bash environment If you need help then follow the instructions in either Option A or Option B below","title":"2.1 Setup AWS credentials and configuration "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#option-a-aws-cli","text":"This option uses the AWS CLI. If you do not have this installed, or do not want to install it, then use Option B To see if the AWS CLI is installed: $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.1 or higher is fine If you instead got command not found then either install the AWS CLI or use Option B Run aws configure and provide the following values: $ aws configure AWS Access Key ID [*************xxxx]: <Your AWS Access Key ID> AWS Secret Access Key [**************xxxx]: <Your AWS Secret Access Key> Default region name: [us-east-2]: us-east-2 (or your chosen region) Default output format [None]: json","title":"Option A AWS CLI"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#option-b-manually-creating-credential-files","text":"If you already did Option A , then skip this create a .aws directory under your home directory mkdir ~/.aws Change directory to there cd ~/.aws Use a text editor (vim, emacs, notepad) to create a text file (no extension) named credentials . In this file you should have the following text. [default] aws_access_key_id = <Your access key> aws_secret_access_key = <Your secret key> Create a text file (no extension) named config . In this file you should have the following text: [default] region = us-east-2 (or your chosen region) output = json","title":"Option B Manually creating credential files"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#22-set-up-the-bash-environment","text":"Using bash is an effective way to execute the failure injection tests for this workshop. The bash scripts make use of the AWS CLI. If you will be using bash, then follow the directions in this section. If you cannot use bash, then skip to the next section . Prerequisites awscli AWS CLI installed $ aws --version aws-cli/1.16.249 Python/3.6.8... Version 1.1 or higher is fine If you instead got command not found then see instructions here to install awscli jq command-line JSON processor installed. $ jq --version jq-1.5-1-a5b5cbe Version 1.4 or higher is fine If you instead got command not found then see instructions here to install jq Download the fail_instance.sh script from the resiliency bash scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script: bash/fail_instance.sh Set the script to be executable. chmod u+x fail_instance.sh","title":"2.2 Set up the bash environment "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#23-set-up-the-programming-language-environment-for-python-java-c-or-powershell","text":"If you will be using bash and executed the steps in the previous section, then you can skip this and go to the section: Test Resiliency Using Failure Injection If you will be using Python, Java, C#, or PowerShell for this workshop, click here for instructions on setting up your environment","title":"2.3 Set up the programming language environment (for Python, Java, C#, or PowerShell) "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#3-test-resiliency-using-failure-injection","text":"Failure injection (also known as chaos testing ) is an effective and essential method to validate and understand the resiliency of your workload and is a recommended practice of the AWS Well-Architected Reliability Pillar . Here you will initiate various failure scenarios and assess how your system reacts.","title":"3. Test Resiliency Using Failure Injection "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#preparation","text":"Before testing, please prepare the following: Region must be the one you selected when you deployed your WebApp We will be using the AWS Console to assess the impact of our testing Throughout this lab, make sure you are in the correct region. For example the following screen shot shows the desired region assuming your WebApp was deployed to Ohio region Get VPC ID A VPC (Amazon Virtual Private Cloud) is a logically isolated section of the AWS Cloud where you have deployed the resources for your service For these tests you will need to know the VPC ID of the VPC you created as part of deploying the service Navigate to the VPC management console: https://console.aws.amazon.com/vpc In the left pane, click Your VPCs 1 - Tick the checkbox next to WebApp1-VPC 2 - Copy the VPC ID Save the VPC ID - you will use later whenever <vpc-id> is indicated in a command Get familiar with the service website Point a web browser at the URL you saved from earlier * If you do not recall this, then in the WebApp1-Static stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created) Note the instance_id (begins with i- ) - this is the EC2 instance serving this request Refresh the website several times watching these values Note the values change. You have deployed two web servers per each of three Availability Zones. The AWS Elastic Load Balancer (ELB) sends your request to any of these three healthy instances.","title":"Preparation"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#31-ec2-failure-injection","text":"This failure injection will simulate a critical problem with one of the three web servers used by your service. Navigate to the EC2 console at http://console.aws.amazon.com/ec2 and click Instances in the left pane. There are three EC2 instances with a name beginning with WebApp1 . For these EC2 instances note: Each has a unique Instance ID There is two instances per each Availability Zone All instances are healthy Open up two more console in separate tabs/windows. From the left pane, open Target Groups and Auto Scaling Groups in separate tabs. You now have three console views open To fail one of the EC2 instances, use the VPC ID as the command line argument replacing <vpc-id> in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for) Language Command Bash ./fail_instance.sh <vpc-id> Python python fail_instance.py <vpc-id> Java java -jar app-resiliency-1.0.jar EC2 <vpc-id> C# .\\AppResiliency EC2 <vpc-id> PowerShell .\\fail_instance.ps1 <vpc-id> The specific output will vary based on the command used, but will include a reference to the ID of the EC2 instance and an indicator of success. Here is the output for the Bash command. Note the CurrentState is shutting-down $ ./fail_instance.sh vpc-04f8541d10ed81c80 Terminating i-0710435abc631eab3 { \"TerminatingInstances\": [ { \"CurrentState\": { \"Code\": 32, \"Name\": \"shutting-down\" }, \"InstanceId\": \"i-0710435abc631eab3\", \"PreviousState\": { \"Code\": 16, \"Name\": \"running\" } } ] } Go to the EC2 Instances console which you already have open (or click here to open a new one ) Refresh it. ( Note : it is usually more efficient to use the refresh button in the console, than to refresh the browser) Observe the status of the instance reported by the script. In the screen cap below it is shutting down as reported by the script and will ultimately transition to terminated .","title":"3.1 EC2 failure injection"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#32-system-response-to-ec2-instance-failure","text":"Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.","title":"3.2 System response to EC2 instance failure"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#321-system-availability","text":"Refresh the service website several times. Note the following: Website remains available The remaining two EC2 instances are handling all the requests (as per the displayed instance_id)","title":"3.2.1 System availability"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#322-load-balancing","text":"Load balancing ensures service requests are not routed to unhealthy resources, such as the failed EC2 instance. Go to the Target Groups console you already have open (or click here to open a new one ) If there is more than one target group, select the one with whose name begins with WebAp Click on the Targets tab and observe: Status of the instances in the group. The load balancer will only send traffic to healthy instances. When the auto scaling launches a new instance, it is automatically added to the load balancer target group. In the screen cap below the unhealthy instance is the newly added one. The load balancer will not send traffic to it until it is completed initializing. It will ultimately transition to healthy and then start receiving traffic. Note the new instance was started in the same Availability Zone as the failed one. Amazon EC2 Auto Scaling automatically maintains balance across all of the Availability Zones that you specify. From the same console, now click on the Monitoring tab and view metrics such as Unhealthy hosts and Healthy hosts","title":"3.2.2 Load balancing"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#323-auto-scaling","text":"Autos scaling ensures we have the capacity necessary to meet customer demand. The auto scaling for this service is a simple configuration that ensures at least three EC2 instances are running. More complex configurations in response to CPU or network load are also possible using AWS. Go to the Auto Scaling Groups console you already have open (or click here to open a new one ) If there is more than one auto scaling group, select the one with the name that starts with WebApp1 Click on the Activity History tab and observe: The screen cap below shows that instances were successfully started at 17:25 At 19:29 the instance targeted by the script was put in draining state and a new instance ending in ...62640 was started, but was still initializing. The new instance will ultimately transition to Successful status Draining allows existing, in-flight requests made to an instance to complete, but it will not send any new requests to the instance. Learn more : After the lab see this blog post for more information on draining . Learn more : After the lab see Auto Scaling Groups to learn more how auto scaling groups are setup and how they distribute instances, and Dynamic Scaling for Amazon EC2 Auto Scaling for more details on setting up auto scaling that responds to demand","title":"3.2.3 Auto scaling"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#324-ec2-failure-injection-conclusion","text":"Deploying multiple servers and Elastic Load Balancing enables a service suffer the loss of a server with no availability disruptions as user traffic is automatically routed to the healthy servers. Amazon Auto Scaling ensures unhealthy hosts are removed and replaced with healthy ones to maintain high availability. Availability Zones ( AZ s) are isolated sets of resources within a region, each with redundant power, networking, and connectivity, housed in separate facilities. Each Availability Zone is isolated, but the Availability Zones in a Region are connected through low-latency links. AWS provides you with the flexibility to place instances and store data across multiple Availability Zones within each AWS Region for high resiliency. Learn more : After the lab see this whitepaper on regions and availability zones","title":"3.2.4 EC2 failure injection - conclusion"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#4-tear-down-this-lab","text":"The following instructions will remove the resources that you have created in this lab. If you deployed the CloudFormation stacks as part of the prerequisites for this lab, then delete these stacks to remove all the AWS resources. If you need help with how to delete CloudFormation stacks then follow these instructions to tear down those resources: Delete the WebApp resources Wait for this stack deletion to complete Delete the VPC resources Otherwise, there were no additional new resources added as part of this lab.","title":"4. Tear down this lab "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#references-useful-resources","text":"AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances","title":"References &amp; useful resources"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Programming_Environment.html","text":"Setting up an environment to run the workshop using a programming language If you will be using Bash for this workshop, STOP and return to the Lab Guide instructions for setting up Bash If you will not be using Bash and prefer to use Python, Java, C#, or PowerShell for this workshop, then follow these steps 1. Set up AWS credentials If you have not yet setup your AWS credentials, then follow this guide 2. Language specific setup Choose the appropriate section below for your language 2.1 Setting Up the Python Environment The scripts are written in python with boto3. On Amazon Linux, this is already installed. Use your local operating system instructions to install boto3: https://github.com/boto/boto3 Download the fail_instance.py from the resiliency Python scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script: python/fail_instance.py 2.2 Setting Up the Java Environment The command line utility in Java requires Java 8 SE. $ java -version openjdk version \"1.8.0_222\" OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10) OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode) If you have java 1.7 installed (as will be the case for In Amazon Linux), you need to install Java 8 and remove Java 7. For Amazon Linux and RedHat $ sudo yum install java-1.8.0-openjdk $ sudo yum remove java-1.7.0-openjdk For Debian, Ubuntu $ sudo apt install openjdk-8-jdk $ sudo apt install openjdk-7-jdk Next choose one of the following options: Option A or Option B Option A : If you are comfortable with git Clone the aws-well-architected-labs repo $ git clone https://github.com/awslabs/aws-well-architected-labs.git Cloning into 'aws-well-architected-labs'... ... Checking out files: 100% (1935/1935), done. go to the build directory cd aws-well-architected-labs/Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Code/FailureSimulations/java/appresiliency Option B : Download the zipfile of the executables at the following URL https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip go to the build directory: cd java/appresiliency Build: mvn clean package shade:shade cd target - this is where your jar files were built and where you can run from the command line 2.3 Setting Up the C# Environment Download the zipfile of the executables at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip Unzip the folder in a location convenient for you to execute the command line programs. 2.4 Setting up the Powershell Environment If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ Download the fail_instance.sh script from the resiliency PowerShell scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script: powershell/fail_instance.sh If your PowerShell script is refused authorization to access your AWS account, consult Getting Started with the AWS Tools for Windows PowerShell Click here to return to the Lab Guide","title":"Setting up an environment to run the workshop using a programming language"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Programming_Environment.html#setting-up-an-environment-to-run-the-workshop-using-a-programming-language","text":"If you will be using Bash for this workshop, STOP and return to the Lab Guide instructions for setting up Bash If you will not be using Bash and prefer to use Python, Java, C#, or PowerShell for this workshop, then follow these steps","title":"Setting up an environment to run the workshop using a programming language"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Programming_Environment.html#1-set-up-aws-credentials","text":"If you have not yet setup your AWS credentials, then follow this guide","title":"1. Set up AWS credentials"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Programming_Environment.html#2-language-specific-setup","text":"Choose the appropriate section below for your language","title":"2. Language specific setup"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Programming_Environment.html#21-setting-up-the-python-environment","text":"The scripts are written in python with boto3. On Amazon Linux, this is already installed. Use your local operating system instructions to install boto3: https://github.com/boto/boto3 Download the fail_instance.py from the resiliency Python scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script: python/fail_instance.py","title":"2.1 Setting Up the Python Environment"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Programming_Environment.html#22-setting-up-the-java-environment","text":"The command line utility in Java requires Java 8 SE. $ java -version openjdk version \"1.8.0_222\" OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10) OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode) If you have java 1.7 installed (as will be the case for In Amazon Linux), you need to install Java 8 and remove Java 7. For Amazon Linux and RedHat $ sudo yum install java-1.8.0-openjdk $ sudo yum remove java-1.7.0-openjdk For Debian, Ubuntu $ sudo apt install openjdk-8-jdk $ sudo apt install openjdk-7-jdk Next choose one of the following options: Option A or Option B Option A : If you are comfortable with git Clone the aws-well-architected-labs repo $ git clone https://github.com/awslabs/aws-well-architected-labs.git Cloning into 'aws-well-architected-labs'... ... Checking out files: 100% (1935/1935), done. go to the build directory cd aws-well-architected-labs/Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Code/FailureSimulations/java/appresiliency Option B : Download the zipfile of the executables at the following URL https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip go to the build directory: cd java/appresiliency Build: mvn clean package shade:shade cd target - this is where your jar files were built and where you can run from the command line","title":"2.2 Setting Up the Java Environment"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Programming_Environment.html#23-setting-up-the-c-environment","text":"Download the zipfile of the executables at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip Unzip the folder in a location convenient for you to execute the command line programs.","title":"2.3 Setting Up the C# Environment"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Programming_Environment.html#24-setting-up-the-powershell-environment","text":"If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ Download the fail_instance.sh script from the resiliency PowerShell scripts on GitHub to a location convenient for you to execute it. You can use the following link to download the script: powershell/fail_instance.sh If your PowerShell script is refused authorization to access your AWS account, consult Getting Started with the AWS Tools for Windows PowerShell Click here to return to the Lab Guide","title":"2.4 Setting up the Powershell Environment"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Software_Install.html","text":"Software Install This reference will help you install software necessary to setup your workshop environment AWS CLI jq AWS CLI The AWS Command Line Interface (AWS CLI) is a unified tool that provides a consistent interface for interacting with all parts of AWS. Linux This includes: All native Linux installs MacOS Windows Subsystem for Linux (WSL) Run the following command $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.0 or higher is fine If you instead got command not found then you need to install awscli : $ pip3 install awscli --upgrade --user ...(lots of output)... Successfully installed... * If that succeeded, then you are finished. Return to the Lab Guide If that does not work, then do the following: See the detailed installation instructions here STOP HERE and return to the Lab Guide jq jq is a command-line JSON processor. is like sed for JSON data. It is used in the workshop bash scripts to parse AWS CLI output. Linux Run the following command $ jq --version jq-1.5-1-a5b5cbe * Any version is fine. * If you instead got command not found then you need to install jq : $ sudo apt-get install jq ...(lots of output)... $ jq --version jq-1.5-1-a5b5cbe * If that succeeded, then you are finished. Return to the Lab Guide If that does not work, then do the following Download the jq executable $ wget https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 2019-10-11 17:41:42 (1.97 MB/s) - \u2018jq-linux64\u2019 saved [3953824/3953824] You can find out what your execution path is with the following command. $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable. $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory. $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq Click here to return to the Lab Guide","title":"Software Install"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Software_Install.html#software-install","text":"This reference will help you install software necessary to setup your workshop environment AWS CLI jq","title":"Software Install"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Software_Install.html#aws-cli","text":"The AWS Command Line Interface (AWS CLI) is a unified tool that provides a consistent interface for interacting with all parts of AWS.","title":"AWS CLI "},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Software_Install.html#linux","text":"This includes: All native Linux installs MacOS Windows Subsystem for Linux (WSL) Run the following command $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.0 or higher is fine If you instead got command not found then you need to install awscli : $ pip3 install awscli --upgrade --user ...(lots of output)... Successfully installed... * If that succeeded, then you are finished. Return to the Lab Guide If that does not work, then do the following: See the detailed installation instructions here STOP HERE and return to the Lab Guide","title":"Linux"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Software_Install.html#jq","text":"jq is a command-line JSON processor. is like sed for JSON data. It is used in the workshop bash scripts to parse AWS CLI output.","title":"jq"},{"location":"Reliability/200_Testing_for_Resiliency_of_EC2/Documentation/Software_Install.html#linux_1","text":"Run the following command $ jq --version jq-1.5-1-a5b5cbe * Any version is fine. * If you instead got command not found then you need to install jq : $ sudo apt-get install jq ...(lots of output)... $ jq --version jq-1.5-1-a5b5cbe * If that succeeded, then you are finished. Return to the Lab Guide If that does not work, then do the following Download the jq executable $ wget https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 2019-10-11 17:41:42 (1.97 MB/s) - \u2018jq-linux64\u2019 saved [3953824/3953824] You can find out what your execution path is with the following command. $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable. $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory. $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq Click here to return to the Lab Guide","title":"Linux"},{"location":"Reliability/300_Health_Checks_and_Dependencies/README.html","text":"Level 300: Implementing Health Checks and Managing Dependencies to improve Reliability https://wellarchitectedlabs.com Introduction This hands-on lab will guide you through the steps to improve reliability of a service by decoupling service dependencies, using health checks, and demonstrating when to use fail-open and fail-closed behaviors. The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework Amazon Builders' Library This lab additionally illustrates best practices as described in the Amazon Builders' Library article: Implementing health checks Goals After you have completed this lab, you will be able to: Implement graceful degradation to transform applicable hard dependencies into soft dependencies Monitor all layers of the workload to detect failures Route traffic only to healthy application instances Configure fail-open and fail-closed behaviors as appropriate in response to detected faults Use AWS services to reduce mean time to recovery (MTTR) Prequisites If you are running the at an AWS sponsored workshop then you may be provided with an AWS Account to use, in which case the following pre-requisites will be satisfied by the provided AWS account. If you are running this using your own AWS Account, then please note the following prerequisites: An AWS Account that you are able to use for testing. This account MUST NOT be used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets and route tables, Security Groups, Internet Gateways, NAT Gateways, Elastic IP Addresses, IAM Roles, instance profiles, AWS Auto Scaling launch configurations, Application Load Balancers, Auto Scaling Groups, DynamoDB tables, SSM Parameters, and EC2 instances. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Reliability/300_Health_Checks_and_Dependencies/README.html#level-300-implementing-health-checks-and-managing-dependencies-to-improve-reliability","text":"https://wellarchitectedlabs.com","title":"Level 300: Implementing Health Checks and Managing Dependencies to improve Reliability"},{"location":"Reliability/300_Health_Checks_and_Dependencies/README.html#introduction","text":"This hands-on lab will guide you through the steps to improve reliability of a service by decoupling service dependencies, using health checks, and demonstrating when to use fail-open and fail-closed behaviors. The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework","title":"Introduction"},{"location":"Reliability/300_Health_Checks_and_Dependencies/README.html#amazon-builders-library","text":"This lab additionally illustrates best practices as described in the Amazon Builders' Library article: Implementing health checks","title":"Amazon Builders' Library"},{"location":"Reliability/300_Health_Checks_and_Dependencies/README.html#goals","text":"After you have completed this lab, you will be able to: Implement graceful degradation to transform applicable hard dependencies into soft dependencies Monitor all layers of the workload to detect failures Route traffic only to healthy application instances Configure fail-open and fail-closed behaviors as appropriate in response to detected faults Use AWS services to reduce mean time to recovery (MTTR)","title":"Goals"},{"location":"Reliability/300_Health_Checks_and_Dependencies/README.html#prequisites","text":"If you are running the at an AWS sponsored workshop then you may be provided with an AWS Account to use, in which case the following pre-requisites will be satisfied by the provided AWS account. If you are running this using your own AWS Account, then please note the following prerequisites: An AWS Account that you are able to use for testing. This account MUST NOT be used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets and route tables, Security Groups, Internet Gateways, NAT Gateways, Elastic IP Addresses, IAM Roles, instance profiles, AWS Auto Scaling launch configurations, Application Load Balancers, Auto Scaling Groups, DynamoDB tables, SSM Parameters, and EC2 instances. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prequisites"},{"location":"Reliability/300_Health_Checks_and_Dependencies/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Reliability/300_Health_Checks_and_Dependencies/README.html#license","text":"","title":"License"},{"location":"Reliability/300_Health_Checks_and_Dependencies/README.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/300_Health_Checks_and_Dependencies/README.html#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html","text":"Level 300: Implementing Health Checks and Managing Dependencies to Improve Reliability Author Seth Eliot, Reliability Lead, Well-Architected, AWS Amazon Builders' Library and AWS Well-Architected This lab illustrates best practices for reliability as described in the AWS Well-Architected Reliability pillar. It focuses on resiliency practices described in the Amazon Builders' Library article: Implementing health checks . Table of Contents Deploy the application Handle failure of service dependencies Implement deep health checks Fail open when appropriate Tear down this lab 1. Deploy the application You will create a multi-tier architecture using AWS and run a simple service on it. The service is a web server running on Amazon EC2 fronted by an Elastic Load Balancer reverse-proxy, with a dependency on Amazon DynamoDB. Note : The concepts covered by this lab apply whether your service dependency is an AWS resource like Amazon DynamoDB, or an external service called via API. The DynamoDB dependency here acts as a mock for an external service called RecommendationService . The getRecommendation API on this service is a dependency for the web service used in this lab. getRecommendation is actually a get_item call to a DynamoDB table. 1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor : Follow the instructions here for accessing your AWS account If you are using your own AWS account : Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab. 1.2 Deploy the application using an AWS CloudFormation template You will deploy the service infrastructure including simple service code and some sample data. It is recommended that you use the Ohio region. This region is also known as us-east-2 , which you will see referenced throughout this lab. If you choose to use a different region, you will need to ensure future steps are consistent with your region choice. 1.2.1 Deploy the VPC infrastructure If you are comfortable deploying a CloudFormation stack, then use the express steps listed immediately below. If you need additional guidance in how to deploy a CloudFormation stack, then follow the directions for the Automated Deployment of VPC lab, and then return here for the next step: 1.2.2 Deploy the web service infrastructure and service Express Steps (Deploy the VPC infrastructure) Download the vpc-alb-app-db.yaml CloudFormation template Create a CloudFormation stack (with new resources) by uploading this CloudFormation Template file For Stack name use WebApp1-VPC (case sensitive) Leave all CloudFormation Parameters at their default values Click Next until the last page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack 1.2.2 Deploy the web app infrastructure and service Wait until the VPC CloudFormation stack status is CREATE_COMPLETE , then continue. This will take about four minutes. If you are comfortable deploying a CloudFormation stack, then use the express steps listed immediately below. If you need additional guidance in how to deploy a CloudFormation stack, then follow the directions for the Create an AWS CloudFormation Stack from a template lab, and then return here for the next step: 1.3 View the website for web service Express Steps (Deploy the WebApp infrastructure and service) Download the staticwebapp.yaml CloudFormation template Create a CloudFormation stack (with new resources) by uploading this CloudFormation Template file For Stack name use HealthCheckLab Leave all CloudFormation Parameters at their default values Click Next until the last page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack 1.3 View the website for web service Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation . Wait until HealthCheckLab stack status is CREATE_COMPLETE before proceeding. This should take about four minutes Click on the HealthCheckLab stack Click on the Outputs tab For the Key WebsiteURL copy the value. This is the URL of your test web service Hint : it will start with http://healt-alb and end in <aws region>.elb.amazonaws.com Click the URL and it will bring up the website: The website simulates a recommendation engine making personalized suggestions for classic television shows. You should note the following features: Area A shows the personalized recommendation It shows first name of the user and the show that was recommended The workshop simulation is simple. On every request it chooses a user at random, and shows a recommendation statically mapped to that user. The user names, television show names, and this mapping are in a DynamoDB table, which is simulating the RecommendationService Area B shows metadata which is useful to you during the lab The instance_id and availability_zone enable you to see which EC2 server and Availability Zone were used for each request There is one EC2 instance deployed per Availability Zone Refresh the website several times, note that the EC2 instance and Availability Zone change from among the three available This is Elastic Load Balancing (ELB) distributing these stateless requests among the available EC2 server instances across Availability Zones Well-Architected for Reliability: Best practices Use highly available network connectivity for your workload public endpoints : Elastic Load Balancing provides load balancing across Availability Zones, performs Layer 4 (TCP) or Layer 7 (http/https) routing, integrates with AWS WAF, and integrates with AWS Auto Scaling to help create a self-healing infrastructure and absorb increases in traffic while releasing resources when traffic decreases. Implement loosely coupled dependencies : Dependencies such as queuing systems, streaming systems, workflows, and load balancers are loosely coupled. Loose coupling helps isolate behavior of a component from other components that depend on it, increasing resiliency and agility. Deploy the workload to multiple locations : Distribute workload data and resources across multiple Availability Zones or, where necessary, across AWS Regions. These locations can be as diverse as required. 2. Handle failure of service dependencies 2.1 System dependency initially healthy You already observed that all three EC2 instances are successfully serving requests In a new tab navigate to ELB Target Groups console By clicking here to open the AWS Management Console or navigating through the AWS Management Console: Services > EC2 > Load Balancing > Target Groups Leave this tab open as you will be referring back to it multiple times Click on the Targets tab (bottom half of screen) Under Registered Targets observe the three EC2 instances serving your web service Note that they are all healthy (see Status and Description ) In this state the ELB will route traffic to any of the three servers From the Target Groups console, now click on the the Health checks tab Note here that the Path is configured to /healthcheck Copy the URL of the web service to a new tab and append /healthcheck to the end of the URL The new URL should look like: http://healt-alb1l-<...>.elb.amazonaws.com/healthcheck Refresh several times and observe the health check on the three servers Note the check is successful The EC2 servers receive user requests (for a TV show recommendation) on the path / and they receive health check requests from the Elastic Load Balancer on the path /healthcheck The health check always returns an http 200 code for any request to it. The server code running on each EC2 instance can be viewed here , or you can view the health check code excerpt below: Click here to see the health check code excerpt # Healthcheck request - will be used by the Elastic Load Balancer elif self.path == '/healthcheck': # Return a healthy code self.send_response(200) self.send_header('Content-type', 'text/html') self.end_headers() 2.2 Simulate dependency not available 2.2.1 Disable RecommendationService You will now simulate a complete failure of the RecommendationService . Every request in turn makes a (simulated) call to the getRecommendation API on this service. These will all fail for every request on every server. In a new tab, navigate to the Parameter Store on the AWS Systems Manager console By clicking here to open the AWS Management Console or navigating through the AWS Management Console: Services > Systems Manager > Parameter Store Leave this tab open as you will be referring back to it one additional time Click on RecommendationServiceEnabled Click Edit In the Value box, type false Click Save Changes A status message should say Edit parameter request succeeded The RecommendationServiceEnabled parameter is used only for this lab. The server code reads its value, and simulates a failure in RecommendationService (all reads to the DynamoDB table simulating the service will fail) when it is false . 2.2.2 Observe behavior when dependency not available Refresh the test web service multiple times Note that it fails with 502 Bad Gateway For each request one of the servers receiving the request attempts to call the RecommendationService but catastrophically fails and fails to return a reply (empty reply) to the load balancer, which in turn presents this as a http 502 failure. You can observe this by opening a new tab and navigating to ELB Load Balancers console: By clicking here to open the AWS Management Console or navigating through the AWS Management Console: Services > EC2 > Load Balancing > Load Balancers Click on the Monitoring tab (bottom half of screen) Observe the ELB 5XXs (Count) and HTTP 502s (Count) errors for the load balancer It will take a minute for the metrics to show up. Make sure you refresh the web service page multiple times in your browser These are the error codes the load balancer returns on every request during this simulated outage Compare these metrics to those for the target group (the EC2 servers themselves) Return to the Target Groups console and click the Monitoring tab there Observe HTTP 5XXs ( Count ) errors shows no data The servers themselves are not returning actual http error codes, they are failing to return any data at all We need to update the server code to handle when the dependency is not available 2.3 Update server code to handle dependency not available The getRecommendation API is actually a get_item call on a DynamoDB table. Examine the server code to see how errors are currently handled The server code running on each EC2 instance can be viewed here Search for the call to the RecommendationService . It looks like this: response = call_getRecommendation(self.region, user_id) What happens if this call fails? Choose one of the options below ( Option 1 - Expert or Option 2 - Assisted ) to improve the code and handle the failure 2.3.1 Option 1 - Expert option: make and deploy your changes to the code You may choose this option, or skip to Option 2 - Assisted option This option requires you have access to place a file in a location accessible via https/https via a URL. For example a public readable S3 bucket, gist (use the raw option to get the URL), or your private webserver. Download the existing server code from here: server_basic.py Modify the code to handle the call to the RecommendationService When the call to RecommendationService fails then instead of using the response data you requested and did not get, return a static response: Instead of user first name return Valued Customer Instead of a personalized recommended TV show, return I Love Lucy Try to also return some diagnostic information on the cause of the error Put your updated server code in a location where it can be downloaded via its URL using wget In the AWS Console go the HealthCheckLab CloudFormation stack and Update it: Leave Use current template selected and click Next Find the ServerCodeUrl parameter and enter the URL for your new code When stack status is CREATE_COMPLETE (about four minutes) then continue If you completed the Option 1 - Expert option , then skip the Option 2 - Assisted option section and continue with 2.3.3 Error handling code 2.3.2 Option 2 - Assisted option: deploy workshop provided code The new server code including error handling can be viewed here Search for Error handling in the comments (occurs twice). What will this code do now if the dependency call fails? Deploy the new error handling code Navigate to the AWS CloudFormation console Click on the HealthCheckLab stack Click Update Leave Use current template selected and click Next Find the ServerCodeUrl parameter and enter the following: https://aws-well-architected-labs-ohio.s3.us-east-2.amazonaws.com/Healthcheck/Code/server_errorhandling.py Click Next until the last page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Update stack Click on Events , and click the refresh icon to observe the stack progress New EC2 instances running the error handling code are being deployed When stack status is CREATE_COMPLETE (about four minutes) then continue 2.3.3 Error handling code This is the error handling code from server_errorhandling.py . The Option 2 - Assisted option uses this code. If you used the Option 1 - Expert option , you can consult this code as a guide. Click here to see code # Error handling: # surround the call to RecommendationService in a try catch try: # Call the getRecommendation API on the RecommendationService response = call_getRecommendation(self.region, user_id) # Parses value of recommendation from DynamoDB JSON return value # {'Item': { # 'ServiceAPI': {'S': 'getRecommendation'}, # 'UserID': {'N': '1'}, # 'Result': {'S': 'M*A*S*H'}, ... tv_show = response['Item']['Result']['S'] user_name = response['Item']['UserName']['S'] message += recommendation_message (user_name, tv_show, True) # Error handling: # If the service dependency fails, and we cannot make a personalized recommendation # then give a pre-selected (static) recommendation # and report diagnostic information except Exception as e: message += recommendation_message ('Valued Customer', 'I Love Lucy', False) message += '<br><br><br><h2>Diagnostic Info:</h2>' message += '<br>We are unable to provide personalized recommendations' message += '<br>If this persists, please report the following info to us:' message += str(traceback.format_exception_only(e.__class__, e)) 2.3.4 Observe behavior of web service with added error handling After the new error-handling code has successfully deployed, refresh the test web service page multiple times. Observe: It works. It no longer returns an error All three EC2 instances and Availability Zones are being used A default recommendation for Valued Customer is displayed instead of a user-personalized one There is now Diagnostic Info . What does it mean? Refer back to the newly deployed code to understand why the website behaves this way now The Website is working again, but in a degraded capacity since it is no longer serving personalized recommendations. While this is less than ideal, it is much better than when it was failing with http 502 errors. The RecommendationService is not available, so the app instead returns a static response (the default recommendation) instead of the data it would have obtained from RecommendationService . Well-Architected for Reliability: Best practice Implement graceful degradation to transform applicable hard dependencies into soft dependencies : When a component's dependencies are unhealthy, the component itself can still function, although in a degraded manner. For example, when a dependency call fails, instead use a predetermined static response. 3. Implement deep health checks 3.1 Re-enable the dependency service For the next part of the lab restore access to the getRecommendation API on the RecommendationService Return to the AWS Systems Manager > Parameter Store on the AWS Management Console Set the value of RecommendationServiceEnabled back to true and Save changes Confirm the web service is now returning \"personalized\" recommendations again 3.2 Inject fault on a single server Previously you simulated a failure of the service dependency. Now you will simulate a failure on a single server (of the three servers running). You will simulate a fault on this server that prevents only it from calling the otherwise healthy service dependency. Navigate to the EC2 Instances console There should be three EC2 instances with Instance State running , one in each Availability Zone (they will have Name WebApp1 ) Click the gear icon in the upper-right and select IAM Instance Profile Name (in addition to what is already selected) Select only the EC2 instance in Availability Zone us-east-2c Click Action > Instance Settings > Attach/Replace IAM Role From IAM role , click WebApp1-EC2-noDDB-Role-HealthCheckLab Click Apply Click Close This will return you to the EC2 Instances console. Observe under IAM Instance Profile Name (it is one of the displayed columns) which IAM roles each EC2 instance has attached The IAM role attached to an EC2 instance determines what permissions it has to access AWS resources. You changed the role of the us-east-2c instance to one that is almost the same as the other two, except it does not have access to DynamoDB. Since DynamoDB is used to mock our service dependency, the us-east-2c server no longer has access to the service dependency ( RecommendationService ). Stale credentials is an actual fault that servers might experience. Your actions above simulate stale (invalid) credentials on the us-east-2c server. 3.4 Observe application behavior and determine how to fix it Observe the website behavior now Refresh the website multiple times noting which Availability Zone the serving the request The servers in us-east-2a and us-east-2b continue to function normally The server in us-east-2c still succeeds, but it uses the static response. Why is this? The service dependency RecommendationServiceEnabled is still healthy It is the server in us-east-2c that is unhealthy - it has stale credentials Return to the Target Groups and under the Targets tab observe the results of the ELB health checks They are all Status healthy , and are therefore all receiving traffic. Why does the server in us-east-2c show healthy for this check? The service would deliver a better experience if it: Identified the us-east-2c server as unhealthy and did not route traffic to it Replaced this server with a healthy one Well-Architected for Reliability: Best practices Make services stateless where possible : Services should either not require state, or should offload state such that between different client requests, there is no dependence on locally stored data on disk or in memory. This enables servers to be replaced at will without causing an availability impact. Amazon ElastiCache or Amazon DynamoDB are good destinations for offloaded state. Automate healing on all layers : Upon detection of a failure, use automated capabilities to perform actions to remediate. Ability to restart is an important tool to remediate failures. As discussed previously for distributed systems, a best practice is to make services stateless where possible. This prevents loss of data or availability on restart. In the cloud, you can (and generally should) replace the entire resource (for example, EC2 instance, or Lambda function) as part of the restart. The restart itself is a simple and reliable way to recover from failure. Many different types of failures occur in workloads. Failures can occur in hardware, software, communications, and operations. Rather than constructing novel mechanisms to trap, identify, and correct each of the different types of failures, map many different categories of failures to the same recovery strategy. An instance might fail due to hardware failure, an operating system bug, memory leak, or other causes. Rather than building custom remediation for each situation, treat any of them as an instance failure. Terminate the instance, and allow AWS Auto Scaling to replace it. Later, carry out the analysis on the failed resource out of band. From the Target Groups console click on the the Health checks tab The ELB health check is configured to return healthy when it receives an http 200 response on the /healthcheck path Since the healthcheck code simply always returns http 200, the bad server still returns http 200 and is seen as healthy . 3.4 Create a deep healthcheck to identify bad servers Update server code to add a deep health check response You will create and configure a new health check that will include a check on whether the server can access its dependency This is a deep health check -- it checks the actual function of the server including the ability to call service dependencies This will be implemented by updating the server code on the /healthcheck path Choose one of the options below ( Option 1 - Expert or Option 2 - Assisted ) to improve the code and add the deep health check. 3.4.1 Option 1 - Expert option: make and deploy your changes to the code You may choose this option, or skip to Option 2 - Assisted option This option requires you have access to place a file in a location accessible via https/https via a URL. For example a public readable S3 bucket, gist (use the raw option to get the URL), or your private webserver. Start the existing server code that you added error handling to, or alternatively download the lab sample code from here: server_errorhandling.py Calls to /healthcheck should in turn make a test call to RecommendationService using User ID 0 If the RecommendationService returns the string test for both Result and UserName then it is healthy If it is healthy then return http code 200 (OK) If it is not healthy then return http code 503 (Service Unavailable) Also return the same EC2 meta-data that is returned on the call to the / path Put your updated server code in a location where it can be downloaded via its URL using wget In the AWS Console go the HealthCheckLab CloudFormation stack and Update it: Leave Use current template selected and click Next Find the ServerCodeUrl parameter and enter the URL for your new code When stack status is CREATE_COMPLETE (about four minutes) then continue If you completed the Option 1 - Expert option , then skip the Option 2 - Assisted option section and continue with 3.4.3 Health check code 3.4.2 Option 2 - Assisted option: deploy workshop provided code The new server code including error handling can be viewed here Search for Healthcheck request in the comments. What will this code do now if called on this health check URL? Deploy the new health check code Navigate to the AWS CloudFormation console Click on the HealthCheckLab stack Click Update Leave Use current template selected and click Next Find the ServerCodeUrl parameter and enter the following: https://aws-well-architected-labs-ohio.s3.us-east-2.amazonaws.com/Healthcheck/Code/server_healthcheck.py Click Next until the last page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Update stack Click on Events , and click the refresh icon to observe the stack progress New EC2 instances running the error handling code are being deployed When stack status is CREATE_COMPLETE (about four minutes) then continue 3.4.3 Health check code This is the health check code from server_healthcheck.py . The Option 2 - Assisted option uses this code. If you used the Option 1 - Expert option , you can consult this code as a guide. Click here to see code # Healthcheck request - will be used by the Elastic Load Balancer elif self.path == '/healthcheck': is_healthy = False error_msg = '' TEST = 'test' # Make a request to RecommendationService using a predefined # test call as part of health assessment for this server try: # call RecommendationService using the test user user_id = str(0) response = call_getRecommendation(self.region, user_id) # Parses value of recommendation from DynamoDB JSON return value tv_show = response['Item']['Result']['S'] user_name = response['Item']['UserName']['S'] # Server is healthy of RecommendationService returned the expected response is_healthy = (tv_show == TEST) and (user_name == TEST) # If the service dependency fails, capture diagnostic info except Exception as e: error_msg += str(traceback.format_exception_only(e.__class__, e)) # Based on the health assessment # If it succeeded return a healthy code # If it failed return a server failure code message = \"\" if (is_healthy): self.send_response(200) self.send_header('Content-type', 'text/html') self.end_headers() message += \"<h1>Success</h1>\" # Add metadata message += get_metadata() else: self.send_response(503) self.send_header('Content-type', 'text/html') self.end_headers() message += \"<h1>Fail</h1>\" message += \"<h3>Error message:</h3>\" message += error_msg # Add metadata message += get_metadata() self.wfile.write( bytes( html.format(Title=\"healthcheck\", Content=message), \"utf-8\" ) ) 3.4.4 Verify Elastic Load Balancer (ELB) is configured to use the new deep health check From the Target Groups console click on the the Health checks tab For Path verify the value is /healthcheck Click the Targets tab so you can monitor health check status 3.4.5 Observe behavior of web service with added deep health check Continue the lab after the HealthCheckLab CloudFormation stack is complete. The CloudFormation stack update reset the EC2 instance IAM roles, so the system is back to its original no-fault state. You will re-introduce the single-server fault and observe the new behavior. Refresh the web service multiple times and note all three servers are functioning without error Copy the URL of the web service to a new tab and append /healthcheck to the end of the URL The new URL should look like: http://healt-alb1l-<...>.elb.amazonaws.com/healthcheck Refresh several times and observe the health check on the three servers Note the check is successful - the check now includes a call to the RecommendationService (the DynamoDB table) Go to the Target Groups console click on the Targets tab and note the health status as per the ELB health checks. To re-introduce the stale credentials fault, again change the IAM role for the EC2 instance in us-east-2c to WebApp1-EC2-noDDB-Role-HealthCheckLab See 3.2 Inject fault on one of the servers if you need a reminder of how to do this. Go to the Target Groups console click on the Targets tab and note the health status as per the ELB health checks (remember to refresh) Note that the server in us-east-2c is now failing the health check with a http code 503 Service Not Available With an Interval of 15 seconds, and a Healthy threshold of 2 , it can take up to 30 seconds to see the status update. The ELB has identified the us-east-2c server as unhealthy and will not route traffic to it This is known as fail-closed behavior Refresh the web service multiple times and note it is however still functioning without error And unlike before it is no longer returning a static response - it only returns personalized recommendations Note that only the servers in us-east-2a and us-east-2b are serving requests Well-Architected for Reliability: Best practices Monitor all components of the workload to detect failures : Continuously monitor the health of your workload so that you and your automated systems are aware of degradation or complete failure as soon as they occur. Failover to healthy resources : Ensure that if a resource failure occurs, that healthy resources can continue to serve requests. Well-Architected for Reliability: Health Checks The load balancer will only route traffic to healthy application instances. The health check needs to be at the data plane/application layer indicating the capability of the application on the instance. This check should not be against the control plane. A health check URL for the web application will be present and configured for use by the load balancer Repair the server Navigate to the EC2 Instances console and select only the instance in us-east-2c Click Action > Instance State > Terminate Click Yes, Terminate The EC2 instance will shut down Amazon EC2 Auto Scaling will recognize there are less then the three Desired Capacity and will start up a new EC2 instance The new instance replaces the one with the stale credentials fault, and loads fresh credentials From the Target Groups console Targets tab note the health check status of the new server in us-east-2c The new instance in us-east-2c will first show Description Target registration is in progress Then Description is This target is currently passing target group's health checks , then you may continue the workshop (The Description may show Health checks failed with these codes: [502] , before getting to a healthy state. This is expected as the server initializes) From the time you terminate the EC2 instance, it will take four to five minutes to get the new EC2 instance up and in a healthy state Refresh the web service multiple times and note that personalized recommendations are once again being served from all three servers 4. Fail open when appropriate 4.1 Disable RecommendationService again Confirm the service is healthy Refresh the web service multiple times and note that personalized recommendations are being served from all three servers You will now simulate another complete failure of the RecommendationService . Every request will fail for every request on every server Return to the AWS Systems Manager > Parameter Store on the AWS Management Console Set the value of RecommendationServiceEnabled once again to false and Save changes What is the expected behavior? The previous time you simulated a complete failure of the RecommendationService The web service failed with a http 502 error Then you implemented error handling and the following were observed The service returned a static response (as per the error handling code) Since the healthcheck code at that time was configured to only return http 200, it reported healthy status for all servers Now, with the new deep health check in place... What status do you expect the elastic load balancer to report for the servers? How will the AWS Elastic Load Balancer handle traffic routing to the servers? 4.2 Observe fail-open behavior Refresh the web service multiple times Look at which servers (and Availability Zones) are serving requests Note that the service does not fail But as expected (without access to RecommendationServiceEnabled ) it always serves static responses Refresh the health check URL multiple times The deep health detects that RecommendationServiceEnabled is not available and returns a failure code for all servers From the Target Groups console Targets tab note the health check status of all the servers (you may need ot refresh) They all report unhealthy with http code 503. This is the code the deep health check is configured to return when the dependency is not available Note the message at the top of the tab (if you do not see a message, try refreshing the entire page using the browser refresh function) The Amazon Builders' Library: Implementing health checks When an individual server fails a health check, the load balancer stops sending it traffic. But when all servers fail health checks at the same time, the load balancer fails open, allowing traffic to all servers. When we rely on fail-open behavior, we make sure to test the failure modes of the dependency heath check. A system set to fail-open does not shut down when failure conditions are present. Instead, the system remains \u201copen\u201d and operations continue. The AWS Application Load Balancer here exhibits this fail-open behavior and the service continues to serve requests sent to it by the load balancer. Reset the value of RecommendationServiceEnabled to true and observe that the service resumes serving personalized recommendations. The RecommendationServiceEnabled parameter was initially intended to simulate the failure of RecommendationService for this lab But now that we have implemented fail-open behavior and graceful degradation we could use the RecommendationServiceEnabled parameter as an emergency lever to cuto-off traffic to RecommendationService if there was a serious problem with it. Well-Architected for Reliability: Best practice Implement emergency levers : These are rapid processes that may mitigate availability impact on your workload. They can be operated in the absence of a root cause. An ideal emergency lever reduces the cognitive burden on the resolvers to zero by providing fully deterministic activation and deactivation criteria. Example levers include blocking all robot traffic or serving a static response. Levers are often manual, but they can also be automated. 5. Tear down this lab If you are attending an in-person workshop and were provided with an AWS account by the instructor : There is no need to tear down the lab. Feel free to continue exploring. Log out of your AWS account when done. If you are using your own AWS account : You may leave these resources deployed for as long as you want. When you are ready to delete these resources, see the following instructions Remove AWS CloudFormation provisioned resources How to delete an AWS CloudFormation stack If you are already familiar with how to delete an AWS CloudFormation stack, then skip to the next section: Delete workshop CloudFormation stacks Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion Click the stack name Select the Events column Refresh to see new events Delete workshop CloudFormation stacks First delete the HealthCheckLab CloudFormation stack Wait for the HealthCheckLab CloudFormation stack to complete (it will no longer be shown on the list of actice stacks) Then delete the WebApp1-VPC CloudFormation stack Remove CloudWatch logs After deletion of the WebApp1-VPC CloudFormation stack is complete then delete the CloudWatch Logs: Open the CloudFormation console at https://console.aws.amazon.com/cloudwatch/ . Click Logs in the left navigation. Click the radio button on the left of the WebApp1-VPC-VPCFlowLogGroup-\\<some unique ID> . Click the Actions Button then click Delete Log Group . Verify the log group name then click Yes, Delete . References & useful resources Patterns for Resilient Architecture \u2014 Part 3 Amazon Builders' Library: Implementing health checks Well-Architected Framework (see the Reliability pillar) Well-Architected best practices for reliability Health Checks for Your Target Groups (for your Application Load Balancer) License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#level-300-implementing-health-checks-and-managing-dependencies-to-improve-reliability","text":"","title":"Level 300: Implementing Health Checks and Managing Dependencies to Improve Reliability"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#author","text":"Seth Eliot, Reliability Lead, Well-Architected, AWS","title":"Author"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#amazon-builders-library-and-aws-well-architected","text":"This lab illustrates best practices for reliability as described in the AWS Well-Architected Reliability pillar. It focuses on resiliency practices described in the Amazon Builders' Library article: Implementing health checks .","title":"Amazon Builders' Library and AWS Well-Architected"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#table-of-contents","text":"Deploy the application Handle failure of service dependencies Implement deep health checks Fail open when appropriate Tear down this lab","title":"Table of Contents"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#1-deploy-the-application","text":"You will create a multi-tier architecture using AWS and run a simple service on it. The service is a web server running on Amazon EC2 fronted by an Elastic Load Balancer reverse-proxy, with a dependency on Amazon DynamoDB. Note : The concepts covered by this lab apply whether your service dependency is an AWS resource like Amazon DynamoDB, or an external service called via API. The DynamoDB dependency here acts as a mock for an external service called RecommendationService . The getRecommendation API on this service is a dependency for the web service used in this lab. getRecommendation is actually a get_item call to a DynamoDB table.","title":"1. Deploy the application "},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#11-log-into-the-aws-console","text":"If you are attending an in-person workshop and were provided with an AWS account by the instructor : Follow the instructions here for accessing your AWS account If you are using your own AWS account : Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab.","title":"1.1 Log into the AWS console "},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#12-deploy-the-application-using-an-aws-cloudformation-template","text":"You will deploy the service infrastructure including simple service code and some sample data. It is recommended that you use the Ohio region. This region is also known as us-east-2 , which you will see referenced throughout this lab. If you choose to use a different region, you will need to ensure future steps are consistent with your region choice.","title":"1.2 Deploy the application using an AWS CloudFormation template"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#121-deploy-the-vpc-infrastructure","text":"If you are comfortable deploying a CloudFormation stack, then use the express steps listed immediately below. If you need additional guidance in how to deploy a CloudFormation stack, then follow the directions for the Automated Deployment of VPC lab, and then return here for the next step: 1.2.2 Deploy the web service infrastructure and service","title":"1.2.1 Deploy the VPC infrastructure"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#express-steps-deploy-the-vpc-infrastructure","text":"Download the vpc-alb-app-db.yaml CloudFormation template Create a CloudFormation stack (with new resources) by uploading this CloudFormation Template file For Stack name use WebApp1-VPC (case sensitive) Leave all CloudFormation Parameters at their default values Click Next until the last page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack","title":"Express Steps (Deploy the VPC infrastructure)"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#122-deploy-the-web-app-infrastructure-and-service","text":"Wait until the VPC CloudFormation stack status is CREATE_COMPLETE , then continue. This will take about four minutes. If you are comfortable deploying a CloudFormation stack, then use the express steps listed immediately below. If you need additional guidance in how to deploy a CloudFormation stack, then follow the directions for the Create an AWS CloudFormation Stack from a template lab, and then return here for the next step: 1.3 View the website for web service","title":"1.2.2 Deploy the web app infrastructure and service"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#express-steps-deploy-the-webapp-infrastructure-and-service","text":"Download the staticwebapp.yaml CloudFormation template Create a CloudFormation stack (with new resources) by uploading this CloudFormation Template file For Stack name use HealthCheckLab Leave all CloudFormation Parameters at their default values Click Next until the last page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack","title":"Express Steps (Deploy the WebApp infrastructure and service)"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#13-view-the-website-for-web-service","text":"Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation . Wait until HealthCheckLab stack status is CREATE_COMPLETE before proceeding. This should take about four minutes Click on the HealthCheckLab stack Click on the Outputs tab For the Key WebsiteURL copy the value. This is the URL of your test web service Hint : it will start with http://healt-alb and end in <aws region>.elb.amazonaws.com Click the URL and it will bring up the website: The website simulates a recommendation engine making personalized suggestions for classic television shows. You should note the following features: Area A shows the personalized recommendation It shows first name of the user and the show that was recommended The workshop simulation is simple. On every request it chooses a user at random, and shows a recommendation statically mapped to that user. The user names, television show names, and this mapping are in a DynamoDB table, which is simulating the RecommendationService Area B shows metadata which is useful to you during the lab The instance_id and availability_zone enable you to see which EC2 server and Availability Zone were used for each request There is one EC2 instance deployed per Availability Zone Refresh the website several times, note that the EC2 instance and Availability Zone change from among the three available This is Elastic Load Balancing (ELB) distributing these stateless requests among the available EC2 server instances across Availability Zones Well-Architected for Reliability: Best practices Use highly available network connectivity for your workload public endpoints : Elastic Load Balancing provides load balancing across Availability Zones, performs Layer 4 (TCP) or Layer 7 (http/https) routing, integrates with AWS WAF, and integrates with AWS Auto Scaling to help create a self-healing infrastructure and absorb increases in traffic while releasing resources when traffic decreases. Implement loosely coupled dependencies : Dependencies such as queuing systems, streaming systems, workflows, and load balancers are loosely coupled. Loose coupling helps isolate behavior of a component from other components that depend on it, increasing resiliency and agility. Deploy the workload to multiple locations : Distribute workload data and resources across multiple Availability Zones or, where necessary, across AWS Regions. These locations can be as diverse as required.","title":"1.3 View the website for web service "},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#2-handle-failure-of-service-dependencies","text":"","title":"2. Handle failure of service dependencies "},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#21-system-dependency-initially-healthy","text":"You already observed that all three EC2 instances are successfully serving requests In a new tab navigate to ELB Target Groups console By clicking here to open the AWS Management Console or navigating through the AWS Management Console: Services > EC2 > Load Balancing > Target Groups Leave this tab open as you will be referring back to it multiple times Click on the Targets tab (bottom half of screen) Under Registered Targets observe the three EC2 instances serving your web service Note that they are all healthy (see Status and Description ) In this state the ELB will route traffic to any of the three servers From the Target Groups console, now click on the the Health checks tab Note here that the Path is configured to /healthcheck Copy the URL of the web service to a new tab and append /healthcheck to the end of the URL The new URL should look like: http://healt-alb1l-<...>.elb.amazonaws.com/healthcheck Refresh several times and observe the health check on the three servers Note the check is successful The EC2 servers receive user requests (for a TV show recommendation) on the path / and they receive health check requests from the Elastic Load Balancer on the path /healthcheck The health check always returns an http 200 code for any request to it. The server code running on each EC2 instance can be viewed here , or you can view the health check code excerpt below: Click here to see the health check code excerpt # Healthcheck request - will be used by the Elastic Load Balancer elif self.path == '/healthcheck': # Return a healthy code self.send_response(200) self.send_header('Content-type', 'text/html') self.end_headers()","title":"2.1 System dependency initially healthy"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#22-simulate-dependency-not-available","text":"","title":"2.2 Simulate dependency not available"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#221-disable-recommendationservice","text":"You will now simulate a complete failure of the RecommendationService . Every request in turn makes a (simulated) call to the getRecommendation API on this service. These will all fail for every request on every server. In a new tab, navigate to the Parameter Store on the AWS Systems Manager console By clicking here to open the AWS Management Console or navigating through the AWS Management Console: Services > Systems Manager > Parameter Store Leave this tab open as you will be referring back to it one additional time Click on RecommendationServiceEnabled Click Edit In the Value box, type false Click Save Changes A status message should say Edit parameter request succeeded The RecommendationServiceEnabled parameter is used only for this lab. The server code reads its value, and simulates a failure in RecommendationService (all reads to the DynamoDB table simulating the service will fail) when it is false .","title":"2.2.1 Disable RecommendationService"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#222-observe-behavior-when-dependency-not-available","text":"Refresh the test web service multiple times Note that it fails with 502 Bad Gateway For each request one of the servers receiving the request attempts to call the RecommendationService but catastrophically fails and fails to return a reply (empty reply) to the load balancer, which in turn presents this as a http 502 failure. You can observe this by opening a new tab and navigating to ELB Load Balancers console: By clicking here to open the AWS Management Console or navigating through the AWS Management Console: Services > EC2 > Load Balancing > Load Balancers Click on the Monitoring tab (bottom half of screen) Observe the ELB 5XXs (Count) and HTTP 502s (Count) errors for the load balancer It will take a minute for the metrics to show up. Make sure you refresh the web service page multiple times in your browser These are the error codes the load balancer returns on every request during this simulated outage Compare these metrics to those for the target group (the EC2 servers themselves) Return to the Target Groups console and click the Monitoring tab there Observe HTTP 5XXs ( Count ) errors shows no data The servers themselves are not returning actual http error codes, they are failing to return any data at all We need to update the server code to handle when the dependency is not available","title":"2.2.2 Observe behavior when dependency not available"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#23-update-server-code-to-handle-dependency-not-available","text":"The getRecommendation API is actually a get_item call on a DynamoDB table. Examine the server code to see how errors are currently handled The server code running on each EC2 instance can be viewed here Search for the call to the RecommendationService . It looks like this: response = call_getRecommendation(self.region, user_id) What happens if this call fails? Choose one of the options below ( Option 1 - Expert or Option 2 - Assisted ) to improve the code and handle the failure","title":"2.3 Update server code to handle dependency not available"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#231-option-1-expert-option-make-and-deploy-your-changes-to-the-code","text":"You may choose this option, or skip to Option 2 - Assisted option This option requires you have access to place a file in a location accessible via https/https via a URL. For example a public readable S3 bucket, gist (use the raw option to get the URL), or your private webserver. Download the existing server code from here: server_basic.py Modify the code to handle the call to the RecommendationService When the call to RecommendationService fails then instead of using the response data you requested and did not get, return a static response: Instead of user first name return Valued Customer Instead of a personalized recommended TV show, return I Love Lucy Try to also return some diagnostic information on the cause of the error Put your updated server code in a location where it can be downloaded via its URL using wget In the AWS Console go the HealthCheckLab CloudFormation stack and Update it: Leave Use current template selected and click Next Find the ServerCodeUrl parameter and enter the URL for your new code When stack status is CREATE_COMPLETE (about four minutes) then continue If you completed the Option 1 - Expert option , then skip the Option 2 - Assisted option section and continue with 2.3.3 Error handling code","title":"2.3.1 Option 1 - Expert option: make and deploy your changes to the code"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#232-option-2-assisted-option-deploy-workshop-provided-code","text":"The new server code including error handling can be viewed here Search for Error handling in the comments (occurs twice). What will this code do now if the dependency call fails?","title":"2.3.2 Option 2 - Assisted option: deploy workshop provided code"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#deploy-the-new-error-handling-code","text":"Navigate to the AWS CloudFormation console Click on the HealthCheckLab stack Click Update Leave Use current template selected and click Next Find the ServerCodeUrl parameter and enter the following: https://aws-well-architected-labs-ohio.s3.us-east-2.amazonaws.com/Healthcheck/Code/server_errorhandling.py Click Next until the last page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Update stack Click on Events , and click the refresh icon to observe the stack progress New EC2 instances running the error handling code are being deployed When stack status is CREATE_COMPLETE (about four minutes) then continue","title":"Deploy the new error handling code"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#233-error-handling-code","text":"This is the error handling code from server_errorhandling.py . The Option 2 - Assisted option uses this code. If you used the Option 1 - Expert option , you can consult this code as a guide. Click here to see code # Error handling: # surround the call to RecommendationService in a try catch try: # Call the getRecommendation API on the RecommendationService response = call_getRecommendation(self.region, user_id) # Parses value of recommendation from DynamoDB JSON return value # {'Item': { # 'ServiceAPI': {'S': 'getRecommendation'}, # 'UserID': {'N': '1'}, # 'Result': {'S': 'M*A*S*H'}, ... tv_show = response['Item']['Result']['S'] user_name = response['Item']['UserName']['S'] message += recommendation_message (user_name, tv_show, True) # Error handling: # If the service dependency fails, and we cannot make a personalized recommendation # then give a pre-selected (static) recommendation # and report diagnostic information except Exception as e: message += recommendation_message ('Valued Customer', 'I Love Lucy', False) message += '<br><br><br><h2>Diagnostic Info:</h2>' message += '<br>We are unable to provide personalized recommendations' message += '<br>If this persists, please report the following info to us:' message += str(traceback.format_exception_only(e.__class__, e))","title":"2.3.3 Error handling code"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#234-observe-behavior-of-web-service-with-added-error-handling","text":"After the new error-handling code has successfully deployed, refresh the test web service page multiple times. Observe: It works. It no longer returns an error All three EC2 instances and Availability Zones are being used A default recommendation for Valued Customer is displayed instead of a user-personalized one There is now Diagnostic Info . What does it mean? Refer back to the newly deployed code to understand why the website behaves this way now The Website is working again, but in a degraded capacity since it is no longer serving personalized recommendations. While this is less than ideal, it is much better than when it was failing with http 502 errors. The RecommendationService is not available, so the app instead returns a static response (the default recommendation) instead of the data it would have obtained from RecommendationService . Well-Architected for Reliability: Best practice Implement graceful degradation to transform applicable hard dependencies into soft dependencies : When a component's dependencies are unhealthy, the component itself can still function, although in a degraded manner. For example, when a dependency call fails, instead use a predetermined static response.","title":"2.3.4 Observe behavior of web service with added error handling"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#3-implement-deep-health-checks","text":"","title":"3. Implement deep health checks "},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#31-re-enable-the-dependency-service","text":"For the next part of the lab restore access to the getRecommendation API on the RecommendationService Return to the AWS Systems Manager > Parameter Store on the AWS Management Console Set the value of RecommendationServiceEnabled back to true and Save changes Confirm the web service is now returning \"personalized\" recommendations again","title":"3.1 Re-enable the dependency service"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#32-inject-fault-on-a-single-server","text":"Previously you simulated a failure of the service dependency. Now you will simulate a failure on a single server (of the three servers running). You will simulate a fault on this server that prevents only it from calling the otherwise healthy service dependency. Navigate to the EC2 Instances console There should be three EC2 instances with Instance State running , one in each Availability Zone (they will have Name WebApp1 ) Click the gear icon in the upper-right and select IAM Instance Profile Name (in addition to what is already selected) Select only the EC2 instance in Availability Zone us-east-2c Click Action > Instance Settings > Attach/Replace IAM Role From IAM role , click WebApp1-EC2-noDDB-Role-HealthCheckLab Click Apply Click Close This will return you to the EC2 Instances console. Observe under IAM Instance Profile Name (it is one of the displayed columns) which IAM roles each EC2 instance has attached The IAM role attached to an EC2 instance determines what permissions it has to access AWS resources. You changed the role of the us-east-2c instance to one that is almost the same as the other two, except it does not have access to DynamoDB. Since DynamoDB is used to mock our service dependency, the us-east-2c server no longer has access to the service dependency ( RecommendationService ). Stale credentials is an actual fault that servers might experience. Your actions above simulate stale (invalid) credentials on the us-east-2c server.","title":"3.2 Inject fault on a single server "},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#34-observe-application-behavior-and-determine-how-to-fix-it","text":"Observe the website behavior now Refresh the website multiple times noting which Availability Zone the serving the request The servers in us-east-2a and us-east-2b continue to function normally The server in us-east-2c still succeeds, but it uses the static response. Why is this? The service dependency RecommendationServiceEnabled is still healthy It is the server in us-east-2c that is unhealthy - it has stale credentials Return to the Target Groups and under the Targets tab observe the results of the ELB health checks They are all Status healthy , and are therefore all receiving traffic. Why does the server in us-east-2c show healthy for this check? The service would deliver a better experience if it: Identified the us-east-2c server as unhealthy and did not route traffic to it Replaced this server with a healthy one Well-Architected for Reliability: Best practices Make services stateless where possible : Services should either not require state, or should offload state such that between different client requests, there is no dependence on locally stored data on disk or in memory. This enables servers to be replaced at will without causing an availability impact. Amazon ElastiCache or Amazon DynamoDB are good destinations for offloaded state. Automate healing on all layers : Upon detection of a failure, use automated capabilities to perform actions to remediate. Ability to restart is an important tool to remediate failures. As discussed previously for distributed systems, a best practice is to make services stateless where possible. This prevents loss of data or availability on restart. In the cloud, you can (and generally should) replace the entire resource (for example, EC2 instance, or Lambda function) as part of the restart. The restart itself is a simple and reliable way to recover from failure. Many different types of failures occur in workloads. Failures can occur in hardware, software, communications, and operations. Rather than constructing novel mechanisms to trap, identify, and correct each of the different types of failures, map many different categories of failures to the same recovery strategy. An instance might fail due to hardware failure, an operating system bug, memory leak, or other causes. Rather than building custom remediation for each situation, treat any of them as an instance failure. Terminate the instance, and allow AWS Auto Scaling to replace it. Later, carry out the analysis on the failed resource out of band. From the Target Groups console click on the the Health checks tab The ELB health check is configured to return healthy when it receives an http 200 response on the /healthcheck path Since the healthcheck code simply always returns http 200, the bad server still returns http 200 and is seen as healthy .","title":"3.4 Observe application behavior and determine how to fix it"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#34-create-a-deep-healthcheck-to-identify-bad-servers","text":"Update server code to add a deep health check response You will create and configure a new health check that will include a check on whether the server can access its dependency This is a deep health check -- it checks the actual function of the server including the ability to call service dependencies This will be implemented by updating the server code on the /healthcheck path Choose one of the options below ( Option 1 - Expert or Option 2 - Assisted ) to improve the code and add the deep health check.","title":"3.4 Create a deep healthcheck to identify bad servers"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#341-option-1-expert-option-make-and-deploy-your-changes-to-the-code","text":"You may choose this option, or skip to Option 2 - Assisted option This option requires you have access to place a file in a location accessible via https/https via a URL. For example a public readable S3 bucket, gist (use the raw option to get the URL), or your private webserver. Start the existing server code that you added error handling to, or alternatively download the lab sample code from here: server_errorhandling.py Calls to /healthcheck should in turn make a test call to RecommendationService using User ID 0 If the RecommendationService returns the string test for both Result and UserName then it is healthy If it is healthy then return http code 200 (OK) If it is not healthy then return http code 503 (Service Unavailable) Also return the same EC2 meta-data that is returned on the call to the / path Put your updated server code in a location where it can be downloaded via its URL using wget In the AWS Console go the HealthCheckLab CloudFormation stack and Update it: Leave Use current template selected and click Next Find the ServerCodeUrl parameter and enter the URL for your new code When stack status is CREATE_COMPLETE (about four minutes) then continue If you completed the Option 1 - Expert option , then skip the Option 2 - Assisted option section and continue with 3.4.3 Health check code","title":"3.4.1 Option 1 - Expert option: make and deploy your changes to the code"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#342-option-2-assisted-option-deploy-workshop-provided-code","text":"The new server code including error handling can be viewed here Search for Healthcheck request in the comments. What will this code do now if called on this health check URL?","title":"3.4.2 Option 2 - Assisted option: deploy workshop provided code"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#deploy-the-new-health-check-code","text":"Navigate to the AWS CloudFormation console Click on the HealthCheckLab stack Click Update Leave Use current template selected and click Next Find the ServerCodeUrl parameter and enter the following: https://aws-well-architected-labs-ohio.s3.us-east-2.amazonaws.com/Healthcheck/Code/server_healthcheck.py Click Next until the last page At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Update stack Click on Events , and click the refresh icon to observe the stack progress New EC2 instances running the error handling code are being deployed When stack status is CREATE_COMPLETE (about four minutes) then continue","title":"Deploy the new health check code"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#343-health-check-code","text":"This is the health check code from server_healthcheck.py . The Option 2 - Assisted option uses this code. If you used the Option 1 - Expert option , you can consult this code as a guide. Click here to see code # Healthcheck request - will be used by the Elastic Load Balancer elif self.path == '/healthcheck': is_healthy = False error_msg = '' TEST = 'test' # Make a request to RecommendationService using a predefined # test call as part of health assessment for this server try: # call RecommendationService using the test user user_id = str(0) response = call_getRecommendation(self.region, user_id) # Parses value of recommendation from DynamoDB JSON return value tv_show = response['Item']['Result']['S'] user_name = response['Item']['UserName']['S'] # Server is healthy of RecommendationService returned the expected response is_healthy = (tv_show == TEST) and (user_name == TEST) # If the service dependency fails, capture diagnostic info except Exception as e: error_msg += str(traceback.format_exception_only(e.__class__, e)) # Based on the health assessment # If it succeeded return a healthy code # If it failed return a server failure code message = \"\" if (is_healthy): self.send_response(200) self.send_header('Content-type', 'text/html') self.end_headers() message += \"<h1>Success</h1>\" # Add metadata message += get_metadata() else: self.send_response(503) self.send_header('Content-type', 'text/html') self.end_headers() message += \"<h1>Fail</h1>\" message += \"<h3>Error message:</h3>\" message += error_msg # Add metadata message += get_metadata() self.wfile.write( bytes( html.format(Title=\"healthcheck\", Content=message), \"utf-8\" ) )","title":"3.4.3 Health check code"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#344-verify-elastic-load-balancer-elb-is-configured-to-use-the-new-deep-health-check","text":"From the Target Groups console click on the the Health checks tab For Path verify the value is /healthcheck Click the Targets tab so you can monitor health check status","title":"3.4.4 Verify Elastic Load Balancer (ELB) is configured to use the new deep health check"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#345-observe-behavior-of-web-service-with-added-deep-health-check","text":"Continue the lab after the HealthCheckLab CloudFormation stack is complete. The CloudFormation stack update reset the EC2 instance IAM roles, so the system is back to its original no-fault state. You will re-introduce the single-server fault and observe the new behavior. Refresh the web service multiple times and note all three servers are functioning without error Copy the URL of the web service to a new tab and append /healthcheck to the end of the URL The new URL should look like: http://healt-alb1l-<...>.elb.amazonaws.com/healthcheck Refresh several times and observe the health check on the three servers Note the check is successful - the check now includes a call to the RecommendationService (the DynamoDB table) Go to the Target Groups console click on the Targets tab and note the health status as per the ELB health checks. To re-introduce the stale credentials fault, again change the IAM role for the EC2 instance in us-east-2c to WebApp1-EC2-noDDB-Role-HealthCheckLab See 3.2 Inject fault on one of the servers if you need a reminder of how to do this. Go to the Target Groups console click on the Targets tab and note the health status as per the ELB health checks (remember to refresh) Note that the server in us-east-2c is now failing the health check with a http code 503 Service Not Available With an Interval of 15 seconds, and a Healthy threshold of 2 , it can take up to 30 seconds to see the status update. The ELB has identified the us-east-2c server as unhealthy and will not route traffic to it This is known as fail-closed behavior Refresh the web service multiple times and note it is however still functioning without error And unlike before it is no longer returning a static response - it only returns personalized recommendations Note that only the servers in us-east-2a and us-east-2b are serving requests Well-Architected for Reliability: Best practices Monitor all components of the workload to detect failures : Continuously monitor the health of your workload so that you and your automated systems are aware of degradation or complete failure as soon as they occur. Failover to healthy resources : Ensure that if a resource failure occurs, that healthy resources can continue to serve requests. Well-Architected for Reliability: Health Checks The load balancer will only route traffic to healthy application instances. The health check needs to be at the data plane/application layer indicating the capability of the application on the instance. This check should not be against the control plane. A health check URL for the web application will be present and configured for use by the load balancer","title":"3.4.5 Observe behavior of web service with added deep health check"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#repair-the-server","text":"Navigate to the EC2 Instances console and select only the instance in us-east-2c Click Action > Instance State > Terminate Click Yes, Terminate The EC2 instance will shut down Amazon EC2 Auto Scaling will recognize there are less then the three Desired Capacity and will start up a new EC2 instance The new instance replaces the one with the stale credentials fault, and loads fresh credentials From the Target Groups console Targets tab note the health check status of the new server in us-east-2c The new instance in us-east-2c will first show Description Target registration is in progress Then Description is This target is currently passing target group's health checks , then you may continue the workshop (The Description may show Health checks failed with these codes: [502] , before getting to a healthy state. This is expected as the server initializes) From the time you terminate the EC2 instance, it will take four to five minutes to get the new EC2 instance up and in a healthy state Refresh the web service multiple times and note that personalized recommendations are once again being served from all three servers","title":"Repair the server"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#4-fail-open-when-appropriate","text":"","title":"4. Fail open when appropriate"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#41-disable-recommendationservice-again","text":"Confirm the service is healthy Refresh the web service multiple times and note that personalized recommendations are being served from all three servers You will now simulate another complete failure of the RecommendationService . Every request will fail for every request on every server Return to the AWS Systems Manager > Parameter Store on the AWS Management Console Set the value of RecommendationServiceEnabled once again to false and Save changes What is the expected behavior? The previous time you simulated a complete failure of the RecommendationService The web service failed with a http 502 error Then you implemented error handling and the following were observed The service returned a static response (as per the error handling code) Since the healthcheck code at that time was configured to only return http 200, it reported healthy status for all servers Now, with the new deep health check in place... What status do you expect the elastic load balancer to report for the servers? How will the AWS Elastic Load Balancer handle traffic routing to the servers?","title":"4.1 Disable RecommendationService again"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#42-observe-fail-open-behavior","text":"Refresh the web service multiple times Look at which servers (and Availability Zones) are serving requests Note that the service does not fail But as expected (without access to RecommendationServiceEnabled ) it always serves static responses Refresh the health check URL multiple times The deep health detects that RecommendationServiceEnabled is not available and returns a failure code for all servers From the Target Groups console Targets tab note the health check status of all the servers (you may need ot refresh) They all report unhealthy with http code 503. This is the code the deep health check is configured to return when the dependency is not available Note the message at the top of the tab (if you do not see a message, try refreshing the entire page using the browser refresh function) The Amazon Builders' Library: Implementing health checks When an individual server fails a health check, the load balancer stops sending it traffic. But when all servers fail health checks at the same time, the load balancer fails open, allowing traffic to all servers. When we rely on fail-open behavior, we make sure to test the failure modes of the dependency heath check. A system set to fail-open does not shut down when failure conditions are present. Instead, the system remains \u201copen\u201d and operations continue. The AWS Application Load Balancer here exhibits this fail-open behavior and the service continues to serve requests sent to it by the load balancer. Reset the value of RecommendationServiceEnabled to true and observe that the service resumes serving personalized recommendations. The RecommendationServiceEnabled parameter was initially intended to simulate the failure of RecommendationService for this lab But now that we have implemented fail-open behavior and graceful degradation we could use the RecommendationServiceEnabled parameter as an emergency lever to cuto-off traffic to RecommendationService if there was a serious problem with it. Well-Architected for Reliability: Best practice Implement emergency levers : These are rapid processes that may mitigate availability impact on your workload. They can be operated in the absence of a root cause. An ideal emergency lever reduces the cognitive burden on the resolvers to zero by providing fully deterministic activation and deactivation criteria. Example levers include blocking all robot traffic or serving a static response. Levers are often manual, but they can also be automated.","title":"4.2 Observe fail-open behavior"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#5-tear-down-this-lab","text":"If you are attending an in-person workshop and were provided with an AWS account by the instructor : There is no need to tear down the lab. Feel free to continue exploring. Log out of your AWS account when done. If you are using your own AWS account : You may leave these resources deployed for as long as you want. When you are ready to delete these resources, see the following instructions","title":"5. Tear down this lab "},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#remove-aws-cloudformation-provisioned-resources","text":"","title":"Remove AWS CloudFormation provisioned resources"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#how-to-delete-an-aws-cloudformation-stack","text":"If you are already familiar with how to delete an AWS CloudFormation stack, then skip to the next section: Delete workshop CloudFormation stacks Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion Click the stack name Select the Events column Refresh to see new events","title":"How to delete an AWS CloudFormation stack"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#delete-workshop-cloudformation-stacks","text":"First delete the HealthCheckLab CloudFormation stack Wait for the HealthCheckLab CloudFormation stack to complete (it will no longer be shown on the list of actice stacks) Then delete the WebApp1-VPC CloudFormation stack","title":"Delete workshop CloudFormation stacks"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#remove-cloudwatch-logs","text":"After deletion of the WebApp1-VPC CloudFormation stack is complete then delete the CloudWatch Logs: Open the CloudFormation console at https://console.aws.amazon.com/cloudwatch/ . Click Logs in the left navigation. Click the radio button on the left of the WebApp1-VPC-VPCFlowLogGroup-\\<some unique ID> . Click the Actions Button then click Delete Log Group . Verify the log group name then click Yes, Delete .","title":"Remove CloudWatch logs"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#references-useful-resources","text":"Patterns for Resilient Architecture \u2014 Part 3 Amazon Builders' Library: Implementing health checks Well-Architected Framework (see the Reliability pillar) Well-Architected best practices for reliability Health Checks for Your Target Groups (for your Application Load Balancer)","title":"References &amp; useful resources"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#license","text":"","title":"License"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Lab_Guide.html#code-license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code License"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Documentation/CFNCreateStack.html","text":"Deploying an AWS CloudFormation stack to create web app infrastructure and service Download the CloudFormation template. (You can right-click then choose Save link as ; or you can right click and copy the link to use with wget ) WebApp: staticwebapp.yaml Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack > With new resources Leave Prepare template setting as-is 1 - For Template source select Upload a template file 2 - Click Choose file and supply the CloudFormation template you downloaded: staticwebapp.yaml Click Next For Stack name use HealthCheckLab Leave all Parameters with their default values and click Next Click Next At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress. This will take approximately five minutes to deploy. When it shows status CREATE_COMPLETE , then you are finished with this step.","title":"Deploying an AWS CloudFormation stack to create web app infrastructure and service"},{"location":"Reliability/300_Health_Checks_and_Dependencies/Documentation/CFNCreateStack.html#deploying-an-aws-cloudformation-stack-to-create-web-app-infrastructure-and-service","text":"Download the CloudFormation template. (You can right-click then choose Save link as ; or you can right click and copy the link to use with wget ) WebApp: staticwebapp.yaml Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click Create Stack > With new resources Leave Prepare template setting as-is 1 - For Template source select Upload a template file 2 - Click Choose file and supply the CloudFormation template you downloaded: staticwebapp.yaml Click Next For Stack name use HealthCheckLab Leave all Parameters with their default values and click Next Click Next At the bottom of the page, select I acknowledge that AWS CloudFormation might create IAM resources with custom names Click Create stack This will take you to the CloudFormation stack status page, showing the stack creation in progress. This will take approximately five minutes to deploy. When it shows status CREATE_COMPLETE , then you are finished with this step.","title":"Deploying an AWS CloudFormation stack to create web app infrastructure and service"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html","text":"Level 300: Testing for Resiliency of EC2, RDS, and S3 https://wellarchitectedlabs.com Introduction The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). One primary capability that AWS provides is the ability to test your systems at a production scale, under load. It is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner. In this lab, you will deploy a 3-tier resource, with a reverse proxy (Application Load Balancer), Web Application on Amazon Elastic Compute Cloud (EC2), and MySQL database using Amazon Relational Database Service (RDS). There is also an option to deploy the same stack into a different region, then using MySQL Read Replicas in the other region deployed with Amazon RDS, and then using AWS Database Migration Service to synchronize the data from the primary region into the secondary region. This will provide you the ability to progress from simpler failure testing of an application to failure testing under a simulated AWS regional failure. The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework If you wish to build this code in this lab, the follow the instructions in the Builders Guide document. Goals: Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Resilience testing of RDS Multi-AZ instances Resilience testing of S3 objects Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR) Prequisites: An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling launch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Overview: Troubleshooting Guide for common problems encountered while deploying and conducting this lab Builders Guide for building the AWS Lambda functions and the web server and where to make changes in the lab guide to use the code you built instead of the publicly available executables. Start the Lab! License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#level-300-testing-for-resiliency-of-ec2-rds-and-s3","text":"https://wellarchitectedlabs.com","title":"Level 300: Testing for Resiliency of EC2, RDS, and S3"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#introduction","text":"The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). One primary capability that AWS provides is the ability to test your systems at a production scale, under load. It is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner. In this lab, you will deploy a 3-tier resource, with a reverse proxy (Application Load Balancer), Web Application on Amazon Elastic Compute Cloud (EC2), and MySQL database using Amazon Relational Database Service (RDS). There is also an option to deploy the same stack into a different region, then using MySQL Read Replicas in the other region deployed with Amazon RDS, and then using AWS Database Migration Service to synchronize the data from the primary region into the secondary region. This will provide you the ability to progress from simpler failure testing of an application to failure testing under a simulated AWS regional failure. The skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework If you wish to build this code in this lab, the follow the instructions in the Builders Guide document.","title":"Introduction"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#goals","text":"Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Resilience testing of RDS Multi-AZ instances Resilience testing of S3 objects Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR)","title":"Goals:"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#prequisites","text":"An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to create Amazon Virtual Private Cloud(s) (VPCs), including subnets, security groups, internet gateways, NAT Gateways, Elastic IP Addresses, and route tables. The credentials must also be able to create the database subnet group needed for a Multi-AZ RDS instance. The credential will need permissions to create IAM Role, instance profiles, AWS Auto Scaling launch configurations, application load balancers, auto scaling group, and EC2 instances. An IAM user or federated credentials into that account that has permissions to deploy the deployment automation, which consists of IAM service linked roles, AWS Lambda functions, and an AWS Step Functions state machine to execute the deployment. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prequisites:"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#overview","text":"Troubleshooting Guide for common problems encountered while deploying and conducting this lab Builders Guide for building the AWS Lambda functions and the web server and where to make changes in the lab guide to use the code you built instead of the publicly available executables.","title":"Overview:"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#license","text":"","title":"License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/README.html#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html","text":"Builders Guide for 300 - Testing for Resiliency of EC2, RDS, and S3 Introduction This guide contains the instructions for how to build the Lambda functions, the web application, and the modifications needed for the AWS CloudFormation templates' parameters as well as the JSON passed to the AWS Step Functions state machine to perform the deployment. This guide will also give some specific instructions on the limitations of how you can deploy and what AWS regions it can be run in. Prerequisites An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. Python installer program (pip) Go language development environment Comfort with JSON License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Building and uploading the AWS Lambda Functions Each function also has a makefile included. This make file will use pip to install dependent packages, then zip the entire directory's contents into a zip file that will be located one directory up. You can deploy these to the region you wish to run the Lambda functions using the AWS Command Line Interface (CLI) as follows: % cd <LambdaDirectory> % make % cd .. % aws s3 cp <lambda>.zip s3://<S3 bucket>/<directory prefix>/<lambda>.zip Debugging the AWS Lambda Functions The Lambda functions are all written in Python. They can be run on the command line with the python debugger, pdb, as follows: % python -m pdb <lambda_function>.py The lambda functions all have an event that is passed in the main function that can be used to test your environment. The parameters are the same as they are to the AWS Step Functions state machine: log_level: This is the python logger logging level. To make it verbose in the logs, use the value \"DEBUG\" region_name: This is the region that the infrastructure is going to be deployed to secondary_region_name: This is the region where the red replica for this region will be deployed. (optional) workshop: A name to be added to the tags of the deployed infrastructure cfn_region: This is the region where the bucket that contains the AWS CloudFormation template is located cfn_bucket: This is the name of the S3 bucket where the AWS CloudFormation template is stored. folder: This is the apparent \"folder\" (actually a key prefix) where the CloudFormation template is located in the cfn_bucket. boot_bucket: This is the bucket in the region_name where the boot scripts and executables are located. boot_prefix: This is the apparent \"folder\" (actually a key prefix) where the boot scripts and executables are located. boot_object: This is the script executed on the instances to bootstrap the application. This is an JSON string that looks like the following: { 'log_level' : 'DEBUG', 'region_name' : 'us-west-2', 'secondary_region_name' : 'us-east-2', 'workshop' : '300 - Testing for Resiliency', 'cfn_region' : 'us-east-2', 'cfn_bucket' : 'aws-well-architected-labs-ohio', 'folder' : 'Reliability/', 'boot_bucket' : 'aws-well-architected-labs-ohio', 'boot_prefix' : 'Reliability/', 'boot_object' : 'bootstrap300Reliability.sh', 'websiteimage' : 'https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg' } There is considerable \"shared knowledge\" between the state machine functions that is all hard-coded, like stack names. The state machine passes state of stacks between functions to indicate if the stack has been deployed or not. These take the form of a nested JSON object: { 'vpc' : { 'stackname' : 'ResiliencyVPC', 'status' : 'CREATE_COMPLETE' } } There will be a status for each stack as they deploy to prevent any attempt to deploy when a previous stack is either not present, or not complete. The applications all have the relevant nested stacks passed in the debug event, so you need to ensure you test them in the same order that the state machine deploys them within. The Troubleshooting guide has additional details on how to debug the function when it is executing in AWS Lambda. Building and Uploading the Web Application The web application is written in the Go programming langauage. You must have the go language installed where you are building the executable. There is also a makefile to build this application. You can also upload the executable using the same method as follows: % cd go % make % aws s3 cp FragileWebApp s3://<S3 bucket>/<diretory prefix>/FragileWebapp The web application is very fragile in that it will always write an entry on every hit it receives. This will cause the application to be tightly coupled to the database (a violation of the AWS Well-Architected Reliability Pillar!). However, it is small and easy to understand and deploy. The Bootstrapping Script The bootstrapping script assumes 4 things: The name of the SQL to run to create the table used is hardcoded to \"createIPTable.sql\" The password is hardcoded to match the hardcoded password in the CloudFormation template that creates the RDS instance. The name of the Executable is \"FragileWebApp\" The bucket location(s) should really be passed as a 5th and/or 6th command line variable and is marked as TODOs. The SQL in the Bootstrapping Script The database and table are hard coded to match what the executable is expecting. There are also commands required to support AWS Database Migration Service (DMS) replication to set the retention configuration of the binlog, and add permisssions for the user that AWS DMS uses. Deploying the State Machine The AWS Step Functions state machine must be deployed in the same region as the bucket where you uploaded the zipped code. This is because the Lambda functions can only be created in the same AWS Region as the location of the bucket. In addition, the Lambda functions must be in the same AWS Region as the state machine in order for the state machine to invoke it. CloudFormation templates The CloudFomation templates and the bootstrapping scripts need to be deployed in the same region. This is not a limitation, except for the fact that the parameters built in the Lambda function make this assumption. Also, the Amazon Machine Images (AMIs) for the web servers are only mapped into us-east-2 (Ohio) and us-west-2 (Oregon).","title":"Builders Guide for 300 - Testing for Resiliency of EC2, RDS, and S3"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#builders-guide-for-300-testing-for-resiliency-of-ec2-rds-and-s3","text":"","title":"Builders Guide for 300 - Testing for Resiliency of EC2, RDS, and S3"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#introduction","text":"This guide contains the instructions for how to build the Lambda functions, the web application, and the modifications needed for the AWS CloudFormation templates' parameters as well as the JSON passed to the AWS Step Functions state machine to perform the deployment. This guide will also give some specific instructions on the limitations of how you can deploy and what AWS regions it can be run in.","title":"Introduction"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#prerequisites","text":"An AWS Account that you are able to use for tesintg, that is not used for production or other purposes. Python installer program (pip) Go language development environment Comfort with JSON","title":"Prerequisites"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#license","text":"","title":"License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#building-and-uploading-the-aws-lambda-functions","text":"Each function also has a makefile included. This make file will use pip to install dependent packages, then zip the entire directory's contents into a zip file that will be located one directory up. You can deploy these to the region you wish to run the Lambda functions using the AWS Command Line Interface (CLI) as follows: % cd <LambdaDirectory> % make % cd .. % aws s3 cp <lambda>.zip s3://<S3 bucket>/<directory prefix>/<lambda>.zip","title":"Building and uploading the AWS Lambda Functions"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#debugging-the-aws-lambda-functions","text":"The Lambda functions are all written in Python. They can be run on the command line with the python debugger, pdb, as follows: % python -m pdb <lambda_function>.py The lambda functions all have an event that is passed in the main function that can be used to test your environment. The parameters are the same as they are to the AWS Step Functions state machine: log_level: This is the python logger logging level. To make it verbose in the logs, use the value \"DEBUG\" region_name: This is the region that the infrastructure is going to be deployed to secondary_region_name: This is the region where the red replica for this region will be deployed. (optional) workshop: A name to be added to the tags of the deployed infrastructure cfn_region: This is the region where the bucket that contains the AWS CloudFormation template is located cfn_bucket: This is the name of the S3 bucket where the AWS CloudFormation template is stored. folder: This is the apparent \"folder\" (actually a key prefix) where the CloudFormation template is located in the cfn_bucket. boot_bucket: This is the bucket in the region_name where the boot scripts and executables are located. boot_prefix: This is the apparent \"folder\" (actually a key prefix) where the boot scripts and executables are located. boot_object: This is the script executed on the instances to bootstrap the application. This is an JSON string that looks like the following: { 'log_level' : 'DEBUG', 'region_name' : 'us-west-2', 'secondary_region_name' : 'us-east-2', 'workshop' : '300 - Testing for Resiliency', 'cfn_region' : 'us-east-2', 'cfn_bucket' : 'aws-well-architected-labs-ohio', 'folder' : 'Reliability/', 'boot_bucket' : 'aws-well-architected-labs-ohio', 'boot_prefix' : 'Reliability/', 'boot_object' : 'bootstrap300Reliability.sh', 'websiteimage' : 'https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg' } There is considerable \"shared knowledge\" between the state machine functions that is all hard-coded, like stack names. The state machine passes state of stacks between functions to indicate if the stack has been deployed or not. These take the form of a nested JSON object: { 'vpc' : { 'stackname' : 'ResiliencyVPC', 'status' : 'CREATE_COMPLETE' } } There will be a status for each stack as they deploy to prevent any attempt to deploy when a previous stack is either not present, or not complete. The applications all have the relevant nested stacks passed in the debug event, so you need to ensure you test them in the same order that the state machine deploys them within. The Troubleshooting guide has additional details on how to debug the function when it is executing in AWS Lambda.","title":"Debugging the AWS Lambda Functions"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#building-and-uploading-the-web-application","text":"The web application is written in the Go programming langauage. You must have the go language installed where you are building the executable. There is also a makefile to build this application. You can also upload the executable using the same method as follows: % cd go % make % aws s3 cp FragileWebApp s3://<S3 bucket>/<diretory prefix>/FragileWebapp The web application is very fragile in that it will always write an entry on every hit it receives. This will cause the application to be tightly coupled to the database (a violation of the AWS Well-Architected Reliability Pillar!). However, it is small and easy to understand and deploy.","title":"Building and Uploading the Web Application"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#the-bootstrapping-script","text":"The bootstrapping script assumes 4 things: The name of the SQL to run to create the table used is hardcoded to \"createIPTable.sql\" The password is hardcoded to match the hardcoded password in the CloudFormation template that creates the RDS instance. The name of the Executable is \"FragileWebApp\" The bucket location(s) should really be passed as a 5th and/or 6th command line variable and is marked as TODOs.","title":"The Bootstrapping Script"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#the-sql-in-the-bootstrapping-script","text":"The database and table are hard coded to match what the executable is expecting. There are also commands required to support AWS Database Migration Service (DMS) replication to set the retention configuration of the binlog, and add permisssions for the user that AWS DMS uses.","title":"The SQL in the Bootstrapping Script"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#deploying-the-state-machine","text":"The AWS Step Functions state machine must be deployed in the same region as the bucket where you uploaded the zipped code. This is because the Lambda functions can only be created in the same AWS Region as the location of the bucket. In addition, the Lambda functions must be in the same AWS Region as the state machine in order for the state machine to invoke it.","title":"Deploying the State Machine"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Builders_Guide.html#cloudformation-templates","text":"The CloudFomation templates and the bootstrapping scripts need to be deployed in the same region. This is not a limitation, except for the fact that the parameters built in the Lambda function make this assumption. Also, the Amazon Machine Images (AMIs) for the web servers are only mapped into us-east-2 (Ohio) and us-west-2 (Oregon).","title":"CloudFormation templates"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html","text":"Level 300: Performing Resiliency Testing for EC2, RDS, and S3 Authors Rodney Lester, Reliability Lead, Well-Architected, AWS Adrian Hornsby, Tech Evangelist, AWS Seth Eliot, Resiliency Lead, Well-Architected, AWS Table of Contents Deploy the Infrastructure Configure Execution Environment Test Resiliency Using Failure Injection Tear down this lab 1. Deploy the Infrastructure You will create a multi-tier architecture using AWS and run a simple service on it. The service is a web server running on Amazon EC2 fronted by an Elastic Load Balancer reverse-proxy, with a data store on Amazon Relational Database Service (RDS). 1.1 Log into the AWS console If you are attending an in-person workshop and were provided with an AWS account by the instructor : Follow the instructions here for accessing your AWS account Note : As part of these instructions you are directed to copy and save AWS credentials for your account. Please do so as you will need them later If you are using your own AWS account : Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab. You will need the AWS credentials, AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY , of this IAM user for later use in this lab. If you do not have this IAM user's credentials or you wish to create a new IAM user with needed permissions, follow the instructions here to create them 1.2 Checking for existing service-linked roles If you are attending an in-person workshop and were provided with an AWS account by the instructor : Skip this step and go directly to step Create the \"deployment machine\" . If you are using your own AWS account : Follow these steps , and then return here and resume with the following instructions. 1.3 Create the \"deployment machine\" Here you will build a state machine using AWS Step Functions and AWS Lambda that orchestrates the deployment of the multi-tier infrastructure. This is not the service infrastructure itself, but meta-infrastructure we use to build the actual infrastructure. Learn more : After the lab see this blog post on how AWS Step Functions and AWS CodePipelines can work together to deploy your infrastructure Decide which deployment option you will use for this lab. It can be run as single region or multi region (two region) deployment. single region is faster to get up and running multi region enables you to test some additional aspects of cross-regional resilience. Decide on one of these options, then in later steps choose the appropriate instructions for the option you have chosen. If you are attending an in-person workshop, your instructor will specify which to use. Get the CloudFormation template: Download the appropriate file (You can right-click then choose download; or you can right click and copy the link to use with wget ) single region : download CloudFormation template here multi region : download CloudFormation template here Ensure you have selected the Ohio region. This region is also known as us-east-2 , which you will see referenced throughout this lab. Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click \u201cCreate Stack:\u201d Leave \"Prepare template\" setting as-is 1 - For \"Template source\" select \"Upload a template file\" 2 - Specify the CloudFormation template you downloaded Click the \u201cNext\u201d button. For \"Stack name\" enter: DeployResiliencyWorkshop On the same screen, for \"Parameters\" enter the appropriate values: If you are attending an in-person workshop and were provided with an AWS account by the instructor : Leave all the parameters at their default values If you are using your own AWS account : Set the first three parameters using these instructions and leave all other parameters at their default values. You optionally may review the default values of this CloudFormation template here Click the \u201cNext\u201d button. On the \"Configure stack options\" page, click \u201cNext\u201d again On the \"Review DeployResiliencyWorkshop\" page, scroll to the bottom and tick the checkbox \u201cI acknowledge that AWS CloudFormation might create IAM resources.\u201d Click the \u201cCreate stack\u201d button. This will take you to the CloudFormation stack status page, showing the stack creation in progress. This will take approximately a minute to deploy. When it shows status CREATE_COMPLETE , then you are finished with this step. 1.4 Deploy infrastructure and run the service Go to the AWS Step Function console at https://console.aws.amazon.com/states On the Step Functions dashboard, you will see \u201cState Machines\u201d and you will have a new one named \u201cDeploymentMachine- random characters .\u201d Click on that state machine. This will bring up an execution console. Click on the \u201cStart execution\u201d button. On the \"New execution\" dialog, for \"Enter an execution name\" delete the auto-generated name and replace it with: BuildResiliency Then for \"Input\" enter JSON that will be used to supply parameter values to the Lambdas in the workflow. single region uses the following values: { \"log_level\": \"DEBUG\", \"region_name\": \"us-east-2\", \"cfn_region\": \"us-east-2\", \"cfn_bucket\": \"aws-well-architected-labs-ohio\", \"folder\": \"Reliability/\", \"workshop\": \"300-ResiliencyofEC2RDSandS3\", \"boot_bucket\": \"aws-well-architected-labs-ohio\", \"boot_prefix\": \"Reliability/\", \"websiteimage\" : \"https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg\" } multi region uses the values here Note : for websiteimage you can supply an alternate link to a public-read-only image in an S3 bucket you control. This will allow you to run S3 resiliency tests as part of the lab Then click the \u201cStart Execution\u201d button. The \"deployment machine\" is now deploying the infrastructure and service you will use for resiliency testing. Time until you can start... Single region Multi region EC2 failure injection test 15-20 min 15-20 min RDS and AZ failure injection tests 20-25 min 40-45 min Multi-region failure injection tests NA 50-55 min Total deployment time 20-25 min 50-55 min You can watch the state machine as it executes by clicking the icon to expand the visual workflow to the full screen. You can also watch the CloudFormation stacks as they are created and transition from CREATE_IN_PROGRESS to CREATE_COMPLETE . Note: If you are in a workshop, the instructor will share background and technical information while your service is deployed. You can start the first test (EC2 failure injection testing) when the web tier has been deployed in the Ohio region. Look for the WaitForWebApp step (for single region ) or WaitForWebApp1 step (for multi region ) to have completed successfully. This will look something like this on the visual workflow. Above screen shot is for single region . for multi region see this diagram instead 1.5 View website for test web service Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation . click on the WebServersforResiliencyTesting stack click on the \"Outputs\" tab For the Key WebSiteURL copy the value. This is the URL of your test web service. Click the value and it will bring up the website: (image will vary depending on what you supplied for websiteimage ) 2. Configure Execution Environment Failure injection is a means of testing resiliency by which a specific failure type is simulated on a service and its response is assessed. You have a choice of environments from which to execute the failure injections for this lab. Bash scripts are a good choice and can be used from a Linux command line. If you prefer Python, Java, Powershell, or C#, then instructions for these are also provided. 2.1 Setup AWS credentials and configuration Your execution environment needs to be configured to enable access to the AWS account you are using for the workshop. This includes Credentials - You identified these credentials back in step 1 AWS access key AWS secret access key AWS session token (used in some cases) Configuration Region: us-east-2 Default output: JSON Note: us-east-2 is the Ohio region If you already know how to configure these, please do so now. If you need help or if you are planning to use PowerShell for this lab, then follow these instructions 2.2 Set up the bash environment Using bash is an effective way to execute the failure injection tests for this workshop. The bash scripts make use of the AWS CLI. If you will be using bash, then follow the directions in this section. If you cannot use bash, then skip to the next section . Prerequisites awscli AWS CLI installed $ aws --version aws-cli/1.16.249 Python/3.6.8... Version 1.1 or higher is fine If you instead got command not found then see instructions here to install awscli jq command-line JSON processor installed. $ jq --version jq-1.5-1-a5b5cbe Version 1.4 or higher is fine If you instead got command not found then see instructions here to install jq Download the resiliency bash scripts from GitHub to a location convenient for you to execute them. You can use the following links to download the scripts: bash/fail_instance.sh bash/failover_rds.sh bash/fail_az.sh Set the scripts to be executable. chmod u+x fail_instance.sh chmod u+x failover_rds.sh chmod u+x fail_az.sh 2.3 Set up the programming language environment (for Python, Java, C#, or PowerShell) If you will be using bash and executed the steps in the previous section, then you can skip this and go to the section: Test Resiliency Using Failure Injection If you will be using Python, Java, C#, or PowerShell for this workshop, click here for instructions on setting up your environment 3. Test Resiliency Using Failure Injection Failure injection (also known as chaos testing ) is an effective and essential method to validate and understand the resiliency of your workload and is a recommended practice of the AWS Well-Architected Reliability Pillar . Here you will initiate various failure scenarios and assess how your system reacts. Preparation Before testing, please prepare the following: Region must be Ohio We will be using the AWS Console to assess the impact of our testing Throughout this lab, make sure you are in the Ohio region Get VPC ID A VPC (Amazon Virtual Private Cloud) is a logically isolated section of the AWS Cloud where you have deployed the resources for your service For these tests you will need to know the VPC ID of the VPC you created as part of deploying the service Navigate to the VPC management console: https://console.aws.amazon.com/vpc In the left pane, click Your VPCs 1 - Tick the checkbox next to ResiliencyVPC 2 - Copy the VPC ID Save the VPC ID - you will use later whenever <vpc-id> is indicated in a command Get familiar with the service website Point a web browser at the URL you saved from earlier. (If you do not recall this, then see these instructions ) Note the availability_zone and instance_id Refresh the website several times watching these values Note the values change. You have deployed one web server per each of three Availability Zones. The AWS Elastic Load Balancer (ELB) sends your request to any of these three healthy instances. Refer to the diagram at the start of this Lab Guide to review your deployed system architecture. Availability Zones ( AZ s) are isolated sets of resources within a region, each with redundant power, networking, and connectivity, housed in separate facilities. Each Availability Zone is isolated, but the Availability Zones in a Region are connected through low-latency links. AWS provides you with the flexibility to place instances and store data across multiple Availability Zones within each AWS Region for high resiliency. Learn more : After the lab see this whitepaper on regions and availability zones 3.1 EC2 failure injection This failure injection will simulate a critical problem with one of the three web servers used by your service. Before starting, view the deployment machine in the AWS Step Functions console to verify the deployment has reached the stage where you can start testing: single region : WaitForWebApp shows completed (green) multi region : WaitForWebApp1 shows completed (green) Navigate to the EC2 console at http://console.aws.amazon.com/ec2 and click Instances in the left pane. There are three EC2 instances with a name beginning with WebServerforResiliency . For these EC2 instances note: Each has a unique Instance ID There is one instance per each Availability Zone All instances are healthy Open up two more console in separate tabs/windows. From the left pane, open Target Groups and Auto Scaling Groups in separate tabs. You now have three console views open To fail one of the EC2 instances, use the VPC ID as the command line argument replacing <vpc-id> in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for) Language Command Bash ./fail_instance.sh <vpc-id> Python python fail_instance.py <vpc-id> Java java -jar app-resiliency-1.0.jar EC2 <vpc-id> C# .\\AppResiliency EC2 <vpc-id> PowerShell .\\fail_instance.ps1 <vpc-id> The specific output will vary based on the command used, but will include a reference to the ID of the EC2 instance and an indicator of success. Here is the output for the Bash command. Note the CurrentState is shutting-down $ ./fail_instance.sh vpc-04f8541d10ed81c80 Terminating i-0710435abc631eab3 { \"TerminatingInstances\": [ { \"CurrentState\": { \"Code\": 32, \"Name\": \"shutting-down\" }, \"InstanceId\": \"i-0710435abc631eab3\", \"PreviousState\": { \"Code\": 16, \"Name\": \"running\" } } ] } Go to the EC2 Instances console which you already have open (or click here to open a new one ) Refresh it. ( Note : it is usually more efficient to use the refresh button in the console, than to refresh the browser) Observe the status of the instance reported by the script. In the screen cap below it is shutting down as reported by the script and will ultimately transition to terminated . 3.2 System response to EC2 instance failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long. 3.2.1 System availability Refresh the service website several times. Note the following: Website remains available The remaining two EC2 instances are handling all the requests (as per the displayed instance_id ) 3.2.2 Load balancing Load balancing ensures service requests are not routed to unhealthy resources, such as the failed EC2 instance. Go to the Target Groups console you already have open (or click here to open a new one ) If there is more than one target group, select the one with the Load Balancer named ResiliencyTestLoadBalancer Click on the Targets tab and observe: Status of the instances in the group. The load balancer will only send traffic to healthy instances. When the auto scaling launches a new instance, it is automatically added to the load balancer target group. In the screen cap below the unhealthy instance is the newly added one. The load balancer will not send traffic to it until it is completed initializing. It will ultimately transition to healthy and then start receiving traffic. Note the new instance was started in the same Availability Zone as the failed one. Amazon EC2 Auto Scaling automatically maintains balance across all of the Availability Zones that you specify. From the same console, now click on the Monitoring tab and view metrics such as Unhealthy hosts and Healthy hosts 3.2.3 Auto scaling Autos scaling ensures we have the capacity necessary to meet customer demand. The auto scaling for this service is a simple configuration that ensures at least three EC2 instances are running. More complex configurations in response to CPU or network load are also possible using AWS. Go to the Auto Scaling Groups console you already have open (or click here to open a new one ) If there is more than one auto scaling group, select the one with the name that starts with WebServersforResiliencyTesting Click on the Activity History tab and observe: The screen cap below shows that all three instances were successfully started at 17:25 At 19:29 the instance targeted by the script was put in draining state and a new instance ending in ...62640 was started, but was still initializing. The new instance will ultimately transition to Successful status Draining allows existing, in-flight requests made to an instance to complete, but it will not send any new requests to the instance. Learn more : After the lab see this blog post for more information on draining . Learn more : After the lab see Auto Scaling Groups to learn more how auto scaling groups are setup and how they distribute instances, and Dynamic Scaling for Amazon EC2 Auto Scaling for more details on setting up auto scaling that responds to demand 3.2.4 EC2 failure injection - conclusion Deploying multiple servers and Elastic Load Balancing enables a service suffer the loss of a server with no availability disruptions as user traffic is automatically routed to the healthy servers. Amazon Auto Scaling ensures unhealthy hosts are removed and replaced with healthy ones to maintain high availability. 3.3 RDS failure injection This failure injection will simulate a critical failure of the Amazon RDS DB instance. Before starting, view the deployment machine in the AWS Step Functions console to verify the deployment has reached the stage where you can start testing: single region : WaitForMultiAZDB shows completed (green) multi region : both WaitForRDSRRStack1 and CheckRDSRRStatus1 show completed (green) Before you initiate the failure simulation, refresh the service website several times. Every time the image is loaded, the website writes a record to the Amazon RDS database Click on click here to go to other page and it will show the latest ten entries in the Amazon RDS DB The DB table shows \"hits\" on our image page Website URL access requests are shown here for traffic against the image page . These include IPs of browser traffic as well as IPs of load balancer health checks For each region the AWS Elastic Load Balancer makes these health checks, so you will see three IP addresses from these Click on click here to go to other page again to return to the image page Go to the RDS Dashboard in the AWS Console at http://console.aws.amazon.com/rds From the RDS dashboard Click on \"DB Instances ( n /40)\" Click on the DB identifier for your database (if you have more than one database, refer to the VPC ID to find the one for this workshop) If running the multi-region deployment, select the DB instance with Role= Master Select the Configuration tab Look at the configured values. Note the following: Value of the Info field is Available RDS DB is configured to be Multi-AZ . The primary DB instance is in AZ us-east-2a and the standby DB instance is in AZ us-east-2b To failover of the RDS instance, use the VPC ID as the command line argument replacing <vpc-id> in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for) Language Command Bash ./failover_rds.sh <vpc-id> Python python fail_rds.py <vpc-id> Java java -jar app-resiliency-1.0.jar RDS <vpc-id> C# .\\AppResiliency RDS <vpc-id> PowerShell .\\failover_rds.ps1 <vpc-id> The specific output will vary based on the command used, but will include some indication that the your Amazon RDS Database is being failedover: Failing over mdk29lg78789zt 3.4 System response to RDS instance failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long. 3.4.1 System availability The website is not available. Some errors you might see reported: No Response / Timeout : Request was successfully sent to EC2 server, but server no longer has connection to an active database 504 Gateway Time-out : Amazon Elastic Load Balancer did not get a response from the server. This can happen when it has removed the servers that are unable to respond and added new ones, but the new ones have not yet finished initialization, and there are no healthy hosts to receive the request 502 Bad Gateway : The Amazon Elastic Load Balancer got a bad request from the server An error you will not see is This site can\u2019t be reached . This is because the Elastic Load Balancer has a node in each of the three Availability Zones and is always available to serve requests. Continue on to the next steps, periodically returning to attempt to refresh the website. 3.4.2 Failover to standby On the database console Configuration tab Refresh and note the values of the Info field. It will ultimately return to Available when the failover is complete. Note the AZs for the primary and standby instances. They have swapped as the standby has no taken over primary responsibility, and the former primary has been restarted. (After RDS failover it can take several minutes for the console to update as shown below. The failover has however completed) From the AWS RDS console, click on the Logs & events tab and scroll down to Recent events . You should see entries like those below. In this case failover took less than a minute. Mon, 14 Oct 2019 19:53:37 GMT - Multi-AZ instance failover started. Mon, 14 Oct 2019 19:53:45 GMT - DB instance restarted Mon, 14 Oct 2019 19:54:21 GMT - Multi-AZ instance failover completed 3.4.2 EC2 server replacement From the AWS RDS console, click on the Monitoring tab and look at DB connections As the failover happens the existing three servers all cannot connect to the DB AWS Auto Scaling detects this (any server not returning an http 200 status is deemed unhealthy), and replaces the three EC2 instances with new ones that establish new connections to the new RDS primary instance The graph shows an unavailability period of about four minutes until at least one DB connection is re-established [optional] Go to the Auto scaling group and AWS Elastic Load Balancer Target group consoles to see how EC2 instance and traffic routing was handled 3.4.3 RDS failure injection - conclusion AWS RDS Database failover took less than a minute Time for AWS Auto Scaling to detect that the instances were unhealthy and to start up new ones took four minutes. This resulted in a four minute non-availability event. Learn more : After the lab see High Availability (Multi-AZ) for Amazon RDS for more details on high availability and failover support for DB instances using Multi-AZ deployments. High Availability (Multi-AZ) for Amazon RDS The primary DB instance switches over automatically to the standby replica if any of the following conditions occur: An Availability Zone outage The primary DB instance fails The DB instance's server type is changed The operating system of the DB instance is undergoing software patching A manual failover of the DB instance was initiated using Reboot with failover 3.6 AZ failure injection This failure injection will simulate a critical problem with one of the three AWS Availability Zones (AZs) used by your service. AWS Availability Zones are powerful tools for helping build highly available applications. If an application is partitioned across AZs, companies are better isolated and protected from issues such as lightning strikes, tornadoes, earthquakes and more. Go to the RDS Dashboard in the AWS Console at http://console.aws.amazon.com/rds and note which Availability Zone the AWS RDS primary DB instance is in. Note : If you previously ran the RDS Failure Injection test , you must wait until the console shows the AZs for the primary and standby instances as swapped, before running this test A good way to run the AZ failure injection is first in an AZ other than this - we'll call this Scenario 1 Then try it again in the same AZ as the AWS RDS primary DB instance - we'll call this Scenario 2 Taking down two out of the three AZs this way is an unlikely use case, however it will show how AWS systems work to maintain service integrity despite extreme circumstances. And executing this way illustrates the impact and response under the two different scenarios. To simulate failure of an AZ, select one of the Availability Zones used by your service ( us-east-2a , us-east-2b , or us-east-2c ) as <az> For scenario 1 select an AZ that is neither primary nor secondary for your RDS DB instance. Given the following RDS console you would choose us-east-2c For scenario 2 select the AZ that is primary for your RDS DB instance. Given the following RDS console you would choose us-east-2b use your VPC ID as <vpc-id> Select one (and only one) of the scripts/programs below. (choose the language that you setup your environment for). Language Command Bash ./fail_az.sh <az> <vpc-id> Python python fail_az.py <vpc-id> <az> Java java -jar app-resiliency-1.0.jar AZ <vpc-id> <az> C# .\\AppResiliency AZ <vpc-id> <az> PowerShell .\\fail_az.ps1 <az> <vpc-id> The specific output will vary based on the command used. Note whether an RDS failover was initiated. This would be the case if you selected the AZ containing the AWS RDS primary DB instance 3.7 System response to AZ failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long. 3.7.1 System availability Refresh the service website several times Scenario 1 : If you selected an AZ not containing the AWS RDS primary DB instance then you should see uninterrupted availability Scenario 2 : If you selected the AZ containing the AWS RDS primary DB instance, then an availability loss similar to what you saw with RDS fault injection testing will occur. 3.7.2 Scenario 1 - Load balancer and web server tiers This scenario is similar to the EC2 failure injection test because there is only one EC2 server per AZ in our architecture. Look at the same screens you as for that test: EC2 Instances Load Balancer Target group Auto Scaling Groups One difference from the EC2 failure test that you will observe is that auto scaling will bring up the replacement EC2 instance in an AZ that already has an EC2 instance as it attempts to balance the requested three EC2 instances across the remaining AZs. 3.7.3 Scenario 2 - Load balancer, web server, and data tiers This scenario is similar to a combination of the RDS failure injection along with EC2 failure injection. In addition to the EC2 related screens look at the Amazon RDS console , navigate to your DB screen and observe the following tabs: Configuration Monitoring Logs & Events 3.7.4 AZ failure injection - conclusion This similarity between scenario 1 and the EC2 failure test, and between scenario 2 and the RDS failure test is illustrative of how an AZ failure impacts your system. The resources in that AZ will have no or limited availability. With the strong partitioning and isolation between Availability Zones however, resources in the other AZs continue to provide your service with needed functionality. Scenario 1 results in loss of the load balancer and web server capabilities in one AZ, while Scenario 2 adds to that the additional loss of the data tier. By ensuring that every tier of your system is in multiple AZs, you create a partitioned architecture resilient to failure. 3.7.4 AZ failure recovery This step is optional. To simulate the AZ returning to health do the following: Go to the Auto Scaling Group console Select the WebServersforResiliencyTesting auto scaling group Actions >> Edit In the Subnet field add any ResiliencyVPC-PrivateSubnet s that are missing (there should be three total) and Save Go to the Network ACL console Look at the NACL entries for the VPC called ResiliencyVPC For any of these NACLs that are not Default do the following Select the NACL Actions >> Edit subnet associations Uncheck all boxes and click Edit Actions >> Delete network ACL Note how the auto scaling redistributes the EC2 serves across the availability zones 3.8 S3 failure injection Failure of S3 means that the image will not be available You may ONLY do this testing if you supplied your own websiteimage reference to an S3 bucket you control 3.8.1 Bucket name You will need to know the bucket name where your image is. For example if the websiteimage value you supplied was \"https://s3.us-east-2.amazonaws.com/my-awesome-bucketname/my_image.jpg\" , then the bucket name is my-awesome-bucketname For this failure simulation it is most straightforward to use the AWS Console as follows. (If you are interested in doing this using the AWS CLI then see here - choose either AWS Console or AWS CLI) AWS Console Navigate to the S3 console: https://console.aws.amazon.com/s3 Select the bucket name where the image is located Select the object, then select the \"Permissions\" tab Select the \"Public Access\" radio button, and deselect the \"Read object\" checkbox and Save To re-enable access (after testing), do the same steps, tick the \"Read object\" checkbox and Save 3.8.3 System response to S3 failure What is the expected effect? How long does it take to take effect? Note that due to browser caching you may still see the image on refreshing the site. On most systems Shift-F5 does a clean refresh with no cache How would you diagnose if this is a larger problem than permissions? 3.9 More testing you can do You can use drift detection in the CloudFormation console to see what had changed, or work on code to heal the failure modes. 4. Tear down this lab If you are attending an in-person workshop and were provided with an AWS account by the instructor : There is no need to tear down the lab. Feel free to continue exploring. Log out of your AWS account when done. If you are using your own AWS account : You may leave these resources deployed for as long as you want. When you are ready to delete these resources, see the following instructions Remove manually provisioned resources Some resources were created by the failure simulation scripts. You need to remove these first Go to the Network ACL console Look at the NACL entries for the VPC called ResiliencyVPC For any of these NACLs that are not Default do the following Select the NACL Actions >> Edit subnet associations Uncheck all boxes and click Edit Actions >> Delete network ACL Remove AWS CloudFormation provisioned resources As part of lab setup you have deployed several AWS CloudFormation stacks. These directions will show you: How to delete an AWS CloudFormation stack In what specific order the stacks must be deleted How to delete an AWS CloudFormation stack Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion Click the stack name Select the Events column Refresh to see new events Delete workshop CloudFormation stacks Since AWS resources deployed by AWS CloudFormation stacks may have dependencies on the stacks that were created before, then deletion must occur in the opposite order they were created Stacks with the same ordinal can be deleted at the same time. All stacks for a given ordinal must be DELETE_COMPLETE before moving on to the next ordinal Single region If you deployed the single region option, then delete your stacks in the following order Order CloudFormation stack 1 WebServersforResiliencyTesting 1 MySQLforResiliencyTesting 2 ResiliencyVPC 2 DeployResiliencyWorkshop Multi region If you deployed the multi region option, then see these instructions for the order in which to delete the CloudFormation stacks Delete remaining resources Delete Lambda execution role used to create custom resource This role was purposely not deleted by the CloudFormation stack, because CloudFormation needs it to delete the custom resource it was used to create. Choose ONE : AWS CLI or AWS Console. Do this step only after ALL CloudFormation stacks are DELETE_COMPLETE Using AWS CLI: aws iam delete-role-policy --role-name LambdaCustomResourceRole-SecureSsmForRds --policy-name LambdaCustomResourcePolicy aws iam delete-role --role-name LambdaCustomResourceRole-SecureSsmForRds Using AWS Console: Go to the IAM Roles Console: https://console.aws.amazon.com/iam/home#/roles Search for SecureSsmForRds Check the box next to LambdaCustomResourceRole-SecureSsmForRds Click Delete role button Click Yes, delete button Delete Systems Manager parameter The password(s) for your Amazon RDS instances were stored in AWS Systems Manager secure parameter store. These steps will verify the parameter(s) were deleted, and if not then guide you to deleting them. Choose ONE : AWS CLI or AWS Console. single region You only need to do the following steps in us-east-2 multi region Do the following steps for both us-east-2 and us-west- 2 Using AWS CLI: In the following command use the workshop name supplied in step 1.4.4. when you ran the step function state machine. If you kept the defaults, the command will work as-is: aws ssm delete-parameter --name 300-ResiliencyofEC2RDSandS3 If you get ParameterNotFound then the password was already deleted by the CloudFormation stack (as expected). Using AWS Console: Select the region Wait until ResiliencyVPC CloudFormation stack is DELETE_COMPLETE in the region Go to the AWS Console for AWS Systems Manager parameter store Look for the parameter created for your infrastructure. If you used our default values, this will be named 300-ResiliencyofEC2RDSandS3 If it is not present (check all regions you deployed to) then you are finished If it is present then Click on the parameter name Click the Delete button Click Delete again References & useful resources EC2 Auto Scaling Groups What Is an Application Load Balancer? High Availability (Multi-AZ) for Amazon RDS Amazon RDS Under the Hood: Multi-AZ Regions and Availability Zones Injecting Chaos to Amazon EC2 using AWS System Manager Build a serverless multi-region, active-active backend solution in an hour License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#level-300-performing-resiliency-testing-for-ec2-rds-and-s3","text":"","title":"Level 300: Performing Resiliency Testing for EC2, RDS, and S3"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#authors","text":"Rodney Lester, Reliability Lead, Well-Architected, AWS Adrian Hornsby, Tech Evangelist, AWS Seth Eliot, Resiliency Lead, Well-Architected, AWS","title":"Authors"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#table-of-contents","text":"Deploy the Infrastructure Configure Execution Environment Test Resiliency Using Failure Injection Tear down this lab","title":"Table of Contents"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#1-deploy-the-infrastructure","text":"You will create a multi-tier architecture using AWS and run a simple service on it. The service is a web server running on Amazon EC2 fronted by an Elastic Load Balancer reverse-proxy, with a data store on Amazon Relational Database Service (RDS).","title":"1. Deploy the Infrastructure "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#11-log-into-the-aws-console","text":"If you are attending an in-person workshop and were provided with an AWS account by the instructor : Follow the instructions here for accessing your AWS account Note : As part of these instructions you are directed to copy and save AWS credentials for your account. Please do so as you will need them later If you are using your own AWS account : Sign in to the AWS Management Console as an IAM user who has PowerUserAccess or AdministratorAccess permissions, to ensure successful execution of this lab. You will need the AWS credentials, AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY , of this IAM user for later use in this lab. If you do not have this IAM user's credentials or you wish to create a new IAM user with needed permissions, follow the instructions here to create them","title":"1.1 Log into the AWS console "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#12-checking-for-existing-service-linked-roles","text":"If you are attending an in-person workshop and were provided with an AWS account by the instructor : Skip this step and go directly to step Create the \"deployment machine\" . If you are using your own AWS account : Follow these steps , and then return here and resume with the following instructions.","title":"1.2 Checking for existing service-linked roles"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#13-create-the-deployment-machine","text":"Here you will build a state machine using AWS Step Functions and AWS Lambda that orchestrates the deployment of the multi-tier infrastructure. This is not the service infrastructure itself, but meta-infrastructure we use to build the actual infrastructure. Learn more : After the lab see this blog post on how AWS Step Functions and AWS CodePipelines can work together to deploy your infrastructure Decide which deployment option you will use for this lab. It can be run as single region or multi region (two region) deployment. single region is faster to get up and running multi region enables you to test some additional aspects of cross-regional resilience. Decide on one of these options, then in later steps choose the appropriate instructions for the option you have chosen. If you are attending an in-person workshop, your instructor will specify which to use. Get the CloudFormation template: Download the appropriate file (You can right-click then choose download; or you can right click and copy the link to use with wget ) single region : download CloudFormation template here multi region : download CloudFormation template here Ensure you have selected the Ohio region. This region is also known as us-east-2 , which you will see referenced throughout this lab. Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation and click \u201cCreate Stack:\u201d Leave \"Prepare template\" setting as-is 1 - For \"Template source\" select \"Upload a template file\" 2 - Specify the CloudFormation template you downloaded Click the \u201cNext\u201d button. For \"Stack name\" enter: DeployResiliencyWorkshop On the same screen, for \"Parameters\" enter the appropriate values: If you are attending an in-person workshop and were provided with an AWS account by the instructor : Leave all the parameters at their default values If you are using your own AWS account : Set the first three parameters using these instructions and leave all other parameters at their default values. You optionally may review the default values of this CloudFormation template here Click the \u201cNext\u201d button. On the \"Configure stack options\" page, click \u201cNext\u201d again On the \"Review DeployResiliencyWorkshop\" page, scroll to the bottom and tick the checkbox \u201cI acknowledge that AWS CloudFormation might create IAM resources.\u201d Click the \u201cCreate stack\u201d button. This will take you to the CloudFormation stack status page, showing the stack creation in progress. This will take approximately a minute to deploy. When it shows status CREATE_COMPLETE , then you are finished with this step.","title":"1.3 Create the \"deployment machine\" "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#14-deploy-infrastructure-and-run-the-service","text":"Go to the AWS Step Function console at https://console.aws.amazon.com/states On the Step Functions dashboard, you will see \u201cState Machines\u201d and you will have a new one named \u201cDeploymentMachine- random characters .\u201d Click on that state machine. This will bring up an execution console. Click on the \u201cStart execution\u201d button. On the \"New execution\" dialog, for \"Enter an execution name\" delete the auto-generated name and replace it with: BuildResiliency Then for \"Input\" enter JSON that will be used to supply parameter values to the Lambdas in the workflow. single region uses the following values: { \"log_level\": \"DEBUG\", \"region_name\": \"us-east-2\", \"cfn_region\": \"us-east-2\", \"cfn_bucket\": \"aws-well-architected-labs-ohio\", \"folder\": \"Reliability/\", \"workshop\": \"300-ResiliencyofEC2RDSandS3\", \"boot_bucket\": \"aws-well-architected-labs-ohio\", \"boot_prefix\": \"Reliability/\", \"websiteimage\" : \"https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg\" } multi region uses the values here Note : for websiteimage you can supply an alternate link to a public-read-only image in an S3 bucket you control. This will allow you to run S3 resiliency tests as part of the lab Then click the \u201cStart Execution\u201d button. The \"deployment machine\" is now deploying the infrastructure and service you will use for resiliency testing. Time until you can start... Single region Multi region EC2 failure injection test 15-20 min 15-20 min RDS and AZ failure injection tests 20-25 min 40-45 min Multi-region failure injection tests NA 50-55 min Total deployment time 20-25 min 50-55 min You can watch the state machine as it executes by clicking the icon to expand the visual workflow to the full screen. You can also watch the CloudFormation stacks as they are created and transition from CREATE_IN_PROGRESS to CREATE_COMPLETE . Note: If you are in a workshop, the instructor will share background and technical information while your service is deployed. You can start the first test (EC2 failure injection testing) when the web tier has been deployed in the Ohio region. Look for the WaitForWebApp step (for single region ) or WaitForWebApp1 step (for multi region ) to have completed successfully. This will look something like this on the visual workflow. Above screen shot is for single region . for multi region see this diagram instead","title":"1.4 Deploy infrastructure and run the service "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#15-view-website-for-test-web-service","text":"Go to the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation . click on the WebServersforResiliencyTesting stack click on the \"Outputs\" tab For the Key WebSiteURL copy the value. This is the URL of your test web service. Click the value and it will bring up the website: (image will vary depending on what you supplied for websiteimage )","title":"1.5 View website for test web service "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#2-configure-execution-environment","text":"Failure injection is a means of testing resiliency by which a specific failure type is simulated on a service and its response is assessed. You have a choice of environments from which to execute the failure injections for this lab. Bash scripts are a good choice and can be used from a Linux command line. If you prefer Python, Java, Powershell, or C#, then instructions for these are also provided.","title":"2. Configure Execution Environment "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#21-setup-aws-credentials-and-configuration","text":"Your execution environment needs to be configured to enable access to the AWS account you are using for the workshop. This includes Credentials - You identified these credentials back in step 1 AWS access key AWS secret access key AWS session token (used in some cases) Configuration Region: us-east-2 Default output: JSON Note: us-east-2 is the Ohio region If you already know how to configure these, please do so now. If you need help or if you are planning to use PowerShell for this lab, then follow these instructions","title":"2.1 Setup AWS credentials and configuration"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#22-set-up-the-bash-environment","text":"Using bash is an effective way to execute the failure injection tests for this workshop. The bash scripts make use of the AWS CLI. If you will be using bash, then follow the directions in this section. If you cannot use bash, then skip to the next section . Prerequisites awscli AWS CLI installed $ aws --version aws-cli/1.16.249 Python/3.6.8... Version 1.1 or higher is fine If you instead got command not found then see instructions here to install awscli jq command-line JSON processor installed. $ jq --version jq-1.5-1-a5b5cbe Version 1.4 or higher is fine If you instead got command not found then see instructions here to install jq Download the resiliency bash scripts from GitHub to a location convenient for you to execute them. You can use the following links to download the scripts: bash/fail_instance.sh bash/failover_rds.sh bash/fail_az.sh Set the scripts to be executable. chmod u+x fail_instance.sh chmod u+x failover_rds.sh chmod u+x fail_az.sh","title":"2.2 Set up the bash environment "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#23-set-up-the-programming-language-environment-for-python-java-c-or-powershell","text":"If you will be using bash and executed the steps in the previous section, then you can skip this and go to the section: Test Resiliency Using Failure Injection If you will be using Python, Java, C#, or PowerShell for this workshop, click here for instructions on setting up your environment","title":"2.3 Set up the programming language environment (for Python, Java, C#, or PowerShell) "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#3-test-resiliency-using-failure-injection","text":"Failure injection (also known as chaos testing ) is an effective and essential method to validate and understand the resiliency of your workload and is a recommended practice of the AWS Well-Architected Reliability Pillar . Here you will initiate various failure scenarios and assess how your system reacts.","title":"3. Test Resiliency Using Failure Injection "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#preparation","text":"Before testing, please prepare the following: Region must be Ohio We will be using the AWS Console to assess the impact of our testing Throughout this lab, make sure you are in the Ohio region Get VPC ID A VPC (Amazon Virtual Private Cloud) is a logically isolated section of the AWS Cloud where you have deployed the resources for your service For these tests you will need to know the VPC ID of the VPC you created as part of deploying the service Navigate to the VPC management console: https://console.aws.amazon.com/vpc In the left pane, click Your VPCs 1 - Tick the checkbox next to ResiliencyVPC 2 - Copy the VPC ID Save the VPC ID - you will use later whenever <vpc-id> is indicated in a command Get familiar with the service website Point a web browser at the URL you saved from earlier. (If you do not recall this, then see these instructions ) Note the availability_zone and instance_id Refresh the website several times watching these values Note the values change. You have deployed one web server per each of three Availability Zones. The AWS Elastic Load Balancer (ELB) sends your request to any of these three healthy instances. Refer to the diagram at the start of this Lab Guide to review your deployed system architecture. Availability Zones ( AZ s) are isolated sets of resources within a region, each with redundant power, networking, and connectivity, housed in separate facilities. Each Availability Zone is isolated, but the Availability Zones in a Region are connected through low-latency links. AWS provides you with the flexibility to place instances and store data across multiple Availability Zones within each AWS Region for high resiliency. Learn more : After the lab see this whitepaper on regions and availability zones","title":"Preparation"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#31-ec2-failure-injection","text":"This failure injection will simulate a critical problem with one of the three web servers used by your service. Before starting, view the deployment machine in the AWS Step Functions console to verify the deployment has reached the stage where you can start testing: single region : WaitForWebApp shows completed (green) multi region : WaitForWebApp1 shows completed (green) Navigate to the EC2 console at http://console.aws.amazon.com/ec2 and click Instances in the left pane. There are three EC2 instances with a name beginning with WebServerforResiliency . For these EC2 instances note: Each has a unique Instance ID There is one instance per each Availability Zone All instances are healthy Open up two more console in separate tabs/windows. From the left pane, open Target Groups and Auto Scaling Groups in separate tabs. You now have three console views open To fail one of the EC2 instances, use the VPC ID as the command line argument replacing <vpc-id> in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for) Language Command Bash ./fail_instance.sh <vpc-id> Python python fail_instance.py <vpc-id> Java java -jar app-resiliency-1.0.jar EC2 <vpc-id> C# .\\AppResiliency EC2 <vpc-id> PowerShell .\\fail_instance.ps1 <vpc-id> The specific output will vary based on the command used, but will include a reference to the ID of the EC2 instance and an indicator of success. Here is the output for the Bash command. Note the CurrentState is shutting-down $ ./fail_instance.sh vpc-04f8541d10ed81c80 Terminating i-0710435abc631eab3 { \"TerminatingInstances\": [ { \"CurrentState\": { \"Code\": 32, \"Name\": \"shutting-down\" }, \"InstanceId\": \"i-0710435abc631eab3\", \"PreviousState\": { \"Code\": 16, \"Name\": \"running\" } } ] } Go to the EC2 Instances console which you already have open (or click here to open a new one ) Refresh it. ( Note : it is usually more efficient to use the refresh button in the console, than to refresh the browser) Observe the status of the instance reported by the script. In the screen cap below it is shutting down as reported by the script and will ultimately transition to terminated .","title":"3.1 EC2 failure injection"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#32-system-response-to-ec2-instance-failure","text":"Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.","title":"3.2 System response to EC2 instance failure"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#321-system-availability","text":"Refresh the service website several times. Note the following: Website remains available The remaining two EC2 instances are handling all the requests (as per the displayed instance_id )","title":"3.2.1 System availability"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#322-load-balancing","text":"Load balancing ensures service requests are not routed to unhealthy resources, such as the failed EC2 instance. Go to the Target Groups console you already have open (or click here to open a new one ) If there is more than one target group, select the one with the Load Balancer named ResiliencyTestLoadBalancer Click on the Targets tab and observe: Status of the instances in the group. The load balancer will only send traffic to healthy instances. When the auto scaling launches a new instance, it is automatically added to the load balancer target group. In the screen cap below the unhealthy instance is the newly added one. The load balancer will not send traffic to it until it is completed initializing. It will ultimately transition to healthy and then start receiving traffic. Note the new instance was started in the same Availability Zone as the failed one. Amazon EC2 Auto Scaling automatically maintains balance across all of the Availability Zones that you specify. From the same console, now click on the Monitoring tab and view metrics such as Unhealthy hosts and Healthy hosts","title":"3.2.2 Load balancing"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#323-auto-scaling","text":"Autos scaling ensures we have the capacity necessary to meet customer demand. The auto scaling for this service is a simple configuration that ensures at least three EC2 instances are running. More complex configurations in response to CPU or network load are also possible using AWS. Go to the Auto Scaling Groups console you already have open (or click here to open a new one ) If there is more than one auto scaling group, select the one with the name that starts with WebServersforResiliencyTesting Click on the Activity History tab and observe: The screen cap below shows that all three instances were successfully started at 17:25 At 19:29 the instance targeted by the script was put in draining state and a new instance ending in ...62640 was started, but was still initializing. The new instance will ultimately transition to Successful status Draining allows existing, in-flight requests made to an instance to complete, but it will not send any new requests to the instance. Learn more : After the lab see this blog post for more information on draining . Learn more : After the lab see Auto Scaling Groups to learn more how auto scaling groups are setup and how they distribute instances, and Dynamic Scaling for Amazon EC2 Auto Scaling for more details on setting up auto scaling that responds to demand","title":"3.2.3 Auto scaling"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#324-ec2-failure-injection-conclusion","text":"Deploying multiple servers and Elastic Load Balancing enables a service suffer the loss of a server with no availability disruptions as user traffic is automatically routed to the healthy servers. Amazon Auto Scaling ensures unhealthy hosts are removed and replaced with healthy ones to maintain high availability.","title":"3.2.4 EC2 failure injection - conclusion"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#33-rds-failure-injection","text":"This failure injection will simulate a critical failure of the Amazon RDS DB instance. Before starting, view the deployment machine in the AWS Step Functions console to verify the deployment has reached the stage where you can start testing: single region : WaitForMultiAZDB shows completed (green) multi region : both WaitForRDSRRStack1 and CheckRDSRRStatus1 show completed (green) Before you initiate the failure simulation, refresh the service website several times. Every time the image is loaded, the website writes a record to the Amazon RDS database Click on click here to go to other page and it will show the latest ten entries in the Amazon RDS DB The DB table shows \"hits\" on our image page Website URL access requests are shown here for traffic against the image page . These include IPs of browser traffic as well as IPs of load balancer health checks For each region the AWS Elastic Load Balancer makes these health checks, so you will see three IP addresses from these Click on click here to go to other page again to return to the image page Go to the RDS Dashboard in the AWS Console at http://console.aws.amazon.com/rds From the RDS dashboard Click on \"DB Instances ( n /40)\" Click on the DB identifier for your database (if you have more than one database, refer to the VPC ID to find the one for this workshop) If running the multi-region deployment, select the DB instance with Role= Master Select the Configuration tab Look at the configured values. Note the following: Value of the Info field is Available RDS DB is configured to be Multi-AZ . The primary DB instance is in AZ us-east-2a and the standby DB instance is in AZ us-east-2b To failover of the RDS instance, use the VPC ID as the command line argument replacing <vpc-id> in one (and only one) of the scripts/programs below. (choose the language that you setup your environment for) Language Command Bash ./failover_rds.sh <vpc-id> Python python fail_rds.py <vpc-id> Java java -jar app-resiliency-1.0.jar RDS <vpc-id> C# .\\AppResiliency RDS <vpc-id> PowerShell .\\failover_rds.ps1 <vpc-id> The specific output will vary based on the command used, but will include some indication that the your Amazon RDS Database is being failedover: Failing over mdk29lg78789zt","title":"3.3 RDS failure injection"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#34-system-response-to-rds-instance-failure","text":"Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.","title":"3.4 System response to RDS instance failure"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#341-system-availability","text":"The website is not available. Some errors you might see reported: No Response / Timeout : Request was successfully sent to EC2 server, but server no longer has connection to an active database 504 Gateway Time-out : Amazon Elastic Load Balancer did not get a response from the server. This can happen when it has removed the servers that are unable to respond and added new ones, but the new ones have not yet finished initialization, and there are no healthy hosts to receive the request 502 Bad Gateway : The Amazon Elastic Load Balancer got a bad request from the server An error you will not see is This site can\u2019t be reached . This is because the Elastic Load Balancer has a node in each of the three Availability Zones and is always available to serve requests. Continue on to the next steps, periodically returning to attempt to refresh the website.","title":"3.4.1 System availability"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#342-failover-to-standby","text":"On the database console Configuration tab Refresh and note the values of the Info field. It will ultimately return to Available when the failover is complete. Note the AZs for the primary and standby instances. They have swapped as the standby has no taken over primary responsibility, and the former primary has been restarted. (After RDS failover it can take several minutes for the console to update as shown below. The failover has however completed) From the AWS RDS console, click on the Logs & events tab and scroll down to Recent events . You should see entries like those below. In this case failover took less than a minute. Mon, 14 Oct 2019 19:53:37 GMT - Multi-AZ instance failover started. Mon, 14 Oct 2019 19:53:45 GMT - DB instance restarted Mon, 14 Oct 2019 19:54:21 GMT - Multi-AZ instance failover completed","title":"3.4.2 Failover to standby"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#342-ec2-server-replacement","text":"From the AWS RDS console, click on the Monitoring tab and look at DB connections As the failover happens the existing three servers all cannot connect to the DB AWS Auto Scaling detects this (any server not returning an http 200 status is deemed unhealthy), and replaces the three EC2 instances with new ones that establish new connections to the new RDS primary instance The graph shows an unavailability period of about four minutes until at least one DB connection is re-established [optional] Go to the Auto scaling group and AWS Elastic Load Balancer Target group consoles to see how EC2 instance and traffic routing was handled","title":"3.4.2 EC2 server replacement"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#343-rds-failure-injection-conclusion","text":"AWS RDS Database failover took less than a minute Time for AWS Auto Scaling to detect that the instances were unhealthy and to start up new ones took four minutes. This resulted in a four minute non-availability event. Learn more : After the lab see High Availability (Multi-AZ) for Amazon RDS for more details on high availability and failover support for DB instances using Multi-AZ deployments. High Availability (Multi-AZ) for Amazon RDS The primary DB instance switches over automatically to the standby replica if any of the following conditions occur: An Availability Zone outage The primary DB instance fails The DB instance's server type is changed The operating system of the DB instance is undergoing software patching A manual failover of the DB instance was initiated using Reboot with failover","title":"3.4.3 RDS failure injection - conclusion"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#36-az-failure-injection","text":"This failure injection will simulate a critical problem with one of the three AWS Availability Zones (AZs) used by your service. AWS Availability Zones are powerful tools for helping build highly available applications. If an application is partitioned across AZs, companies are better isolated and protected from issues such as lightning strikes, tornadoes, earthquakes and more. Go to the RDS Dashboard in the AWS Console at http://console.aws.amazon.com/rds and note which Availability Zone the AWS RDS primary DB instance is in. Note : If you previously ran the RDS Failure Injection test , you must wait until the console shows the AZs for the primary and standby instances as swapped, before running this test A good way to run the AZ failure injection is first in an AZ other than this - we'll call this Scenario 1 Then try it again in the same AZ as the AWS RDS primary DB instance - we'll call this Scenario 2 Taking down two out of the three AZs this way is an unlikely use case, however it will show how AWS systems work to maintain service integrity despite extreme circumstances. And executing this way illustrates the impact and response under the two different scenarios. To simulate failure of an AZ, select one of the Availability Zones used by your service ( us-east-2a , us-east-2b , or us-east-2c ) as <az> For scenario 1 select an AZ that is neither primary nor secondary for your RDS DB instance. Given the following RDS console you would choose us-east-2c For scenario 2 select the AZ that is primary for your RDS DB instance. Given the following RDS console you would choose us-east-2b use your VPC ID as <vpc-id> Select one (and only one) of the scripts/programs below. (choose the language that you setup your environment for). Language Command Bash ./fail_az.sh <az> <vpc-id> Python python fail_az.py <vpc-id> <az> Java java -jar app-resiliency-1.0.jar AZ <vpc-id> <az> C# .\\AppResiliency AZ <vpc-id> <az> PowerShell .\\fail_az.ps1 <az> <vpc-id> The specific output will vary based on the command used. Note whether an RDS failover was initiated. This would be the case if you selected the AZ containing the AWS RDS primary DB instance","title":"3.6 AZ failure injection"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#37-system-response-to-az-failure","text":"Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.","title":"3.7 System response to AZ failure"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#371-system-availability","text":"Refresh the service website several times Scenario 1 : If you selected an AZ not containing the AWS RDS primary DB instance then you should see uninterrupted availability Scenario 2 : If you selected the AZ containing the AWS RDS primary DB instance, then an availability loss similar to what you saw with RDS fault injection testing will occur.","title":"3.7.1 System availability"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#372-scenario-1-load-balancer-and-web-server-tiers","text":"This scenario is similar to the EC2 failure injection test because there is only one EC2 server per AZ in our architecture. Look at the same screens you as for that test: EC2 Instances Load Balancer Target group Auto Scaling Groups One difference from the EC2 failure test that you will observe is that auto scaling will bring up the replacement EC2 instance in an AZ that already has an EC2 instance as it attempts to balance the requested three EC2 instances across the remaining AZs.","title":"3.7.2 Scenario 1 - Load balancer and web server tiers"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#373-scenario-2-load-balancer-web-server-and-data-tiers","text":"This scenario is similar to a combination of the RDS failure injection along with EC2 failure injection. In addition to the EC2 related screens look at the Amazon RDS console , navigate to your DB screen and observe the following tabs: Configuration Monitoring Logs & Events","title":"3.7.3 Scenario 2 - Load balancer, web server, and data tiers"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#374-az-failure-injection-conclusion","text":"This similarity between scenario 1 and the EC2 failure test, and between scenario 2 and the RDS failure test is illustrative of how an AZ failure impacts your system. The resources in that AZ will have no or limited availability. With the strong partitioning and isolation between Availability Zones however, resources in the other AZs continue to provide your service with needed functionality. Scenario 1 results in loss of the load balancer and web server capabilities in one AZ, while Scenario 2 adds to that the additional loss of the data tier. By ensuring that every tier of your system is in multiple AZs, you create a partitioned architecture resilient to failure.","title":"3.7.4 AZ failure injection - conclusion"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#374-az-failure-recovery","text":"This step is optional. To simulate the AZ returning to health do the following: Go to the Auto Scaling Group console Select the WebServersforResiliencyTesting auto scaling group Actions >> Edit In the Subnet field add any ResiliencyVPC-PrivateSubnet s that are missing (there should be three total) and Save Go to the Network ACL console Look at the NACL entries for the VPC called ResiliencyVPC For any of these NACLs that are not Default do the following Select the NACL Actions >> Edit subnet associations Uncheck all boxes and click Edit Actions >> Delete network ACL Note how the auto scaling redistributes the EC2 serves across the availability zones","title":"3.7.4 AZ failure recovery"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#38-s3-failure-injection","text":"Failure of S3 means that the image will not be available You may ONLY do this testing if you supplied your own websiteimage reference to an S3 bucket you control","title":"3.8 S3 failure injection"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#381-bucket-name","text":"You will need to know the bucket name where your image is. For example if the websiteimage value you supplied was \"https://s3.us-east-2.amazonaws.com/my-awesome-bucketname/my_image.jpg\" , then the bucket name is my-awesome-bucketname For this failure simulation it is most straightforward to use the AWS Console as follows. (If you are interested in doing this using the AWS CLI then see here - choose either AWS Console or AWS CLI)","title":"3.8.1 Bucket name"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#aws-console","text":"Navigate to the S3 console: https://console.aws.amazon.com/s3 Select the bucket name where the image is located Select the object, then select the \"Permissions\" tab Select the \"Public Access\" radio button, and deselect the \"Read object\" checkbox and Save To re-enable access (after testing), do the same steps, tick the \"Read object\" checkbox and Save","title":"AWS Console"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#383-system-response-to-s3-failure","text":"What is the expected effect? How long does it take to take effect? Note that due to browser caching you may still see the image on refreshing the site. On most systems Shift-F5 does a clean refresh with no cache How would you diagnose if this is a larger problem than permissions?","title":"3.8.3 System response to S3 failure "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#39-more-testing-you-can-do","text":"You can use drift detection in the CloudFormation console to see what had changed, or work on code to heal the failure modes.","title":"3.9 More testing you can do"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#4-tear-down-this-lab","text":"If you are attending an in-person workshop and were provided with an AWS account by the instructor : There is no need to tear down the lab. Feel free to continue exploring. Log out of your AWS account when done. If you are using your own AWS account : You may leave these resources deployed for as long as you want. When you are ready to delete these resources, see the following instructions","title":"4. Tear down this lab "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#remove-manually-provisioned-resources","text":"Some resources were created by the failure simulation scripts. You need to remove these first Go to the Network ACL console Look at the NACL entries for the VPC called ResiliencyVPC For any of these NACLs that are not Default do the following Select the NACL Actions >> Edit subnet associations Uncheck all boxes and click Edit Actions >> Delete network ACL","title":"Remove manually provisioned resources"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#remove-aws-cloudformation-provisioned-resources","text":"As part of lab setup you have deployed several AWS CloudFormation stacks. These directions will show you: How to delete an AWS CloudFormation stack In what specific order the stacks must be deleted","title":"Remove AWS CloudFormation provisioned resources"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#how-to-delete-an-aws-cloudformation-stack","text":"Go to the AWS CloudFormation console: https://console.aws.amazon.com/cloudformation Select the CloudFormation stack to delete and click Delete In the confirmation dialog, click Delete stack The Status changes to DELETE_IN_PROGRESS Click the refresh button to update and status will ultimately progress to DELETE_COMPLETE When complete, the stack will no longer be displayed. To see deleted stacks use the drop down next to the Filter text box. To see progress during stack deletion Click the stack name Select the Events column Refresh to see new events","title":"How to delete an AWS CloudFormation stack"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#delete-workshop-cloudformation-stacks","text":"Since AWS resources deployed by AWS CloudFormation stacks may have dependencies on the stacks that were created before, then deletion must occur in the opposite order they were created Stacks with the same ordinal can be deleted at the same time. All stacks for a given ordinal must be DELETE_COMPLETE before moving on to the next ordinal","title":"Delete workshop CloudFormation stacks"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#single-region","text":"If you deployed the single region option, then delete your stacks in the following order Order CloudFormation stack 1 WebServersforResiliencyTesting 1 MySQLforResiliencyTesting 2 ResiliencyVPC 2 DeployResiliencyWorkshop","title":"Single region"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#multi-region","text":"If you deployed the multi region option, then see these instructions for the order in which to delete the CloudFormation stacks","title":"Multi region"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#delete-remaining-resources","text":"","title":"Delete remaining resources"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#delete-lambda-execution-role-used-to-create-custom-resource","text":"This role was purposely not deleted by the CloudFormation stack, because CloudFormation needs it to delete the custom resource it was used to create. Choose ONE : AWS CLI or AWS Console. Do this step only after ALL CloudFormation stacks are DELETE_COMPLETE Using AWS CLI: aws iam delete-role-policy --role-name LambdaCustomResourceRole-SecureSsmForRds --policy-name LambdaCustomResourcePolicy aws iam delete-role --role-name LambdaCustomResourceRole-SecureSsmForRds Using AWS Console: Go to the IAM Roles Console: https://console.aws.amazon.com/iam/home#/roles Search for SecureSsmForRds Check the box next to LambdaCustomResourceRole-SecureSsmForRds Click Delete role button Click Yes, delete button","title":"Delete Lambda execution role used to create custom resource"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#delete-systems-manager-parameter","text":"The password(s) for your Amazon RDS instances were stored in AWS Systems Manager secure parameter store. These steps will verify the parameter(s) were deleted, and if not then guide you to deleting them. Choose ONE : AWS CLI or AWS Console. single region You only need to do the following steps in us-east-2 multi region Do the following steps for both us-east-2 and us-west- 2 Using AWS CLI: In the following command use the workshop name supplied in step 1.4.4. when you ran the step function state machine. If you kept the defaults, the command will work as-is: aws ssm delete-parameter --name 300-ResiliencyofEC2RDSandS3 If you get ParameterNotFound then the password was already deleted by the CloudFormation stack (as expected). Using AWS Console: Select the region Wait until ResiliencyVPC CloudFormation stack is DELETE_COMPLETE in the region Go to the AWS Console for AWS Systems Manager parameter store Look for the parameter created for your infrastructure. If you used our default values, this will be named 300-ResiliencyofEC2RDSandS3 If it is not present (check all regions you deployed to) then you are finished If it is present then Click on the parameter name Click the Delete button Click Delete again","title":"Delete Systems Manager parameter"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#references-useful-resources","text":"EC2 Auto Scaling Groups What Is an Application Load Balancer? High Availability (Multi-AZ) for Amazon RDS Amazon RDS Under the Hood: Multi-AZ Regions and Availability Zones Injecting Chaos to Amazon EC2 using AWS System Manager Build a serverless multi-region, active-active backend solution in an hour","title":"References &amp; useful resources"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#license","text":"","title":"License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Lab_Guide.html#code-license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code License"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html","text":"Troubleshooting Guide for 300 - Testing for Resiliency of EC2, RDS, and S3 Introduction The purpose of this guide is to prepare for the expected questions and problems. Common AWS Account Problems If running these labs on your own, you will need to use an AWS account that meets the following qualifications. If you are at a live workshop, you may have been supplied with an AWS account for the lab. If not, and you cannot remedy your account issues, please see a proctor who can help pair you with another student who does have these permissions and you can \u201cpair lab\u201d. You will need to be able to log into the console as a user with permissions to run CloudFormation. If you do not have permission to run CloudFormation, please create a new IAM User with these permissions or use a different AWS account. The next most common problem in deploying the test application is exceeding the default limit of 5 Elastic IPs in an account. The VPC is created with 3 NAT Gateways, which each require an EIP. You will either will have to release some that you are using, or use a different AWS account. The service linked roles may exist already in an account. If they do, you will see a failure to deploy the first CFN stack for lambda_functions_for_deploy.json . You should delete the stack and redeploy it, but please make sure you are appropriately setting the Boolean parameters of the deployment machine stack. If at a live workshop, please see a proctor if you need more help with this. Problems with Service Linked Roles If you don\u2019t see the existing service linked IAM Role and try to create it in the deployment machine, it will not deploy. It will fail back with an error that the Role already exists under another name. Simply set the parameter to false and redeploy. Problems with the Step Functions State Machine and/or Lambda Functions The state machine is idempotent and can be re-run if something times out. If a function fails, you can debug it by creating a test for the Lambda Function. For example, to the test the DeployVPC Lambda function, navigate to the StepFunctions console, and select the DeployVPC function in the Visual Workflow, and click on the Input in the Step details on the right: Once you\u2019ve clicked the Input, you can select the input and copy it into the copy buffer: Then navigate to the Lambda console and click on the DeployVPC Lambda Function: You can then click the down arrow to the left of the \u201cTest\u201d button with the grayed text \u201cSelect a test event..\u201d and click on \u201cConfigure test events:\u201d Name the event TestDeployVPC and insert the copied input from the step function, then click \u201cCreate:\u201d Now you can click the \u201cTest\u201d button to execute the test: After execution, you can click on the \u201cDetails\u201d and see the log of the function to determine what went wrong: You can also go to the CloudWatch logs to see details of the execution. DeployRDS step fails If you get the following error for DeployRDS then there is an obsolete DB password stored in SSM Parameter store. This can happen if you had a problem with deployment, stopped it, and then restarted it. The ciphertext refers to a customer master key that does not exist, does not exist in this region, or you are not allowed to access. Solution: In the AWS console go to SSM Parameter store Delete the parameters stored there Go to the CloudFormation console and delete (roll back) the ResiliencyVPC stack Resume by re-starting deployment of the infrastructure Note you will need to use a new name, such as BuildResiliency2 RDSStackCompleteChoice -> DeployFailedStatus If your deployment machine fails and looks like this And if the following is true: click on the RDSStackCompleteChoice stage of your workflow select Output RDS stack shows status as CREATE_IN_PROGRESS \"rds\": { \"stackname\": \"MySQLforResiliencyTesting\", \"status\": \"CREATE_IN_PROGRESS\" } Then it is likley that your RDS deployment timed out before the workflow could complete. Do the following to continue: Go to the CloudFormation console Verify the status for MySQLforResiliencyTesting is CREATE_COMPLETE Go back to your state machine Click New Execution Give your execution a new name, unique from previous ones (such as \"BuildResiliency3\") The workflow will quickly determine which stacks have already been deployed, and start immediately on the final (web server) stack. Problems Executing the Scripts If you are not using Amazon Linux, you will need to install the AWS CLI. Installing jq is pretty easy, just download the executable and install it where the PATH will see it. Older versions of bash and the windows bash implementation complain about the { and } characters in the sed commands. They can be deleted in older version of bash (but note that they are required in newer versions of bash). Assisting with the Failure Tests Failure modes individual The shell script will delete the first instance it finds running the VPC. There shouldn\u2019t be a problem with the jq parser of the JSON returned from the ec2 describe-instances but it is theoretically possible. If you get a jq error, it is more likely the web layer has not actually been created correctly. If you see null s in output messages, it is possible that you are not specifying the correct VPC ID. Installing boto3 via pip on an Amazon Linux instance will cause you to have errors on the command line: $ aws help Traceback (most recent call last): File \"/usr/local/bin/aws\", line 19, in \\<module\\> import awscli.clidriver File \"/usr/local/lib/python2.7/dist-packages/awscli/clidriver.py\", line 19, in \\<module\\> from botocore.hooks import AliasedEventEmitter ImportError: cannot import name AliasedEventEmitter To fix this, you need to remove the aws-cli , downgrade boto , and install an older version of the aws-cli : $ sudo yum remove aws-cli $ sudo yum downgrade python27-botocore 1.8 $ sudo yum install aws-cli-1.14.9 EC2 Instance Failure Some additional questions to ask yourself: Open fail_instance.sh/fail_instance.py/InstanceFailover.java/InstanceFailover.cs in an editor. How could you make this randomly select an instance? What are the concerns if you have hundreds or thousands of instances? What if you don\u2019t have an Auto Scaling group? How could you recover? How do they test EC2 AutoRecovery? The answer is to open a support ticket. This requires extra effort on our side. How would you \u201cundo\u201d this failure mode? RDS Failover The console does not update the active AZ of the RDS instance until an unknown amount of time passes (~5 min). This can make it appear it didn\u2019t fail over, but it did. You can see the failover in the events log of RDS. If you see nulls in output messages, it is possible that you are not specifying the correct VPC ID. Some additional questions to ask yourself: Why didn\u2019t the Auto Scaling Group terminate them and replace the instances? Or why did it? How could you make the application resilient to the transient failure? What if this was a single AZ RDS? How would you fail the S3 portion of the application? AZ Failure The Java and C# implementations have some improved error checking over the bash implementation, but essentially perform the same logic. This is what the failure simulation does: Loop through all the Auto Scaling Groups by calling Auto Scaling\u2019s DescribeAutoScalingGroups ; for each group, look at the AZs it is configured for. If the desired AZ in in the list, we can reconfigure the group by calling UpdateAutoScalingGroup to update the AZs to the list without this AZ in it. Call EC2\u2019s DescribeSubnets to identify the subnets in the AZ desired within the VPC. It then creates a NACL, adds entries to block ingress and egress of all ports and protocols, then calls EC2\u2019s ReplaceNetworkAclAssociation to associate the subnets with the NACL. This will cause the ELB to route traffic to the other AZs since it has Cross AZ enabled. Loop through all the RDS Instances by calling RDS DescribeInstances . If this instance\u2019s AvailabilityZone is this AZ, then if it is an RDS Multi-AZ, call RDS RebootDBInstance with ForceFailover set to True . Some additional questions to ask yourself: What is the expected effect? How long does it take to take effect? Look at the Target Group Targets to see them go unhealthy, also watch the EC2 instances to see the one in the target AZ shutdown and be restarted in one of the other AZs. What would you do if the ASG was only in one AZ? You could call the AutoScaling SuspendProcesses and then get the list of instances in the group and call EC2 StopInstances or TerminateInstances How would you undo all these changes? Region Failure Unfortunately, you need a DNS domain registered to effect a Region failover, so you won\u2019t be able to perform this failure. However, you can think about how you would simulate it. Route53 uses HealthChecks to see if the destination is available. You could use the network ACL modification above that relates to the AZ to specify that change on the health check endpoint. (A SecurityGroup modification would likely work as well). This would cause the HeathCheck to fail and the record set to use the second region. You would want to also have an alert fire on this and send a call to DMS StopReplicationTask to stop the replication if you are using DMS to replicate data to an instance in a second AWS Region. Some additional questions to ask yourself: How would you fail back? You could set up a DMS instance that is configured for the source and target going the other way, then recover by call StartReplicationTask , using a load and cdc configuration.","title":"Trouble Shooting Guide"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#troubleshooting-guide-for-300-testing-for-resiliency-of-ec2-rds-and-s3","text":"","title":"Troubleshooting Guide for 300 - Testing for Resiliency of EC2, RDS, and S3"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#introduction","text":"The purpose of this guide is to prepare for the expected questions and problems.","title":"Introduction"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#common-aws-account-problems","text":"If running these labs on your own, you will need to use an AWS account that meets the following qualifications. If you are at a live workshop, you may have been supplied with an AWS account for the lab. If not, and you cannot remedy your account issues, please see a proctor who can help pair you with another student who does have these permissions and you can \u201cpair lab\u201d. You will need to be able to log into the console as a user with permissions to run CloudFormation. If you do not have permission to run CloudFormation, please create a new IAM User with these permissions or use a different AWS account. The next most common problem in deploying the test application is exceeding the default limit of 5 Elastic IPs in an account. The VPC is created with 3 NAT Gateways, which each require an EIP. You will either will have to release some that you are using, or use a different AWS account. The service linked roles may exist already in an account. If they do, you will see a failure to deploy the first CFN stack for lambda_functions_for_deploy.json . You should delete the stack and redeploy it, but please make sure you are appropriately setting the Boolean parameters of the deployment machine stack. If at a live workshop, please see a proctor if you need more help with this.","title":"Common AWS Account Problems"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#problems-with-service-linked-roles","text":"If you don\u2019t see the existing service linked IAM Role and try to create it in the deployment machine, it will not deploy. It will fail back with an error that the Role already exists under another name. Simply set the parameter to false and redeploy.","title":"Problems with Service Linked Roles"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#problems-with-the-step-functions-state-machine-andor-lambda-functions","text":"The state machine is idempotent and can be re-run if something times out. If a function fails, you can debug it by creating a test for the Lambda Function. For example, to the test the DeployVPC Lambda function, navigate to the StepFunctions console, and select the DeployVPC function in the Visual Workflow, and click on the Input in the Step details on the right: Once you\u2019ve clicked the Input, you can select the input and copy it into the copy buffer: Then navigate to the Lambda console and click on the DeployVPC Lambda Function: You can then click the down arrow to the left of the \u201cTest\u201d button with the grayed text \u201cSelect a test event..\u201d and click on \u201cConfigure test events:\u201d Name the event TestDeployVPC and insert the copied input from the step function, then click \u201cCreate:\u201d Now you can click the \u201cTest\u201d button to execute the test: After execution, you can click on the \u201cDetails\u201d and see the log of the function to determine what went wrong: You can also go to the CloudWatch logs to see details of the execution.","title":"Problems with the Step Functions State Machine and/or Lambda Functions"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#deployrds-step-fails","text":"If you get the following error for DeployRDS then there is an obsolete DB password stored in SSM Parameter store. This can happen if you had a problem with deployment, stopped it, and then restarted it. The ciphertext refers to a customer master key that does not exist, does not exist in this region, or you are not allowed to access. Solution: In the AWS console go to SSM Parameter store Delete the parameters stored there Go to the CloudFormation console and delete (roll back) the ResiliencyVPC stack Resume by re-starting deployment of the infrastructure Note you will need to use a new name, such as BuildResiliency2","title":"DeployRDS step fails"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#rdsstackcompletechoice-deployfailedstatus","text":"If your deployment machine fails and looks like this And if the following is true: click on the RDSStackCompleteChoice stage of your workflow select Output RDS stack shows status as CREATE_IN_PROGRESS \"rds\": { \"stackname\": \"MySQLforResiliencyTesting\", \"status\": \"CREATE_IN_PROGRESS\" } Then it is likley that your RDS deployment timed out before the workflow could complete. Do the following to continue: Go to the CloudFormation console Verify the status for MySQLforResiliencyTesting is CREATE_COMPLETE Go back to your state machine Click New Execution Give your execution a new name, unique from previous ones (such as \"BuildResiliency3\") The workflow will quickly determine which stacks have already been deployed, and start immediately on the final (web server) stack.","title":"RDSStackCompleteChoice -&gt; DeployFailedStatus"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#problems-executing-the-scripts","text":"If you are not using Amazon Linux, you will need to install the AWS CLI. Installing jq is pretty easy, just download the executable and install it where the PATH will see it. Older versions of bash and the windows bash implementation complain about the { and } characters in the sed commands. They can be deleted in older version of bash (but note that they are required in newer versions of bash).","title":"Problems Executing the Scripts"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#assisting-with-the-failure-tests","text":"","title":"Assisting with the Failure Tests"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#failure-modes-individual","text":"The shell script will delete the first instance it finds running the VPC. There shouldn\u2019t be a problem with the jq parser of the JSON returned from the ec2 describe-instances but it is theoretically possible. If you get a jq error, it is more likely the web layer has not actually been created correctly. If you see null s in output messages, it is possible that you are not specifying the correct VPC ID. Installing boto3 via pip on an Amazon Linux instance will cause you to have errors on the command line: $ aws help Traceback (most recent call last): File \"/usr/local/bin/aws\", line 19, in \\<module\\> import awscli.clidriver File \"/usr/local/lib/python2.7/dist-packages/awscli/clidriver.py\", line 19, in \\<module\\> from botocore.hooks import AliasedEventEmitter ImportError: cannot import name AliasedEventEmitter To fix this, you need to remove the aws-cli , downgrade boto , and install an older version of the aws-cli : $ sudo yum remove aws-cli $ sudo yum downgrade python27-botocore 1.8 $ sudo yum install aws-cli-1.14.9","title":"Failure modes individual"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#ec2-instance-failure","text":"Some additional questions to ask yourself: Open fail_instance.sh/fail_instance.py/InstanceFailover.java/InstanceFailover.cs in an editor. How could you make this randomly select an instance? What are the concerns if you have hundreds or thousands of instances? What if you don\u2019t have an Auto Scaling group? How could you recover? How do they test EC2 AutoRecovery? The answer is to open a support ticket. This requires extra effort on our side. How would you \u201cundo\u201d this failure mode?","title":"EC2 Instance Failure"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#rds-failover","text":"The console does not update the active AZ of the RDS instance until an unknown amount of time passes (~5 min). This can make it appear it didn\u2019t fail over, but it did. You can see the failover in the events log of RDS. If you see nulls in output messages, it is possible that you are not specifying the correct VPC ID. Some additional questions to ask yourself: Why didn\u2019t the Auto Scaling Group terminate them and replace the instances? Or why did it? How could you make the application resilient to the transient failure? What if this was a single AZ RDS? How would you fail the S3 portion of the application?","title":"RDS Failover"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#az-failure","text":"The Java and C# implementations have some improved error checking over the bash implementation, but essentially perform the same logic. This is what the failure simulation does: Loop through all the Auto Scaling Groups by calling Auto Scaling\u2019s DescribeAutoScalingGroups ; for each group, look at the AZs it is configured for. If the desired AZ in in the list, we can reconfigure the group by calling UpdateAutoScalingGroup to update the AZs to the list without this AZ in it. Call EC2\u2019s DescribeSubnets to identify the subnets in the AZ desired within the VPC. It then creates a NACL, adds entries to block ingress and egress of all ports and protocols, then calls EC2\u2019s ReplaceNetworkAclAssociation to associate the subnets with the NACL. This will cause the ELB to route traffic to the other AZs since it has Cross AZ enabled. Loop through all the RDS Instances by calling RDS DescribeInstances . If this instance\u2019s AvailabilityZone is this AZ, then if it is an RDS Multi-AZ, call RDS RebootDBInstance with ForceFailover set to True . Some additional questions to ask yourself: What is the expected effect? How long does it take to take effect? Look at the Target Group Targets to see them go unhealthy, also watch the EC2 instances to see the one in the target AZ shutdown and be restarted in one of the other AZs. What would you do if the ASG was only in one AZ? You could call the AutoScaling SuspendProcesses and then get the list of instances in the group and call EC2 StopInstances or TerminateInstances How would you undo all these changes?","title":"AZ Failure"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/TroubleShooting_Guide.html#region-failure","text":"Unfortunately, you need a DNS domain registered to effect a Region failover, so you won\u2019t be able to perform this failure. However, you can think about how you would simulate it. Route53 uses HealthChecks to see if the destination is available. You could use the network ACL modification above that relates to the AZ to specify that change on the health check endpoint. (A SecurityGroup modification would likely work as well). This would cause the HeathCheck to fail and the record set to use the second region. You would want to also have an alert fire on this and send a call to DMS StopReplicationTask to stop the replication if you are using DMS to replicate data to an instance in a second AWS Region. Some additional questions to ask yourself: How would you fail back? You could set up a DMS instance that is configured for the source and target going the other way, then recover by call StartReplicationTask , using a load and cdc configuration.","title":"Region Failure"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/AWS_Credentials.html","text":"Setup AWS credentials and configuration You will supply configuration and credentials used by the AWS SDK to access your AWS account. You identified these credentials back in step 1 of the Lab Guide Choose an option Select the appropriate option for configuration of your AWS credentials: Option 1 - Using AWS instructor supplied accounts with Linux-style environment variables Option 2 - Using AWS CLI Option 3 - Creating configuration files manually Option 4 - Using PowerShell commands for Windows Option 1 For instructor supplied AWS accounts If BOTH of the following are true then you may use Option 1 If you are attending an in-person workshop and were provided with an AWS account by the instructor then you should use this option You are running the workshop on a system where environment variables are set using the export command, such as Bash on Amazon Linux Otherwise you should choose Option 2 or Option 3 You should have already copied the credentials for your account. If not then follow the directions here The copied credentials are already in the form of export statements. Run these from your shell command line. Use your values, not the ones below export AWS_ACCESS_KEY_ID=ASIIAMFAKENOPZLX6J5L export AWS_SECRET_ACCESS_KEY=w0pE4j5k4FlUrkIIAMFAKEdiLMKLGZlxyct+GpTam export AWS_SESSION_TOKEN=FQoGZXIvYXdzEDwaIIAMFAKEn0LVImWNQHiLuAWKe+KFkLeIvpOHEruWjyCjrEdyjtW8WCbnmJGM1ES20xq1fcaS5TERHDUabZJ60Kk6nc9uHoCDb1QKHi+MerRIcKJTi3OKz0QMVPAGVqVWgvOBBSQ2lylLVjtMMSQF+yLZsP1bvehQ0ke/Bl/X6RJySOHg2TZGyESPL/INqJiZyEHi+MelAnThepVgWUKFPD5mESBVlpy2LVCE3xPpHFqOm0Q79svRSSW2jLj5NkRXL+xhkcvt+g8vNt1ODEwixwMGpFB2sBHryv6EXNeX6c88vxJ8Zyfkmsqi0xmCW1f9jWAPIXNkt/nEYW4J4coyLKP7QU= export AWS_DEFAULT_REGION=us-east-2 Also run this command as written below export AWS_DEFAULT_OUTPUT=json Note that if you end your bash session, or start a new one, you will need to re-execute the export statements If you completed Option 1 then STOP HERE and return to the Lab Guide Option 2 AWS CLI This option uses the AWS CLI. Note that running the bash failure testing scripts requires this software. If you are using another programming environment for failure testing, you can use Option 3 if you do not or cannot install the AWS CLI. To see if the AWS CLI is installed: $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.1 or higher is fine If you instead got command not found then either install the AWS CLI or use Option 3 Run aws configure and provide the following values: $ aws configure AWS Access Key ID [*************xxxx]: <Your AWS Access Key ID> AWS Secret Access Key [**************xxxx]: <Your AWS Secret Access Key> Default region name: [us-east-2]: us-east-2 Default output format [None]: json Option 3 Manually creating credential files If you already did Option 2 , then skip this create a .aws directory under your home directory mkdir ~/.aws Change directory to there cd ~/.aws Use a text editor (vim, emacs, notepad) to create a text file (no extension) named credentials . In this file you should have the following text. [default] aws_access_key_id = <Your access key> aws_secret_access_key = <Your secret key> Create a text file (no extension) named config . In this file you should have the following text: [default] region = us-east-2 output = json Configure a session token as part of your credentials If you used Option 2 or Option 3 , please follow these steps: Determine if you need to configure a session token as part of your credentials AWS Account Do you need a session token? You are attending an in-person workshop and were provided with an AWS account by the instructor yes You are using your own AWS account, and using credentials from an IAM User (most common case) no You are using your own AWS account, and using credentials from an IAM Role yes Do this only if \"yes\", you need to configure a session token Edit the file ~/.aws/credentials The default profile will already be present. Under it add an entry for aws_session_token [default] aws_access_key_id = <Your access key> aws_secret_access_key = <Your secret key> aws_session_token = <your session token> Clear environment variables If you used option 2 or option 3 then you have put your credentials into files that will be used by the AWS CLI or AWS SDK. However these will preferentially use credentials and configuration in environment variables. Therefore ensure that the following env variables are not set: AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN AWS_DEFAULT_REGION AWS_DEFAULT_OUTPUT AWS_PROFILE How to do this varies depending on system. For Linux: # Use echo $varname to see if it is set $ echo $AWS_ACCESS_KEY_ID ASIATWOQ3L72RPLOP222 # use unset $ unset AWS_ACCESS_KEY_ID # This now returns no value $ echo $AWS_ACCESS_KEY_ID For your convenience: unset AWS_ACCESS_KEY_ID unset AWS_SECRET_ACCESS_KEY unset AWS_SESSION_TOKEN unset AWS_DEFAULT_REGION unset AWS_DEFAULT_OUTPUT unset AWS_PROFILE Option 4 (PowerShell) If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ . Start a Windows PowerShell for AWS session. If prompted for AWS Secret Key during initialization, type Control-C to break out of the dialog. Configure your AWS credentials with the following PowerShell commands. Note that if you are using an instructor supplied AWS account, you must include the optional SessionToken flag and value as shown below in brackets (omit the brackets when running the command): Set-AWSCredentials -AccessKey <Your access key> -SecretKey <Your secret key> \\ [ -SessionToken <your session key> ] -StoreAs <SomeProfileName> Initialize-AWSDefaults -ProfileName <SomeProfileName> -Region us-east-2 Click here to return to the Lab Guide","title":"Setup AWS credentials and configuration"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/AWS_Credentials.html#setup-aws-credentials-and-configuration","text":"You will supply configuration and credentials used by the AWS SDK to access your AWS account. You identified these credentials back in step 1 of the Lab Guide","title":"Setup AWS credentials and configuration"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/AWS_Credentials.html#choose-an-option","text":"Select the appropriate option for configuration of your AWS credentials: Option 1 - Using AWS instructor supplied accounts with Linux-style environment variables Option 2 - Using AWS CLI Option 3 - Creating configuration files manually Option 4 - Using PowerShell commands for Windows","title":"Choose an option"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/AWS_Credentials.html#option-1-for-instructor-supplied-aws-accounts","text":"If BOTH of the following are true then you may use Option 1 If you are attending an in-person workshop and were provided with an AWS account by the instructor then you should use this option You are running the workshop on a system where environment variables are set using the export command, such as Bash on Amazon Linux Otherwise you should choose Option 2 or Option 3 You should have already copied the credentials for your account. If not then follow the directions here The copied credentials are already in the form of export statements. Run these from your shell command line. Use your values, not the ones below export AWS_ACCESS_KEY_ID=ASIIAMFAKENOPZLX6J5L export AWS_SECRET_ACCESS_KEY=w0pE4j5k4FlUrkIIAMFAKEdiLMKLGZlxyct+GpTam export AWS_SESSION_TOKEN=FQoGZXIvYXdzEDwaIIAMFAKEn0LVImWNQHiLuAWKe+KFkLeIvpOHEruWjyCjrEdyjtW8WCbnmJGM1ES20xq1fcaS5TERHDUabZJ60Kk6nc9uHoCDb1QKHi+MerRIcKJTi3OKz0QMVPAGVqVWgvOBBSQ2lylLVjtMMSQF+yLZsP1bvehQ0ke/Bl/X6RJySOHg2TZGyESPL/INqJiZyEHi+MelAnThepVgWUKFPD5mESBVlpy2LVCE3xPpHFqOm0Q79svRSSW2jLj5NkRXL+xhkcvt+g8vNt1ODEwixwMGpFB2sBHryv6EXNeX6c88vxJ8Zyfkmsqi0xmCW1f9jWAPIXNkt/nEYW4J4coyLKP7QU= export AWS_DEFAULT_REGION=us-east-2 Also run this command as written below export AWS_DEFAULT_OUTPUT=json Note that if you end your bash session, or start a new one, you will need to re-execute the export statements If you completed Option 1 then STOP HERE and return to the Lab Guide","title":"Option 1 For instructor supplied AWS accounts"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/AWS_Credentials.html#option-2-aws-cli","text":"This option uses the AWS CLI. Note that running the bash failure testing scripts requires this software. If you are using another programming environment for failure testing, you can use Option 3 if you do not or cannot install the AWS CLI. To see if the AWS CLI is installed: $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.1 or higher is fine If you instead got command not found then either install the AWS CLI or use Option 3 Run aws configure and provide the following values: $ aws configure AWS Access Key ID [*************xxxx]: <Your AWS Access Key ID> AWS Secret Access Key [**************xxxx]: <Your AWS Secret Access Key> Default region name: [us-east-2]: us-east-2 Default output format [None]: json","title":"Option 2 AWS CLI"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/AWS_Credentials.html#option-3-manually-creating-credential-files","text":"If you already did Option 2 , then skip this create a .aws directory under your home directory mkdir ~/.aws Change directory to there cd ~/.aws Use a text editor (vim, emacs, notepad) to create a text file (no extension) named credentials . In this file you should have the following text. [default] aws_access_key_id = <Your access key> aws_secret_access_key = <Your secret key> Create a text file (no extension) named config . In this file you should have the following text: [default] region = us-east-2 output = json","title":"Option 3 Manually creating credential files"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/AWS_Credentials.html#configure-a-session-token-as-part-of-your-credentials","text":"If you used Option 2 or Option 3 , please follow these steps: Determine if you need to configure a session token as part of your credentials AWS Account Do you need a session token? You are attending an in-person workshop and were provided with an AWS account by the instructor yes You are using your own AWS account, and using credentials from an IAM User (most common case) no You are using your own AWS account, and using credentials from an IAM Role yes Do this only if \"yes\", you need to configure a session token Edit the file ~/.aws/credentials The default profile will already be present. Under it add an entry for aws_session_token [default] aws_access_key_id = <Your access key> aws_secret_access_key = <Your secret key> aws_session_token = <your session token>","title":"Configure a session token as part of your credentials"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/AWS_Credentials.html#clear-environment-variables","text":"If you used option 2 or option 3 then you have put your credentials into files that will be used by the AWS CLI or AWS SDK. However these will preferentially use credentials and configuration in environment variables. Therefore ensure that the following env variables are not set: AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN AWS_DEFAULT_REGION AWS_DEFAULT_OUTPUT AWS_PROFILE How to do this varies depending on system. For Linux: # Use echo $varname to see if it is set $ echo $AWS_ACCESS_KEY_ID ASIATWOQ3L72RPLOP222 # use unset $ unset AWS_ACCESS_KEY_ID # This now returns no value $ echo $AWS_ACCESS_KEY_ID For your convenience: unset AWS_ACCESS_KEY_ID unset AWS_SECRET_ACCESS_KEY unset AWS_SESSION_TOKEN unset AWS_DEFAULT_REGION unset AWS_DEFAULT_OUTPUT unset AWS_PROFILE","title":"Clear environment variables"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/AWS_Credentials.html#option-4-powershell","text":"If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ . Start a Windows PowerShell for AWS session. If prompted for AWS Secret Key during initialization, type Control-C to break out of the dialog. Configure your AWS credentials with the following PowerShell commands. Note that if you are using an instructor supplied AWS account, you must include the optional SessionToken flag and value as shown below in brackets (omit the brackets when running the command): Set-AWSCredentials -AccessKey <Your access key> -SecretKey <Your secret key> \\ [ -SessionToken <your session key> ] -StoreAs <SomeProfileName> Initialize-AWSDefaults -ProfileName <SomeProfileName> -Region us-east-2 Click here to return to the Lab Guide","title":"Option 4 (PowerShell)"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/CFN_Parameters.html","text":"CloudFormation Parameters All entries are Case-Sensitive single region stack Parameter Default Value CreateTheAutoScalingServiceRole true CreateTheELBServiceRole true CreateTheRDSServiceRole true LambdaFunctionsBucket aws-well-architected-labs-ohio RDSLambdaKey Reliability/RDSLambda.zip VPCLambdaKey Reliability/Reliability/VPCLambda.zip WaitForStackLambdaKey Reliability/WaitForStack.zip WebAppLambdaKey Reliability/WebAppLambda.zip multi region stack Parameter Default Value CreateTheAutoScalingServiceRole true CreateTheELBServiceRole true CreateTheRDSServiceRole true DMSLambdaKey Reliability/DMSLambda.zip LambdaFunctionsBucket aws-well-architected-labs-ohio RDSLambdaKey Reliability/RDSLambda.zip RDSRRLambdaKey Reliability/RDSReadReplicaLambda.zip VPCLambdaKey Reliability/Reliability/VPCLambda.zip WaitForStackLambdaKey Reliability/WaitForStack.zip WebAppLambdaKey Reliability/WebAppLambda.zip Click here to return to the Lab Guide","title":"CloudFormation Parameters"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/CFN_Parameters.html#cloudformation-parameters","text":"All entries are Case-Sensitive","title":"CloudFormation Parameters"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/CFN_Parameters.html#single-region-stack","text":"Parameter Default Value CreateTheAutoScalingServiceRole true CreateTheELBServiceRole true CreateTheRDSServiceRole true LambdaFunctionsBucket aws-well-architected-labs-ohio RDSLambdaKey Reliability/RDSLambda.zip VPCLambdaKey Reliability/Reliability/VPCLambda.zip WaitForStackLambdaKey Reliability/WaitForStack.zip WebAppLambdaKey Reliability/WebAppLambda.zip","title":"single region stack"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/CFN_Parameters.html#multi-region-stack","text":"Parameter Default Value CreateTheAutoScalingServiceRole true CreateTheELBServiceRole true CreateTheRDSServiceRole true DMSLambdaKey Reliability/DMSLambda.zip LambdaFunctionsBucket aws-well-architected-labs-ohio RDSLambdaKey Reliability/RDSLambda.zip RDSRRLambdaKey Reliability/RDSReadReplicaLambda.zip VPCLambdaKey Reliability/Reliability/VPCLambda.zip WaitForStackLambdaKey Reliability/WaitForStack.zip WebAppLambdaKey Reliability/WebAppLambda.zip Click here to return to the Lab Guide","title":"multi region stack"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Multi_Region_Event_Data.html","text":"New Execution Input for multi region Deployment On the \"New execution\" dialog, for \"Enter an execution name\" enter BuildResiliency Then for \"Input\" enter JSON that will be used to supply parameter values to the Lambdas in the workflow. multi region uses the following values { \"region1\": { \"log_level\": \"DEBUG\", \"region_name\": \"us-east-2\", \"secondary_region_name\": \"us-west-2\", \"cfn_region\": \"us-east-2\", \"cfn_bucket\": \"aws-well-architected-labs-ohio\", \"folder\": \"Reliability/\", \"workshop\": \"300-ResiliencyofEC2RDSandS3\", \"boot_bucket\": \"aws-well-architected-labs-ohio\", \"boot_prefix\": \"Reliability/\", \"websiteimage\" : \"https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg\" }, \"region2\": { \"log_level\": \"DEBUG\", \"region_name\": \"us-west-2\", \"secondary_region_name\": \"us-east-2\", \"cfn_region\": \"us-east-2\", \"cfn_bucket\": \"aws-well-architected-labs-ohio\", \"folder\": \"Reliability/\", \"workshop\": \"300-ResiliencyofEC2RDSandS3\", \"boot_bucket\": \"aws-well-architected-labs-ohio\", \"boot_prefix\": \"Reliability/\", \"websiteimage\" : \"https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg\" } } Note : for websiteimage you can supply an alternate link to a public-read-only image in an S3 bucket you control. This will allow you to run S3 resiliency tests as part of the lab Click here to return to the Lab Guide","title":"New Execution Input for **multi region** Deployment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Multi_Region_Event_Data.html#new-execution-input-for-multi-region-deployment","text":"On the \"New execution\" dialog, for \"Enter an execution name\" enter BuildResiliency Then for \"Input\" enter JSON that will be used to supply parameter values to the Lambdas in the workflow. multi region uses the following values { \"region1\": { \"log_level\": \"DEBUG\", \"region_name\": \"us-east-2\", \"secondary_region_name\": \"us-west-2\", \"cfn_region\": \"us-east-2\", \"cfn_bucket\": \"aws-well-architected-labs-ohio\", \"folder\": \"Reliability/\", \"workshop\": \"300-ResiliencyofEC2RDSandS3\", \"boot_bucket\": \"aws-well-architected-labs-ohio\", \"boot_prefix\": \"Reliability/\", \"websiteimage\" : \"https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg\" }, \"region2\": { \"log_level\": \"DEBUG\", \"region_name\": \"us-west-2\", \"secondary_region_name\": \"us-east-2\", \"cfn_region\": \"us-east-2\", \"cfn_bucket\": \"aws-well-architected-labs-ohio\", \"folder\": \"Reliability/\", \"workshop\": \"300-ResiliencyofEC2RDSandS3\", \"boot_bucket\": \"aws-well-architected-labs-ohio\", \"boot_prefix\": \"Reliability/\", \"websiteimage\" : \"https://s3.us-east-2.amazonaws.com/arc327-well-architected-for-reliability/Cirque_of_the_Towers.jpg\" } } Note : for websiteimage you can supply an alternate link to a public-read-only image in an S3 bucket you control. This will allow you to run S3 resiliency tests as part of the lab Click here to return to the Lab Guide","title":"New Execution Input for multi region Deployment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Multi_Region_Stack_Deletion.html","text":"Delete workshop CloudFormation stacks - Multi region deployment Since AWS resources deployed by AWS CloudFormation stacks may have dependencies on the stacks that were created before, then deletion must occur in the opposite order they were created Stacks with the same ordinal can be deleted at the same time. All stacks for a given ordinal must be DELETE_COMPLETE before moving on to the next ordinal The AWS Console does not let you select multiple stacks for deletion. To simultaneously delete stacks, individually select one stack at a time and click the Delete button. Helpful hint: have the AWS CloudFormation console for each region open in separate tabs CloudFormation console for Ohio CloudFormation console for Oregon Order CloudFormation stack Region 1 DMSforResiliencyTesting Oregon 1 MySQLReadReplicaResiliencyTesting Oregon 1 MySQLReadReplicaResiliencyTesting Ohio 2 WebServersforResiliencyTesting Ohio 2 MySQLforResiliencyTesting Ohio 2 WebServersforResiliencyTesting Oregon 2 MySQLforResiliencyTesting Oregon 3 ResiliencyVPC Ohio 3 ResiliencyVPC Oregon 3 DeployResiliencyWorkshop Ohio Click here to return to the Lab Guide","title":"Delete workshop CloudFormation stacks - Multi region deployment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Multi_Region_Stack_Deletion.html#delete-workshop-cloudformation-stacks-multi-region-deployment","text":"Since AWS resources deployed by AWS CloudFormation stacks may have dependencies on the stacks that were created before, then deletion must occur in the opposite order they were created Stacks with the same ordinal can be deleted at the same time. All stacks for a given ordinal must be DELETE_COMPLETE before moving on to the next ordinal The AWS Console does not let you select multiple stacks for deletion. To simultaneously delete stacks, individually select one stack at a time and click the Delete button. Helpful hint: have the AWS CloudFormation console for each region open in separate tabs CloudFormation console for Ohio CloudFormation console for Oregon Order CloudFormation stack Region 1 DMSforResiliencyTesting Oregon 1 MySQLReadReplicaResiliencyTesting Oregon 1 MySQLReadReplicaResiliencyTesting Ohio 2 WebServersforResiliencyTesting Ohio 2 MySQLforResiliencyTesting Ohio 2 WebServersforResiliencyTesting Oregon 2 MySQLforResiliencyTesting Oregon 3 ResiliencyVPC Ohio 3 ResiliencyVPC Oregon 3 DeployResiliencyWorkshop Ohio Click here to return to the Lab Guide","title":"Delete workshop CloudFormation stacks - Multi region deployment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Multi_Region_State_Machine.html","text":"Multi Region State Machine Click here to return to the Lab Guide","title":"Multi Region State Machine"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Multi_Region_State_Machine.html#multi-region-state-machine","text":"Click here to return to the Lab Guide","title":"Multi Region State Machine"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Programming_Environment.html","text":"Setting up an environment to run the workshop using a programming language If you will be using Bash for this workshop, STOP and return to the Lab Guide instructions for setting up Bash If you will not be using Bash and prefer to use Python, Java, C#, or PowerShell for this workshop, then follow these steps 1. Set up AWS credentials If you have not yet setup your AWS credentials, then follow this guide 2. Language specific setup Choose the appropriate section below for your language 2.1 Setting Up the Python Environment The scripts are written in python with boto3. On Amazon Linux, this is already installed. Use your local operating system instructions to install boto3: https://github.com/boto/boto3 Download the resiliency Python scripts from GitHub to a location convenient for you to execute them. You can use the following links to download the scripts: python/fail_instance.py python/fail_rds.py python/fail_az.py 2.2 Setting Up the Java Environment The command line utility in Java requires Java 8 SE. $ java -version openjdk version \"1.8.0_222\" OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10) OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode) If you have java 1.7 installed (as will be the case for In Amazon Linux), you need to install Java 8 and remove Java 7. For Amazon Linux and RedHat $ sudo yum install java-1.8.0-openjdk $ sudo yum remove java-1.7.0-openjdk For Debian, Ubuntu $ sudo apt install openjdk-8-jdk $ sudo apt install openjdk-7-jdk Next choose one of the following options: Option A or Option B . Option A: If you are comfortable with git Clone the aws-well-architected-labs repo $ git clone https://github.com/awslabs/aws-well-architected-labs.git Cloning into 'aws-well-architected-labs'... ... Checking out files: 100% (1935/1935), done. go to the build directory cd aws-well-architected-labs/Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Code/FailureSimulations/java/appresiliency Option B: Download the zipfile of the executables at the following URL https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip go to the build directory: cd java/appresiliency Build: mvn clean package shade:shade cd target - this is where your jar files were built and where you can run from the command line 2.3 Setting Up the C# Environment Download the zipfile of the executables at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip Unzip the folder in a location convenient for you to execute the command line programs. 2.4 Setting up the Powershell Environment If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ Download the resiliency PowerShell scripts from GitHub to a location convenient for you to execute them. You can use the following links to download the scripts: powershell/fail_instance.ps1 powershell/failover_rds.ps1 powershell/fail_az.ps1 If your PowerShell scripts are refused authorization to access your AWS account, consult Getting Started with the AWS Tools for Windows PowerShell Click here to return to the Lab Guide","title":"Setting up an environment to run the workshop using a programming language"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Programming_Environment.html#setting-up-an-environment-to-run-the-workshop-using-a-programming-language","text":"If you will be using Bash for this workshop, STOP and return to the Lab Guide instructions for setting up Bash If you will not be using Bash and prefer to use Python, Java, C#, or PowerShell for this workshop, then follow these steps","title":"Setting up an environment to run the workshop using a programming language"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Programming_Environment.html#1-set-up-aws-credentials","text":"If you have not yet setup your AWS credentials, then follow this guide","title":"1. Set up AWS credentials"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Programming_Environment.html#2-language-specific-setup","text":"Choose the appropriate section below for your language","title":"2. Language specific setup"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Programming_Environment.html#21-setting-up-the-python-environment","text":"The scripts are written in python with boto3. On Amazon Linux, this is already installed. Use your local operating system instructions to install boto3: https://github.com/boto/boto3 Download the resiliency Python scripts from GitHub to a location convenient for you to execute them. You can use the following links to download the scripts: python/fail_instance.py python/fail_rds.py python/fail_az.py","title":"2.1 Setting Up the Python Environment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Programming_Environment.html#22-setting-up-the-java-environment","text":"The command line utility in Java requires Java 8 SE. $ java -version openjdk version \"1.8.0_222\" OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10) OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode) If you have java 1.7 installed (as will be the case for In Amazon Linux), you need to install Java 8 and remove Java 7. For Amazon Linux and RedHat $ sudo yum install java-1.8.0-openjdk $ sudo yum remove java-1.7.0-openjdk For Debian, Ubuntu $ sudo apt install openjdk-8-jdk $ sudo apt install openjdk-7-jdk Next choose one of the following options: Option A or Option B . Option A: If you are comfortable with git Clone the aws-well-architected-labs repo $ git clone https://github.com/awslabs/aws-well-architected-labs.git Cloning into 'aws-well-architected-labs'... ... Checking out files: 100% (1935/1935), done. go to the build directory cd aws-well-architected-labs/Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Code/FailureSimulations/java/appresiliency Option B: Download the zipfile of the executables at the following URL https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/javaresiliency.zip go to the build directory: cd java/appresiliency Build: mvn clean package shade:shade cd target - this is where your jar files were built and where you can run from the command line","title":"2.2 Setting Up the Java Environment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Programming_Environment.html#23-setting-up-the-c-environment","text":"Download the zipfile of the executables at the following URL. https://s3.us-east-2.amazonaws.com/aws-well-architected-labs-ohio/Reliability/csharpresiliency.zip Unzip the folder in a location convenient for you to execute the command line programs.","title":"2.3 Setting Up the C# Environment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Programming_Environment.html#24-setting-up-the-powershell-environment","text":"If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ Download the resiliency PowerShell scripts from GitHub to a location convenient for you to execute them. You can use the following links to download the scripts: powershell/fail_instance.ps1 powershell/failover_rds.ps1 powershell/fail_az.ps1 If your PowerShell scripts are refused authorization to access your AWS account, consult Getting Started with the AWS Tools for Windows PowerShell Click here to return to the Lab Guide","title":"2.4 Setting up the Powershell Environment"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/S3_with_AWS_CLI.html","text":"Disable All Public Read Access to an S3 Bucket using AWS CLI Disable read access to S3 bucket This command will disable public read from an entire bucket. If you want to only disable public read from one object, use the AWS Console instructions If your S3 bucket is in a different aWS account, you will need to provide credentials for that account first. aws ssm start-automation-execution --document-name AWS-DisableS3BucketPublicReadWrite --parameters \"{\\\"S3BucketName\\\": [\\\"<bucket-name>\\\"]}\" Return to the Lab Guide , but keep this page open if you want to re-enable public read access to the bucket after testing. Re-enable access (after testing) using the S3 console This requires using the S3 console. Go to the S3 console: https://console.aws.amazon.com/s3 Select the bucket name where the image is located Select the \"Permissions\" tab Click Edit (upper-right) Un-check all the boxes Click Save You are asked to type \"confirm\" - this is a security feature to ensure you truly intend this bucket to allow public access. Click here to return to the Lab Guide","title":"Disable All Public Read Access to an S3 Bucket using AWS CLI"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/S3_with_AWS_CLI.html#disable-all-public-read-access-to-an-s3-bucket-using-aws-cli","text":"","title":"Disable All Public Read Access to an S3 Bucket using AWS CLI"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/S3_with_AWS_CLI.html#disable-read-access-to-s3-bucket","text":"This command will disable public read from an entire bucket. If you want to only disable public read from one object, use the AWS Console instructions If your S3 bucket is in a different aWS account, you will need to provide credentials for that account first. aws ssm start-automation-execution --document-name AWS-DisableS3BucketPublicReadWrite --parameters \"{\\\"S3BucketName\\\": [\\\"<bucket-name>\\\"]}\" Return to the Lab Guide , but keep this page open if you want to re-enable public read access to the bucket after testing.","title":"Disable read access to S3 bucket"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/S3_with_AWS_CLI.html#re-enable-access-after-testing-using-the-s3-console","text":"This requires using the S3 console. Go to the S3 console: https://console.aws.amazon.com/s3 Select the bucket name where the image is located Select the \"Permissions\" tab Click Edit (upper-right) Un-check all the boxes Click Save You are asked to type \"confirm\" - this is a security feature to ensure you truly intend this bucket to allow public access. Click here to return to the Lab Guide","title":"Re-enable access (after testing) using the S3 console"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Self_AWS_Account.html","text":"Creating new AWS credentials for your AWS account Use these instructions to get a AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY which you will need for the workshop If you are using your own AWS account These instructions are for you. Use this guide if you are running the workshop on your own, or with at an event using your own AWS account you have brought with you If you are attending an in-person workshop and were provided with an AWS account by the instructor STOP -- Follow these instructions instead Create new AWS credentials for an IAM User you already control Sign in to the AWS Management Console as a IAM user who has IAM management permissions and open the IAM console at https://console.aws.amazon.com/iam/ In the navigation pane, choose Users . Choose the name of the user whose access keys you want to manage. Select the Permissions tab of this user and confirm that they have either PowerUserAccess or AdministratorAccess policy attached. If not, attach the PowerUserAccess policy using the Add permissions button. Select the Security credentials tab. Choose Create access key . Then choose Download .csv file to save the access key ID and secret access key to a CSV file on your computer. Store the file in a secure location. You will not have access to the secret access key again after this dialog box closes. After you download the CSV file, choose Close. Create a new IAM User for use in the lab Use the instructions only if you cannot Create new AWS credentials for an IAM User you already control . If you have already obtained a AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY using the preceding instructions then STOP and Click here to return to the Lab Guide Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ In the navigation pane, choose Users and then choose Add user . Type the user name for the new user. if you wish you can choose rel300-workshop Select programmatic access . Including access to the AWS Management Console is optional Choose Next: Permissions select Attach existing policies to user directly In the search box type PowerUserAccess tick the check box next to PowerUserAccess Choose Next: Tags Choose Next: Review Choose Create User IMPORTANT : Choose Download.csv file to save the access key ID and secret access key to a CSV file on your computer. Store the file in a secure location. You will not have access to the secret access key again after this dialog box closes. After you download the CSV file, choose Close. Click here to return to the Lab Guide","title":"Creating new AWS credentials for your AWS account"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Self_AWS_Account.html#creating-new-aws-credentials-for-your-aws-account","text":"Use these instructions to get a AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY which you will need for the workshop If you are using your own AWS account These instructions are for you. Use this guide if you are running the workshop on your own, or with at an event using your own AWS account you have brought with you If you are attending an in-person workshop and were provided with an AWS account by the instructor STOP -- Follow these instructions instead","title":"Creating new AWS credentials for your AWS account"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Self_AWS_Account.html#create-new-aws-credentials-for-an-iam-user-you-already-control","text":"Sign in to the AWS Management Console as a IAM user who has IAM management permissions and open the IAM console at https://console.aws.amazon.com/iam/ In the navigation pane, choose Users . Choose the name of the user whose access keys you want to manage. Select the Permissions tab of this user and confirm that they have either PowerUserAccess or AdministratorAccess policy attached. If not, attach the PowerUserAccess policy using the Add permissions button. Select the Security credentials tab. Choose Create access key . Then choose Download .csv file to save the access key ID and secret access key to a CSV file on your computer. Store the file in a secure location. You will not have access to the secret access key again after this dialog box closes. After you download the CSV file, choose Close.","title":"Create new AWS credentials for an IAM User you already control"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Self_AWS_Account.html#create-a-new-iam-user-for-use-in-the-lab","text":"Use the instructions only if you cannot Create new AWS credentials for an IAM User you already control . If you have already obtained a AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY using the preceding instructions then STOP and Click here to return to the Lab Guide Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ In the navigation pane, choose Users and then choose Add user . Type the user name for the new user. if you wish you can choose rel300-workshop Select programmatic access . Including access to the AWS Management Console is optional Choose Next: Permissions select Attach existing policies to user directly In the search box type PowerUserAccess tick the check box next to PowerUserAccess Choose Next: Tags Choose Next: Review Choose Create User IMPORTANT : Choose Download.csv file to save the access key ID and secret access key to a CSV file on your computer. Store the file in a secure location. You will not have access to the secret access key again after this dialog box closes. After you download the CSV file, choose Close. Click here to return to the Lab Guide","title":"Create a new IAM User for use in the lab"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Service_Linked_Roles.html","text":"Service-Linked Roles Does AWS account already have service-linked roles AWS requires \u201cservice-linked\u201d roles for AWS Auto Scaling, Elastic Load Balancing, and Amazon RDS to create the services and metrics they manage. If your AWS account has been previously been used, then these roles may already exist as they would have been automatically created for you. You will determine if any of the following three IAM service-linked roles already exists in the AWS account you are using for this workshop: AWSServiceRoleForElasticLoadBalancing AWSServiceRoleForAutoScaling AWSServiceRoleForRDS Steps to determine if service-linked roles already exist Open the IAM console at https://console.aws.amazon.com/iam/ In the navigation pane, click Roles . In the filter box, type \u201cService\u201d to find the service linked roles that exist in your account and look for the three roles. In this screenshot, the service linked role for AutoScaling exists ( AWSServiceRoleForAutoScaling ), but the roles for Elastic Load Balancing and RDS do not. Note which roles already exist as you will use this information when performing the next step. STOP HERE and return to the Lab Guide Learn more : After the lab see the AWS documentation on Service-Linked Roles Setup CloudFormation for service-linked roles If you are using your own AWS account : Then use these instructions when entering CloudFormation parameters If you are attending an in-person workshop and were provided with an AWS account by the instructor : Skip this step and go to back to the Lab Guide If you already have this role ...then set this parameter false AWSServiceRoleForElasticLoadBalancing CreateTheELBServiceRole AWSServiceRoleForAutoScaling CreateTheAutoScalingServiceRole AWSServiceRoleForRDS CreateTheRDSServiceRole If the service-linked role does not already exist, then leave the parameter value as true Leave all the other parameter values at their default values Click here to return to the Lab Guide","title":"Service-Linked Roles"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Service_Linked_Roles.html#service-linked-roles","text":"","title":"Service-Linked Roles"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Service_Linked_Roles.html#does-aws-account-already-have-service-linked-roles","text":"AWS requires \u201cservice-linked\u201d roles for AWS Auto Scaling, Elastic Load Balancing, and Amazon RDS to create the services and metrics they manage. If your AWS account has been previously been used, then these roles may already exist as they would have been automatically created for you. You will determine if any of the following three IAM service-linked roles already exists in the AWS account you are using for this workshop: AWSServiceRoleForElasticLoadBalancing AWSServiceRoleForAutoScaling AWSServiceRoleForRDS","title":"Does AWS account already have service-linked roles "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Service_Linked_Roles.html#steps-to-determine-if-service-linked-roles-already-exist","text":"Open the IAM console at https://console.aws.amazon.com/iam/ In the navigation pane, click Roles . In the filter box, type \u201cService\u201d to find the service linked roles that exist in your account and look for the three roles. In this screenshot, the service linked role for AutoScaling exists ( AWSServiceRoleForAutoScaling ), but the roles for Elastic Load Balancing and RDS do not. Note which roles already exist as you will use this information when performing the next step. STOP HERE and return to the Lab Guide Learn more : After the lab see the AWS documentation on Service-Linked Roles","title":"Steps to determine if service-linked roles already exist"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Service_Linked_Roles.html#setup-cloudformation-for-service-linked-roles","text":"If you are using your own AWS account : Then use these instructions when entering CloudFormation parameters If you are attending an in-person workshop and were provided with an AWS account by the instructor : Skip this step and go to back to the Lab Guide If you already have this role ...then set this parameter false AWSServiceRoleForElasticLoadBalancing CreateTheELBServiceRole AWSServiceRoleForAutoScaling CreateTheAutoScalingServiceRole AWSServiceRoleForRDS CreateTheRDSServiceRole If the service-linked role does not already exist, then leave the parameter value as true Leave all the other parameter values at their default values Click here to return to the Lab Guide","title":"Setup CloudFormation for service-linked roles "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Software_Install.html","text":"Software Install This reference will help you install software necessary to setup your workshop environment AWS CLI jq AWS CLI The AWS Command Line Interface (AWS CLI) is a unified tool that provides a consistent interface for interacting with all parts of AWS. Linux This includes: All native Linux installs MacOS Windows Subsystem for Linux (WSL) Run the following command $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.0 or higher is fine If you instead got command not found then you need to install awscli : $ pip3 install awscli --upgrade --user ...(lots of output)... Successfully installed... * If that succeeded, then you are finished. Return to the Lab Guide If that does not work, then do the following: See the detailed installation instructions here Other environments (not Linux) See the instructions here https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html STOP HERE and return to the Lab Guide jq jq is a command-line JSON processor. is like sed for JSON data. It is used in the workshop bash scripts to parse AWS CLI output. Run the following command $ jq --version jq-1.5-1-a5b5cbe Any version is fine. If you instead got command not found then you need to install jq . Follow the instructions at https://stedolan.github.io/jq/download/ If that succeeded, then you are finished. Return to the Lab Guide Alternate instructions for Linux If the steps above did not work, and you are running Linux, then try the following Download the jq executable $ wget https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 [...lots of output...] jq-linux64 100%[=================================================>] 3.77M 1.12MB/s in 3.5s 2019-10-11 17:41:42 (1.97 MB/s) - \u2018jq-linux64\u2019 saved [3953824/3953824] You can find out what your execution path is with the following command. $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable. $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory. $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq Click here to return to the Lab Guide","title":"Software Install"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Software_Install.html#software-install","text":"This reference will help you install software necessary to setup your workshop environment AWS CLI jq","title":"Software Install"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Software_Install.html#aws-cli","text":"The AWS Command Line Interface (AWS CLI) is a unified tool that provides a consistent interface for interacting with all parts of AWS.","title":"AWS CLI "},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Software_Install.html#linux","text":"This includes: All native Linux installs MacOS Windows Subsystem for Linux (WSL) Run the following command $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.0 or higher is fine If you instead got command not found then you need to install awscli : $ pip3 install awscli --upgrade --user ...(lots of output)... Successfully installed... * If that succeeded, then you are finished. Return to the Lab Guide If that does not work, then do the following: See the detailed installation instructions here","title":"Linux"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Software_Install.html#other-environments-not-linux","text":"See the instructions here https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html STOP HERE and return to the Lab Guide","title":"Other environments (not Linux)"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Software_Install.html#jq","text":"jq is a command-line JSON processor. is like sed for JSON data. It is used in the workshop bash scripts to parse AWS CLI output. Run the following command $ jq --version jq-1.5-1-a5b5cbe Any version is fine. If you instead got command not found then you need to install jq . Follow the instructions at https://stedolan.github.io/jq/download/ If that succeeded, then you are finished. Return to the Lab Guide","title":"jq"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Software_Install.html#alternate-instructions-for-linux","text":"If the steps above did not work, and you are running Linux, then try the following Download the jq executable $ wget https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 [...lots of output...] jq-linux64 100%[=================================================>] 3.77M 1.12MB/s in 3.5s 2019-10-11 17:41:42 (1.97 MB/s) - \u2018jq-linux64\u2019 saved [3953824/3953824] You can find out what your execution path is with the following command. $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable. $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory. $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq Click here to return to the Lab Guide","title":"Alternate instructions for Linux"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Workshop_AWS_Account.html","text":"Accessing your instructor-provided AWS account Go to https://dashboard.eventengine.run/login Enter the 12 character hashcode you were provided and click \"Proceed\" [optional] assign a name to your account (this is referred to as \"Team name\") click \"Set Team Name\" Enter a name and click \"Set Team Name\" Click \"AWS Console\" AWS credentials IMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop Copy the whole code block corresponding to the system you are using. Access the AWS console Click \"Open Console\". The AWS Console will open. Click here to return to the Lab Guide","title":"Accessing your instructor-provided AWS account"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Workshop_AWS_Account.html#accessing-your-instructor-provided-aws-account","text":"Go to https://dashboard.eventengine.run/login Enter the 12 character hashcode you were provided and click \"Proceed\" [optional] assign a name to your account (this is referred to as \"Team name\") click \"Set Team Name\" Enter a name and click \"Set Team Name\" Click \"AWS Console\"","title":"Accessing your instructor-provided AWS account"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Workshop_AWS_Account.html#aws-credentials","text":"IMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop Copy the whole code block corresponding to the system you are using.","title":"AWS credentials"},{"location":"Reliability/300_Testing_for_Resiliency_of_EC2_RDS_and_S3/Documentation/Workshop_AWS_Account.html#access-the-aws-console","text":"Click \"Open Console\". The AWS Console will open. Click here to return to the Lab Guide","title":"Access the AWS console"},{"location":"Security/README.html","text":"AWS Well-Architected Security Labs Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about security on AWS visit AWS Security and read the AWS Well-Architected Security whitepaper or online https://wa.aws.amazon.com/ . Also check out https://awssecworkshops.com/ for hands-on workshops, AWS Training and Certification Learning Library for official security training options. Labs Level 100: AWS Account and Root User Level 100: Basic Identity and Access Management User, Group, Role Level 100: CloudFront with S3 Bucket Origin Level 100: Enable Security Hub Level 200: Automated Deployment of Detective Controls Level 200: Automated Deployment of EC2 Web Application Level 200: Automated Deployment of IAM Groups and Roles Level 200: Automated Deployment of VPC Level 200: Automated Deployment of Web Application Firewall Level 200: Automated IAM User Cleanup Level 200: Basic EC2 with WAF Protection Level 200: Certificate Manager Request Public Certificate Level 200: CloudFront with WAF Protection Level 300: IAM Permission Boundaries Delegating Role Creation Level 300: IAM Tag Based Access Control for EC2 Level 300: Incident Response Playbook with Jupyter - AWS IAM Level 300: Incident Response with AWS Console and CLI Level 300: Lambda Cross Account IAM Role Assumption Quests Quests are designed to collate a group of relevant labs and other resources together into a common theme for you to follow and learn. Level 100: Introduction to Security Introduction to AWS security basics, used as the workshop in AWS loft events. Level 100: Quick Steps to Security Success In just one day (or an hour a day for a week!) implement some foundational security controls to immediately improve your security posture. Level 200: Incident Response Day This quest is the guide for incident response workshop often ran at AWS led events. Level 300: Security Best Practices Workshop This quest is the guide for security best practices workshop often ran at AWS led events including AWS Summits. Level 300: Security Best Practices Day This quest is the guide for an AWS led event including security best practices day. Includes identity & access management, detective controls, infrastructure protection, data protection and incident response. The following quests are aligned to the security best practice questions in AWS Well-Architected. Managing Credentials & Authentication Control Human Access Control Programmatic Access Detect and Investigate Events Defend Against New Threats Protect Networks Protect Compute Classify Data Protect Data at Rest Protect Data in Transit Incident Response License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Security/README.html#aws-well-architected-security-labs","text":"","title":"AWS Well-Architected Security Labs"},{"location":"Security/README.html#introduction","text":"This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about security on AWS visit AWS Security and read the AWS Well-Architected Security whitepaper or online https://wa.aws.amazon.com/ . Also check out https://awssecworkshops.com/ for hands-on workshops, AWS Training and Certification Learning Library for official security training options.","title":"Introduction"},{"location":"Security/README.html#labs","text":"Level 100: AWS Account and Root User Level 100: Basic Identity and Access Management User, Group, Role Level 100: CloudFront with S3 Bucket Origin Level 100: Enable Security Hub Level 200: Automated Deployment of Detective Controls Level 200: Automated Deployment of EC2 Web Application Level 200: Automated Deployment of IAM Groups and Roles Level 200: Automated Deployment of VPC Level 200: Automated Deployment of Web Application Firewall Level 200: Automated IAM User Cleanup Level 200: Basic EC2 with WAF Protection Level 200: Certificate Manager Request Public Certificate Level 200: CloudFront with WAF Protection Level 300: IAM Permission Boundaries Delegating Role Creation Level 300: IAM Tag Based Access Control for EC2 Level 300: Incident Response Playbook with Jupyter - AWS IAM Level 300: Incident Response with AWS Console and CLI Level 300: Lambda Cross Account IAM Role Assumption","title":"Labs"},{"location":"Security/README.html#quests","text":"Quests are designed to collate a group of relevant labs and other resources together into a common theme for you to follow and learn. Level 100: Introduction to Security Introduction to AWS security basics, used as the workshop in AWS loft events. Level 100: Quick Steps to Security Success In just one day (or an hour a day for a week!) implement some foundational security controls to immediately improve your security posture. Level 200: Incident Response Day This quest is the guide for incident response workshop often ran at AWS led events. Level 300: Security Best Practices Workshop This quest is the guide for security best practices workshop often ran at AWS led events including AWS Summits. Level 300: Security Best Practices Day This quest is the guide for an AWS led event including security best practices day. Includes identity & access management, detective controls, infrastructure protection, data protection and incident response. The following quests are aligned to the security best practice questions in AWS Well-Architected. Managing Credentials & Authentication Control Human Access Control Programmatic Access Detect and Investigate Events Defend Against New Threats Protect Networks Protect Compute Classify Data Protect Data at Rest Protect Data in Transit Incident Response","title":"Quests"},{"location":"Security/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_AWS_Account_and_Root_User/README.html","text":"Level 100: AWS Account and Root User Introduction This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Protecting AWS credentials Fine-grained authorization Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#level-100-aws-account-and-root-user","text":"","title":"Level 100: AWS Account and Root User"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#introduction","text":"This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#goals","text":"Protecting AWS credentials Fine-grained authorization","title":"Goals"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/100_AWS_Account_and_Root_User/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html","text":"Level 100: AWS Account and Root User: Lab Guide 1. Account Settings & Root User Security When you first create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user and is accessed by signing in with the email address and password that you used to create the account. It is strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user, groups and roles. Then securely lock away the root user credentials and use them to perform only a few account and service management tasks. To view the tasks that require you to sign in as the root user, see AWS Tasks That Require Root User . 1.1 Generate and Review the AWS Account Credential Report Its good to get an idea of what you have configured already in your AWS account especially if you have had it for a while. You should audit your security configuration in the following situations: On a periodic basis. You should perform the steps described here at regular intervals as a best practice for security. If there are changes in your organization, such as people leaving. If you have stopped using one or more individual AWS services. This is important for removing permissions that users in your account no longer need. If you've added or removed software in your accounts, such as applications on Amazon EC2 instances, AWS OpsWorks stacks, AWS CloudFormation templates, etc. If you ever suspect that an unauthorized person might have accessed your account. As you review your account's security configuration, follow these guidelines: Be thorough . Look at all aspects of your security configuration, including those you might not use regularly. Don't assume . If you are unfamiliar with some aspect of your security configuration (for example, the reasoning behind a particular policy or the existence of a role), investigate the business need until you are satisfied. Keep things simple . To make auditing (and management) easier, use IAM groups, consistent naming schemes, and straightforward policies. More information can be found at https://docs.aws.amazon.com/general/latest/gr/aws-security-audit-guide.html You can use the AWS Management Console to download a credential report as a comma-separated values (CSV) file. Please note that credential report can take 4 hours to reflect changes. To download a credential report using the AWS Management Console: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Credential report. Click Download Report. Further information about the report can be found at https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html 1.2 Enable a Virtual MFA Device for Your AWS Account Root User You can use IAM in the AWS Management Console to configure and enable a virtual MFA device for your root user. To manage MFA devices for the AWS account, you must be signed in to AWS using your root user credentials. You cannot manage MFA devices for the root user using other credentials. If your MFA device is lost, stolen, or not working, you can still sign in using alternative factors of authentication. To do this, you must verify your identity using the email and phone that are registered with your account. This means that if you can't sign in with your MFA device, you can sign in by verifying your identity using the email and phone that are registered with your account. Before you enable MFA for your root user, review your account settings and contact information to make sure that you have access to the email and phone number. To learn about signing in using alternative factors of authentication, see What If an MFA Device Is Lost or Stops Working ?. To disable this feature, contact AWS Support . Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ Do one of the following: Option 1 : Click Dashboard, and under Security Status, expand Activate MFA on your root user. Option 2 : On the right side of the navigation bar, click your account name, and click Security Credentials. If necessary, click Continue to Security Credentials. Then expand the Multi-Factor Authentication (MFA) section on the page. Click Manage MFA or Activate MFA, depending on which option you chose in the preceding step. In the wizard, click A virtual MFA device and then click Next Step. Confirm that a virtual MFA app is installed on the device, and then click Next Step. IAM generates and displays configuration information for the virtual MFA device, including a QR code graphic. The graphic is a representation of the secret configuration key that is available for manual entry on devices that do not support QR codes. With the Manage MFA Device wizard still open, open the virtual MFA app on the device. If the virtual MFA software supports multiple accounts (multiple virtual MFA devices), then click the option to create a new account (a new virtual device). The easiest way to configure the app is to use the app to scan the QR code. If you cannot scan the code, you can type the configuration information manually. To use the QR code to configure the virtual MFA device, follow the app instructions for scanning the code. For example, you might need to tap the camera icon or tap a command like Scan account barcode, and then use the device's camera to scan the QR code. If you cannot scan the code, type the configuration information manually by typing the Secret Configuration Key value into the app. For example, to do this in the AWS Virtual MFA app, click Manually add account, and then type the secret configuration key and click Create. Important Make a secure backup of the QR code or secret configuration key, or make sure that you enable multiple virtual MFA devices for your account. A virtual MFA device might become unavailable, for example, if you lose the smartphone where the virtual MFA device is hosted). If that happens, you will not be able to sign in to your account and you will have to contact customer service to remove MFA protection for the account. Note The QR code and secret configuration key generated by IAM are tied to your AWS account and cannot be used with a different account. They can, however, be reused to configure a new MFA device for your account in case you lose access to the original MFA device. The device starts generating six-digit numbers. In the Manage MFA Device wizard, in the Authentication Code 1 box, type the six-digit number that's currently displayed by the MFA device. Wait up to 30 seconds for the device to generate a new number, and then type the new six-digit number into the Authentication Code 2 box. Important Submit your request immediately after generating the codes. If you generate the codes and then wait too long to submit the request, the MFA device successfully associates with the user but the MFA device is out of sync. This happens because time-based one-time passwords (TOTP) expire after a short period of time. If this happens, you can resync the device. Click Next Step, and then click Finish. The device is ready for use with AWS. For information about using MFA with the AWS Management Console, see Using MFA Devices With Your IAM Sign-in Page . 1.3 Configure Account Security Challenge Questions Configure account security challenge questions because they are used to verify that you own an AWS account. Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to security challenge questions configuration section. Select three challenge questions and enter answers for each. Securely store the questions and answers as you would passwords or other credentials. Click update. 1.4 Configure Account Alternate Contacts Alternate contacts enable AWS to contact another person about issues with the account, even if you are unavailable. Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to alternate contacts configuration section. Enter contact details for billing, operations and security. Click update. 1.5 Remove Your AWS Account Root User Access Keys You use an access key (an access key ID and secret access key) to make programmatic requests to AWS. However, do not use your AWS account root user access key. The access key for your AWS account gives full access to all your resources for all AWS services, including your billing information. You cannot restrict the permissions associated with your AWS account access key. Check in the credential report; if you don't already have an access key for your AWS account, don't create one unless you absolutely need to. Instead, use your account email address and password to sign in to the AWS Management Console and create an IAM user for yourself that has administrative privileges. This will be explained in a later section. If you do have an access key for your AWS account, delete it unless you have a specific requirement. To delete or rotate your AWS account access keys, go to the Security Credentials page in the AWS Management Console and sign in with your account's email address and password. You can manage your access keys in the Access keys section. Never share your AWS account password or access keys with anyone. 1.6 Periodically Change the AWS Account Root User Password You must be signed in as the AWS account root user in order to change the password. To learn how to reset a forgotten root user password, see Resetting Your Lost or Forgotten Passwords or Access Keys . To change the password for the root user: Use your AWS account email address and password to sign in to the AWS Management Console as the root user. Note If you previously signed in to the console with IAM user credentials, your browser might remember this preference and open your account-specific sign-in page. You cannot use the IAM user sign-in page to sign in with your AWS account root user credentials. If you see the IAM user sign-in page, click Sign-in using root account credentials near the bottom of the page to return to the main sign-in page. From there, you can type your AWS account email address and password. In the upper right corner of the console, click your account name or number and then click My Account. On the right side of the page, next to the Account Settings section, click Edit. On the Password line choose Click here to change your password. Choose a strong password. Although you can set an account password policy for IAM users, that policy does not apply to your AWS account root user. AWS requires that your password meet these conditions: have a minimum of 8 characters and a maximum of 128 characters include a minimum of three of the following mix of character types: uppercase, lowercase, numbers, and ! @ # $ % ^ & * () <> [] {} | _ + - = symbols not be identical to your AWS account name or email address Note AWS is rolling out improvements to the sign-in process. One of those improvements is to enforce a more secure password policy for your account. If your account has been upgraded, you are required to meet the password policy above. If your account has not yet been upgraded, then AWS does not enforce this policy, but highly recommends that you follow its guidelines for a more secure password. To protect your password, it's important to follow these best practices: Change your password periodically and keep your password private, since anyone who knows your password can access your account. Use a different password on AWS than you use on other sites. Avoid passwords that are easy to guess. These include passwords such as secret, password, amazon, or 123456. They also include things like a dictionary word, your name, email address, or other personal information that can easily be obtained. 1.7 Configure a Strong Password Policy for Your Users You can set a password policy on your AWS account to specify complexity requirements and mandatory rotation periods for your IAM users' passwords. The IAM password policy does not apply to the AWS root account password. To create or change a password policy: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. In the navigation pane, click Account Settings. In the Password Policy section, select the options you want to apply to your password policy. Click Apply Password Policy. 2. Tear down this lab Please note that the changes you made to the account and root user have no charges associated with them. References & useful resources AWS Tasks That Require Root User https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Resetting Your Lost or Forgotten Passwords or Access Keys Using MFA Devices With Your IAM Sign-in Page What If an MFA Device Is Lost or Stops Working License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#level-100-aws-account-and-root-user-lab-guide","text":"","title":"Level 100: AWS Account and Root User: Lab Guide"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#1-account-settings-root-user-security","text":"When you first create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS account root user and is accessed by signing in with the email address and password that you used to create the account. It is strongly recommend that you do not use the root user for your everyday tasks, even the administrative ones. Instead, adhere to the best practice of using the root user only to create your first IAM user, groups and roles. Then securely lock away the root user credentials and use them to perform only a few account and service management tasks. To view the tasks that require you to sign in as the root user, see AWS Tasks That Require Root User .","title":"1. Account Settings &amp; Root User Security"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#11-generate-and-review-the-aws-account-credential-report","text":"Its good to get an idea of what you have configured already in your AWS account especially if you have had it for a while. You should audit your security configuration in the following situations: On a periodic basis. You should perform the steps described here at regular intervals as a best practice for security. If there are changes in your organization, such as people leaving. If you have stopped using one or more individual AWS services. This is important for removing permissions that users in your account no longer need. If you've added or removed software in your accounts, such as applications on Amazon EC2 instances, AWS OpsWorks stacks, AWS CloudFormation templates, etc. If you ever suspect that an unauthorized person might have accessed your account. As you review your account's security configuration, follow these guidelines: Be thorough . Look at all aspects of your security configuration, including those you might not use regularly. Don't assume . If you are unfamiliar with some aspect of your security configuration (for example, the reasoning behind a particular policy or the existence of a role), investigate the business need until you are satisfied. Keep things simple . To make auditing (and management) easier, use IAM groups, consistent naming schemes, and straightforward policies. More information can be found at https://docs.aws.amazon.com/general/latest/gr/aws-security-audit-guide.html You can use the AWS Management Console to download a credential report as a comma-separated values (CSV) file. Please note that credential report can take 4 hours to reflect changes. To download a credential report using the AWS Management Console: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Credential report. Click Download Report. Further information about the report can be found at https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html","title":"1.1 Generate and Review the AWS Account Credential Report"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#12-enable-a-virtual-mfa-device-for-your-aws-account-root-user","text":"You can use IAM in the AWS Management Console to configure and enable a virtual MFA device for your root user. To manage MFA devices for the AWS account, you must be signed in to AWS using your root user credentials. You cannot manage MFA devices for the root user using other credentials. If your MFA device is lost, stolen, or not working, you can still sign in using alternative factors of authentication. To do this, you must verify your identity using the email and phone that are registered with your account. This means that if you can't sign in with your MFA device, you can sign in by verifying your identity using the email and phone that are registered with your account. Before you enable MFA for your root user, review your account settings and contact information to make sure that you have access to the email and phone number. To learn about signing in using alternative factors of authentication, see What If an MFA Device Is Lost or Stops Working ?. To disable this feature, contact AWS Support . Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ Do one of the following: Option 1 : Click Dashboard, and under Security Status, expand Activate MFA on your root user. Option 2 : On the right side of the navigation bar, click your account name, and click Security Credentials. If necessary, click Continue to Security Credentials. Then expand the Multi-Factor Authentication (MFA) section on the page. Click Manage MFA or Activate MFA, depending on which option you chose in the preceding step. In the wizard, click A virtual MFA device and then click Next Step. Confirm that a virtual MFA app is installed on the device, and then click Next Step. IAM generates and displays configuration information for the virtual MFA device, including a QR code graphic. The graphic is a representation of the secret configuration key that is available for manual entry on devices that do not support QR codes. With the Manage MFA Device wizard still open, open the virtual MFA app on the device. If the virtual MFA software supports multiple accounts (multiple virtual MFA devices), then click the option to create a new account (a new virtual device). The easiest way to configure the app is to use the app to scan the QR code. If you cannot scan the code, you can type the configuration information manually. To use the QR code to configure the virtual MFA device, follow the app instructions for scanning the code. For example, you might need to tap the camera icon or tap a command like Scan account barcode, and then use the device's camera to scan the QR code. If you cannot scan the code, type the configuration information manually by typing the Secret Configuration Key value into the app. For example, to do this in the AWS Virtual MFA app, click Manually add account, and then type the secret configuration key and click Create. Important Make a secure backup of the QR code or secret configuration key, or make sure that you enable multiple virtual MFA devices for your account. A virtual MFA device might become unavailable, for example, if you lose the smartphone where the virtual MFA device is hosted). If that happens, you will not be able to sign in to your account and you will have to contact customer service to remove MFA protection for the account. Note The QR code and secret configuration key generated by IAM are tied to your AWS account and cannot be used with a different account. They can, however, be reused to configure a new MFA device for your account in case you lose access to the original MFA device. The device starts generating six-digit numbers. In the Manage MFA Device wizard, in the Authentication Code 1 box, type the six-digit number that's currently displayed by the MFA device. Wait up to 30 seconds for the device to generate a new number, and then type the new six-digit number into the Authentication Code 2 box. Important Submit your request immediately after generating the codes. If you generate the codes and then wait too long to submit the request, the MFA device successfully associates with the user but the MFA device is out of sync. This happens because time-based one-time passwords (TOTP) expire after a short period of time. If this happens, you can resync the device. Click Next Step, and then click Finish. The device is ready for use with AWS. For information about using MFA with the AWS Management Console, see Using MFA Devices With Your IAM Sign-in Page .","title":"1.2 Enable a Virtual MFA Device for Your AWS Account Root User"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#13-configure-account-security-challenge-questions","text":"Configure account security challenge questions because they are used to verify that you own an AWS account. Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to security challenge questions configuration section. Select three challenge questions and enter answers for each. Securely store the questions and answers as you would passwords or other credentials. Click update.","title":"1.3 Configure Account Security Challenge Questions"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#14-configure-account-alternate-contacts","text":"Alternate contacts enable AWS to contact another person about issues with the account, even if you are unavailable. Use your AWS account email address and password to sign in as the AWS account root user and open the AWS account settings page at https://console.aws.amazon.com/billing/home?#/account/ . Navigate to alternate contacts configuration section. Enter contact details for billing, operations and security. Click update.","title":"1.4 Configure Account Alternate Contacts"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#15-remove-your-aws-account-root-user-access-keys","text":"You use an access key (an access key ID and secret access key) to make programmatic requests to AWS. However, do not use your AWS account root user access key. The access key for your AWS account gives full access to all your resources for all AWS services, including your billing information. You cannot restrict the permissions associated with your AWS account access key. Check in the credential report; if you don't already have an access key for your AWS account, don't create one unless you absolutely need to. Instead, use your account email address and password to sign in to the AWS Management Console and create an IAM user for yourself that has administrative privileges. This will be explained in a later section. If you do have an access key for your AWS account, delete it unless you have a specific requirement. To delete or rotate your AWS account access keys, go to the Security Credentials page in the AWS Management Console and sign in with your account's email address and password. You can manage your access keys in the Access keys section. Never share your AWS account password or access keys with anyone.","title":"1.5 Remove Your AWS Account Root User Access Keys"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#16-periodically-change-the-aws-account-root-user-password","text":"You must be signed in as the AWS account root user in order to change the password. To learn how to reset a forgotten root user password, see Resetting Your Lost or Forgotten Passwords or Access Keys . To change the password for the root user: Use your AWS account email address and password to sign in to the AWS Management Console as the root user. Note If you previously signed in to the console with IAM user credentials, your browser might remember this preference and open your account-specific sign-in page. You cannot use the IAM user sign-in page to sign in with your AWS account root user credentials. If you see the IAM user sign-in page, click Sign-in using root account credentials near the bottom of the page to return to the main sign-in page. From there, you can type your AWS account email address and password. In the upper right corner of the console, click your account name or number and then click My Account. On the right side of the page, next to the Account Settings section, click Edit. On the Password line choose Click here to change your password. Choose a strong password. Although you can set an account password policy for IAM users, that policy does not apply to your AWS account root user. AWS requires that your password meet these conditions: have a minimum of 8 characters and a maximum of 128 characters include a minimum of three of the following mix of character types: uppercase, lowercase, numbers, and ! @ # $ % ^ & * () <> [] {} | _ + - = symbols not be identical to your AWS account name or email address Note AWS is rolling out improvements to the sign-in process. One of those improvements is to enforce a more secure password policy for your account. If your account has been upgraded, you are required to meet the password policy above. If your account has not yet been upgraded, then AWS does not enforce this policy, but highly recommends that you follow its guidelines for a more secure password. To protect your password, it's important to follow these best practices: Change your password periodically and keep your password private, since anyone who knows your password can access your account. Use a different password on AWS than you use on other sites. Avoid passwords that are easy to guess. These include passwords such as secret, password, amazon, or 123456. They also include things like a dictionary word, your name, email address, or other personal information that can easily be obtained.","title":"1.6 Periodically Change the AWS Account Root User Password"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#17-configure-a-strong-password-policy-for-your-users","text":"You can set a password policy on your AWS account to specify complexity requirements and mandatory rotation periods for your IAM users' passwords. The IAM password policy does not apply to the AWS root account password. To create or change a password policy: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/. In the navigation pane, click Account Settings. In the Password Policy section, select the options you want to apply to your password policy. Click Apply Password Policy.","title":"1.7 Configure a Strong Password Policy for Your Users"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#2-tear-down-this-lab","text":"Please note that the changes you made to the account and root user have no charges associated with them.","title":"2. Tear down this lab"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#references-useful-resources","text":"AWS Tasks That Require Root User https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_getting-report.html AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Resetting Your Lost or Forgotten Passwords or Access Keys Using MFA Devices With Your IAM Sign-in Page What If an MFA Device Is Lost or Stops Working","title":"References &amp; useful resources"},{"location":"Security/100_AWS_Account_and_Root_User/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html","text":"Level 100: Basic Identity and Access Management User, Group, Role Introduction This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Protecting AWS credentials Fine-grained authorization Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#level-100-basic-identity-and-access-management-user-group-role","text":"","title":"Level 100: Basic Identity and Access Management User, Group, Role"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#introduction","text":"This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#goals","text":"Protecting AWS credentials Fine-grained authorization","title":"Goals"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html","text":"Level 100: Basic Identity and Access Management User, Group, Role: Lab Guide 1. AWS Identity & Access Management As a best practice, do not use the AWS account root user for any task where it's not required. Instead, create a new IAM user for each person that requires administrator access. Then make those users administrators (only if they absolutely need full access to everything) by placing the users into an \"Administrators\" group to which you attach the AdministratorAccess managed policy. The following image shows what you will be doing in the next section 1.1 Create Administrator IAM User and Group. 1.1 Create Administrator IAM User and Group To create an administrator user for yourself and add the user to an administrators group: Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Users and then click Add user . For User name , type a user name for yourself, such as Bob or Alice. Each user should have their own user name, do not share credentials. The name can consist of letters, digits, and the following characters: plus (+) , equal (=) , comma (,) , period (.) , at (@) , underscore (_) , and hyphen (-) . The name is not case sensitive and can be a maximum of 64 characters in length. Select the check box next to AWS Management Console access , select Custom password , and then type your new password in the text box. This user will be able to do almost anything in your account, by not giving it programmatic access (access & secret key) you reduce your risk, and we will configure lower-privileged users and roles later. If you're creating the user for someone other than yourself, you can optionally select Require password reset to force the user to create a new password when first signing in. Click Next: Permissions . On the Set permissions for user page, click Add user to group . Click Create group . In the Create group dialog box, type the name for the new group such as Administrators. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 128 characters in length. In the policy list, select the check box next to AdministratorAccess . Then click Create group . Back in the list of groups, verify the check box is next to your new group. Click Refresh if necessary to see the group in the list. Click Next: Tags . For this lab we will not add tags to the user. Click Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, click Create user. You can use this same process to create more groups and users and to give your users access to your AWS account resources. To learn about using policies that restrict user permissions to specific AWS resources, see Access Management and Example Policies . To add users to the group after it's created, see Adding and Removing Users in an IAM Group . Configure MFA on your new administrator user by choosing Users from the navigation pane. In the User Name list, click the name of the intended MFA user. Click the Security credentials tab. Next to Assigned MFA device , click the edit icon. You can now use this administrator user instead of your root user for this AWS account. It is a best practice to use least privileged access approach to granting permissions, not everyone needs full administrator access! The following image shows what you will be doing in the next section 1.2 Create Administrator IAM Role. 1.2 Create Administrator IAM Role To create an administrator role for yourself (and other administrators) to be used with the administrator user and group you just created: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles and then click Create role . Click Another AWS account, then enter your account ID and tick Require MFA , then click Next: Permissions Tick AdministratorAccess from the list, and then click Next: Tags . Click Next: Review Enter a role name, e.g. 'Administrators' then click Create role . Check the role you have configured by clicking the role you have just created. Record both the Role ARN and the link to the console. You can also optionally change the session duration timeout. The role is now created, with full administrative access and MFA enforced. 2. Assume Administrator Role from an IAM user We will assume the role using the IAM user that we previously created in the web console. As the IAM user has full access it is a best practice not to have access keys to assume the role on the CLI, instead we should use a restricted IAM user for this so we can enforce the requirement of MFA. The following image shows what you will be doing in the next section 2.1 Use Administrator Role in Web Console. 2.1 Use Administrator Role in Web Console A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don't sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour. Important The permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored. Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias . Alternatively you can paste the link in your browser that you recorded earlier. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. On the Switch Role page, type the account ID number or the account alias in the Account field, and the name of the role that you created for the Administrator in the Role field. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. You are now using the role with the granted permissions! To stop using a role In the IAM console, click your role's Display Name on the right side of the navigation bar. Click Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored. 3. Tear down this lab Please note that the changes you made to the users, groups, and roles have no charges associated with them. References & useful resources AWS Identity and Access Management User Guide IAM Best Practices and Use Cases License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#level-100-basic-identity-and-access-management-user-group-role-lab-guide","text":"","title":"Level 100: Basic Identity and Access Management User, Group, Role: Lab Guide"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#1-aws-identity-access-management","text":"As a best practice, do not use the AWS account root user for any task where it's not required. Instead, create a new IAM user for each person that requires administrator access. Then make those users administrators (only if they absolutely need full access to everything) by placing the users into an \"Administrators\" group to which you attach the AdministratorAccess managed policy. The following image shows what you will be doing in the next section 1.1 Create Administrator IAM User and Group.","title":"1. AWS Identity &amp; Access Management"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#11-create-administrator-iam-user-and-group","text":"To create an administrator user for yourself and add the user to an administrators group: Use your AWS account email address and password to sign in as the AWS account root user to the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Users and then click Add user . For User name , type a user name for yourself, such as Bob or Alice. Each user should have their own user name, do not share credentials. The name can consist of letters, digits, and the following characters: plus (+) , equal (=) , comma (,) , period (.) , at (@) , underscore (_) , and hyphen (-) . The name is not case sensitive and can be a maximum of 64 characters in length. Select the check box next to AWS Management Console access , select Custom password , and then type your new password in the text box. This user will be able to do almost anything in your account, by not giving it programmatic access (access & secret key) you reduce your risk, and we will configure lower-privileged users and roles later. If you're creating the user for someone other than yourself, you can optionally select Require password reset to force the user to create a new password when first signing in. Click Next: Permissions . On the Set permissions for user page, click Add user to group . Click Create group . In the Create group dialog box, type the name for the new group such as Administrators. The name can consist of letters, digits, and the following characters: plus (+), equal (=), comma (,), period (.), at (@), underscore (_), and hyphen (-). The name is not case sensitive and can be a maximum of 128 characters in length. In the policy list, select the check box next to AdministratorAccess . Then click Create group . Back in the list of groups, verify the check box is next to your new group. Click Refresh if necessary to see the group in the list. Click Next: Tags . For this lab we will not add tags to the user. Click Next: Review to see the list of group memberships to be added to the new user. When you are ready to proceed, click Create user. You can use this same process to create more groups and users and to give your users access to your AWS account resources. To learn about using policies that restrict user permissions to specific AWS resources, see Access Management and Example Policies . To add users to the group after it's created, see Adding and Removing Users in an IAM Group . Configure MFA on your new administrator user by choosing Users from the navigation pane. In the User Name list, click the name of the intended MFA user. Click the Security credentials tab. Next to Assigned MFA device , click the edit icon. You can now use this administrator user instead of your root user for this AWS account. It is a best practice to use least privileged access approach to granting permissions, not everyone needs full administrator access! The following image shows what you will be doing in the next section 1.2 Create Administrator IAM Role.","title":"1.1 Create Administrator IAM User and Group"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#12-create-administrator-iam-role","text":"To create an administrator role for yourself (and other administrators) to be used with the administrator user and group you just created: Sign in to the AWS Management Console and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles and then click Create role . Click Another AWS account, then enter your account ID and tick Require MFA , then click Next: Permissions Tick AdministratorAccess from the list, and then click Next: Tags . Click Next: Review Enter a role name, e.g. 'Administrators' then click Create role . Check the role you have configured by clicking the role you have just created. Record both the Role ARN and the link to the console. You can also optionally change the session duration timeout. The role is now created, with full administrative access and MFA enforced.","title":"1.2 Create Administrator IAM Role"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#2-assume-administrator-role-from-an-iam-user","text":"We will assume the role using the IAM user that we previously created in the web console. As the IAM user has full access it is a best practice not to have access keys to assume the role on the CLI, instead we should use a restricted IAM user for this so we can enforce the requirement of MFA. The following image shows what you will be doing in the next section 2.1 Use Administrator Role in Web Console.","title":"2. Assume Administrator Role from an IAM user"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#21-use-administrator-role-in-web-console","text":"A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don't sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour. Important The permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored. Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias . Alternatively you can paste the link in your browser that you recorded earlier. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. On the Switch Role page, type the account ID number or the account alias in the Account field, and the name of the role that you created for the Administrator in the Role field. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. You are now using the role with the granted permissions! To stop using a role In the IAM console, click your role's Display Name on the right side of the navigation bar. Click Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored.","title":"2.1 Use Administrator Role in Web Console"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#3-tear-down-this-lab","text":"Please note that the changes you made to the users, groups, and roles have no charges associated with them.","title":"3. Tear down this lab"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#references-useful-resources","text":"AWS Identity and Access Management User Guide IAM Best Practices and Use Cases","title":"References &amp; useful resources"},{"location":"Security/100_Basic_Identity_and_Access_Management_User_Group_Role/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html","text":"Level 100: CloudFront with S3 Bucket Origin Introduction This hands-on lab will guide you through the steps to host static web content in an Amazon S3 bucket , protected and accelerated by Amazon CloudFront . Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Protecting S3 bucket from direct public access Improving access time with caching Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#level-100-cloudfront-with-s3-bucket-origin","text":"","title":"Level 100: CloudFront with S3 Bucket Origin"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#introduction","text":"This hands-on lab will guide you through the steps to host static web content in an Amazon S3 bucket , protected and accelerated by Amazon CloudFront . Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#goals","text":"Protecting S3 bucket from direct public access Improving access time with caching","title":"Goals"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab.","title":"Prerequisites"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html","text":"Level 100: CloudFront with S3 Bucket Origin: Lab Guide 1. Create S3 bucket Create an Amazon S3 bucket to host static content using the Amazon S3 console. For more information about Amazon S3, see Introduction to Amazon S3 . Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . From the console dashboard, choose Create bucket . Enter a name for your bucket, type a unique DNS-compliant name for your new bucket. Follow these naming guidelines: The name must be unique across all existing bucket names in Amazon S3. The name must not contain uppercase characters. The name must start with a lowercase letter or number. The name must be between 3 and 63 characters long. Choose an AWS Region where you want the bucket to reside. Choose a Region close to you to minimize latency and costs, or to address regulatory requirements. Note that for this example we will accept the default settings and this bucket is secure by default. Consider enabling additional security options such as logging and encryption, the S3 documentation has additional information such as Protecting Data in Amazon S3 . Click Create . 2. Upload example index.html file Create a simple index.html file, you can create by coping the following text into your favourite text editor. <!DOCTYPE html> <html> <head> <title>Example</title> </head> <body> <h1>Example Heading</h1> <p>Example paragraph.</p> </body> </html> Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . In the console click the name of your bucket you just created. Click the Upload button. Click the Add files button, select your index.html file, then click the Upload button. Your index.html file should now appear in the list. 3. Configure Amazon CloudFront Using the AWS Management Console, we will create a CloudFront distribution, and configure it to serve the S3 bucket we previously created. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home . From the console dashboard, click Create Distribution . Click Get Started in the Web section. Specify the following settings for the distribution: In the Origin Domain Name field Select the S3 bucket you created previously. In Restrict Bucket Access click the Yes radio then click Create a New Identity . Click the Yes, Update Bucket Policy Button . Scroll down to the Distribution Settings section, in the Default Root Object field enter index.html Click Create Distribution. To return to the main CloudFront page click Distributions from the left navigation menu. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution which may take approximately 10 minutes, the value of the Status column for your distribution will change from In Progress to Deployed . When your distribution is deployed, confirm that you can access your content using your new CloudFront Domain Name which you can see in the console. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You now have content in a private S3 bucket, that only CloudFront has secure access to. CloudFront then serves the requests, effectively becoming a secure, reliable static hosting service with additional features available such as custom certificates and alternate domain names . For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation. 4. Tear down this lab The following instructions will remove the CloudFront distribution and S3 bucket created in this lab. Delete the CloudFront distribution: Open the Amazon CloudFront console at (https://console.aws.amazon.com/cloudfront/home). From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Disabled , select the distribution and click the Delete . button, and then to confirm click the Yes, Delete button. Delete the S3 bucket: Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . Check the box next to the bucket you created previously, then click Empty from the menu. Confirm the bucket you are emptying. Once the bucket is empty check the box next to the bucket, then click Delete from the menu. Confirm the bucket you are deleting. References & useful resources Amazon S3 Developer Guide Amazon CloudFront Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#level-100-cloudfront-with-s3-bucket-origin-lab-guide","text":"","title":"Level 100: CloudFront with S3 Bucket Origin: Lab Guide"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#1-create-s3-bucket","text":"Create an Amazon S3 bucket to host static content using the Amazon S3 console. For more information about Amazon S3, see Introduction to Amazon S3 . Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . From the console dashboard, choose Create bucket . Enter a name for your bucket, type a unique DNS-compliant name for your new bucket. Follow these naming guidelines: The name must be unique across all existing bucket names in Amazon S3. The name must not contain uppercase characters. The name must start with a lowercase letter or number. The name must be between 3 and 63 characters long. Choose an AWS Region where you want the bucket to reside. Choose a Region close to you to minimize latency and costs, or to address regulatory requirements. Note that for this example we will accept the default settings and this bucket is secure by default. Consider enabling additional security options such as logging and encryption, the S3 documentation has additional information such as Protecting Data in Amazon S3 . Click Create .","title":"1. Create S3 bucket"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#2-upload-example-indexhtml-file","text":"Create a simple index.html file, you can create by coping the following text into your favourite text editor. <!DOCTYPE html> <html> <head> <title>Example</title> </head> <body> <h1>Example Heading</h1> <p>Example paragraph.</p> </body> </html> Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . In the console click the name of your bucket you just created. Click the Upload button. Click the Add files button, select your index.html file, then click the Upload button. Your index.html file should now appear in the list.","title":"2. Upload example index.html file"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#3-configure-amazon-cloudfront","text":"Using the AWS Management Console, we will create a CloudFront distribution, and configure it to serve the S3 bucket we previously created. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home . From the console dashboard, click Create Distribution . Click Get Started in the Web section. Specify the following settings for the distribution: In the Origin Domain Name field Select the S3 bucket you created previously. In Restrict Bucket Access click the Yes radio then click Create a New Identity . Click the Yes, Update Bucket Policy Button . Scroll down to the Distribution Settings section, in the Default Root Object field enter index.html Click Create Distribution. To return to the main CloudFront page click Distributions from the left navigation menu. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution which may take approximately 10 minutes, the value of the Status column for your distribution will change from In Progress to Deployed . When your distribution is deployed, confirm that you can access your content using your new CloudFront Domain Name which you can see in the console. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You now have content in a private S3 bucket, that only CloudFront has secure access to. CloudFront then serves the requests, effectively becoming a secure, reliable static hosting service with additional features available such as custom certificates and alternate domain names . For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.","title":"3. Configure Amazon CloudFront"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#4-tear-down-this-lab","text":"The following instructions will remove the CloudFront distribution and S3 bucket created in this lab. Delete the CloudFront distribution: Open the Amazon CloudFront console at (https://console.aws.amazon.com/cloudfront/home). From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Disabled , select the distribution and click the Delete . button, and then to confirm click the Yes, Delete button. Delete the S3 bucket: Open the Amazon S3 console at https://console.aws.amazon.com/s3/ . Check the box next to the bucket you created previously, then click Empty from the menu. Confirm the bucket you are emptying. Once the bucket is empty check the box next to the bucket, then click Delete from the menu. Confirm the bucket you are deleting.","title":"4. Tear down this lab"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#references-useful-resources","text":"Amazon S3 Developer Guide Amazon CloudFront Developer Guide","title":"References &amp; useful resources"},{"location":"Security/100_CloudFront_with_S3_Bucket_Origin/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_Create_a_Data_Bunker/README.html","text":"Level 100: Create a Data Bunker Account Overview In this lab we will create a secure data bunker. A data bunker is a secure account which will hold important security data in a secure location. Ensure that only members of your security team have access to this account. In this lab we will create a new security account, create a secure S3 bucket in that account and then turn on CloudTrail for our organisation to send these logs to the bucket in the secure data account. You may want to also think about what other data you need in there such as secure backups. Prerequisites An multi-account structure with AWS Organizations has been setup for your organization You have access to a role with administrative access to the root account for your AWS Organization NOTE: You will be billed for the AWS CloudTrail logs and Amazon S3 storage setup as part of this lab. See AWS CloudTrail Pricing and Amazon S3 Pricing for further details. Detailed Instructions 1. (Highly reccomended) Create a Security account from the organizations master account Best practice is to have a seperate security account for your data bunker. This account should only be accessible by folks in your security group with a read only role. How you create this account will depend on your organization's policies, the instructions below are guidance on how to do this. If you do not currently have organizations setup see the quest Quick Steps to Security Success or read the multi account strategy whitepaper for a more in-depth discussion. Login to the master account of your AWS Organization If you do not have an account within your organization to store security logs. Navigate to AWS Organizations and select Create Account . Include a cross account access role and note it's name (default is OrganizationAccountAccessRole) - we will modify this later to remove unnecessary access (Optional) If your role does not have permission to assume any role you will also have to add an IAM policy. The AWS administrator policy has this by default, otherwise follow the steps in the AWS Organizations Documentation to grant permissions to access the role Consider applying best practices as a baseline such as lock away your AWS account root user access keys and using multi-factor authentication Navigate to Settings and take a note of your Organization ID 2. Create the bucket for CloudTrail logs Swtich roles into the security account for your organization Navigate to S3 Press Create Bucket Enter a name for your bucket, make note of it and click Next Under configuration options enable versioning and enable object lock . This will prevent our logs from being deleted. Press Next Do not modify any permissions - press Next Press Create Bucket Press the bucket we just create and navigate to the Properties tab Under Object Lock , enable compliance mode and set a retention period . The length of the retention period will depend on your organisational requirements. If you are enabling this just for baseline security start with 31 days to keep one month of logs Under the Permissions tab, replace the Bucket Policy with the following, replacing [bucket] and [organization id]. Pres Save { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AWSCloudTrailAclCheck20150319\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"cloudtrail.amazonaws.com\" }, \"Action\": \"s3:GetBucketAcl\", \"Resource\": \"arn:aws:s3:::[bucket]\" }, { \"Sid\": \"AWSCloudTrailWrite20150319\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"cloudtrail.amazonaws.com\" }, \"Action\": \"s3:PutObject\", \"Resource\": \"arn:aws:s3:::[bucket]/AWSLogs/*\", \"Condition\": { \"StringEquals\": { \"s3:x-amz-acl\": \"bucket-owner-full-control\" } } }, { \"Sid\": \"AWSCloudTrailWrite20150319\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"cloudtrail.amazonaws.com\" }, \"Action\": \"s3:PutObject\", \"Resource\": \"arn:aws:s3:::[bucket]/AWSLogs/[organization id]/*\", \"Condition\": { \"StringEquals\": { \"s3:x-amz-acl\": \"bucket-owner-full-control\" } } } ] } (Optional) Next we will add a lifecycle policy to clean up old logs. Navigate to Management (Optional) Add a lifecycle rule named Delete old logs , press Next (Optional) Add a transition rule for both the current and previous versions to move to Glacier after 32 days. Press Next (Optional) Select the current and previous versions and set them to delete after 365 days 3. (Highly recommended) Ensure cross account access is read-only These instructions outline how to modify the cross account access created in step 1 is read-only. As with step 1, this will depend on how your organization's policies. The key is that our security team are not able to modify data in our data bunker. Human access should only be in a break-glass emergency situation. Note: Following these steps will prevent OrganizationAccountAccessRole from making further changes to this account. Ensure other services such as Amazon Guard Duty and AWS Security Hub are configured before proceeding. If further changes are needed you will have to reset the root credentials for the securtity account. Navigate to IAM and select Roles Select the organizations account access role for your orgainzation: Note: the default is OrganizationAccountAccessRole Press Attach Policy and attach the AWS managed ReadOnlyAccess Policy Navigate back to the OrganizationAccountAccessRole and press the X to remove the AdministratorAccess policy 4. Turn on CloudTrail from the root account Switch back to the root account Navigate to CloudTrail Select Trails from the menu on the left Press Create Trail Enter a name for the trail such as OrganizationTrail Select Yes next to Apply trail to my organization Under Storage location , select No for Create new S3 bucket and enter the bucket name of the bucket created in step 2 Verification Switch back to the Security account Navigate to the S3 bucket previously created (Optional) You can start to explore the logs using CloudTrail License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/100_Create_a_Data_Bunker/README.html#level-100-create-a-data-bunker-account","text":"","title":"Level 100: Create a Data Bunker Account"},{"location":"Security/100_Create_a_Data_Bunker/README.html#overview","text":"In this lab we will create a secure data bunker. A data bunker is a secure account which will hold important security data in a secure location. Ensure that only members of your security team have access to this account. In this lab we will create a new security account, create a secure S3 bucket in that account and then turn on CloudTrail for our organisation to send these logs to the bucket in the secure data account. You may want to also think about what other data you need in there such as secure backups.","title":"Overview"},{"location":"Security/100_Create_a_Data_Bunker/README.html#prerequisites","text":"An multi-account structure with AWS Organizations has been setup for your organization You have access to a role with administrative access to the root account for your AWS Organization NOTE: You will be billed for the AWS CloudTrail logs and Amazon S3 storage setup as part of this lab. See AWS CloudTrail Pricing and Amazon S3 Pricing for further details.","title":"Prerequisites"},{"location":"Security/100_Create_a_Data_Bunker/README.html#detailed-instructions","text":"","title":"Detailed Instructions"},{"location":"Security/100_Create_a_Data_Bunker/README.html#1-highly-reccomended-create-a-security-account-from-the-organizations-master-account","text":"Best practice is to have a seperate security account for your data bunker. This account should only be accessible by folks in your security group with a read only role. How you create this account will depend on your organization's policies, the instructions below are guidance on how to do this. If you do not currently have organizations setup see the quest Quick Steps to Security Success or read the multi account strategy whitepaper for a more in-depth discussion. Login to the master account of your AWS Organization If you do not have an account within your organization to store security logs. Navigate to AWS Organizations and select Create Account . Include a cross account access role and note it's name (default is OrganizationAccountAccessRole) - we will modify this later to remove unnecessary access (Optional) If your role does not have permission to assume any role you will also have to add an IAM policy. The AWS administrator policy has this by default, otherwise follow the steps in the AWS Organizations Documentation to grant permissions to access the role Consider applying best practices as a baseline such as lock away your AWS account root user access keys and using multi-factor authentication Navigate to Settings and take a note of your Organization ID","title":"1. (Highly reccomended) Create a Security account from the organizations master account"},{"location":"Security/100_Create_a_Data_Bunker/README.html#2-create-the-bucket-for-cloudtrail-logs","text":"Swtich roles into the security account for your organization Navigate to S3 Press Create Bucket Enter a name for your bucket, make note of it and click Next Under configuration options enable versioning and enable object lock . This will prevent our logs from being deleted. Press Next Do not modify any permissions - press Next Press Create Bucket Press the bucket we just create and navigate to the Properties tab Under Object Lock , enable compliance mode and set a retention period . The length of the retention period will depend on your organisational requirements. If you are enabling this just for baseline security start with 31 days to keep one month of logs Under the Permissions tab, replace the Bucket Policy with the following, replacing [bucket] and [organization id]. Pres Save { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AWSCloudTrailAclCheck20150319\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"cloudtrail.amazonaws.com\" }, \"Action\": \"s3:GetBucketAcl\", \"Resource\": \"arn:aws:s3:::[bucket]\" }, { \"Sid\": \"AWSCloudTrailWrite20150319\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"cloudtrail.amazonaws.com\" }, \"Action\": \"s3:PutObject\", \"Resource\": \"arn:aws:s3:::[bucket]/AWSLogs/*\", \"Condition\": { \"StringEquals\": { \"s3:x-amz-acl\": \"bucket-owner-full-control\" } } }, { \"Sid\": \"AWSCloudTrailWrite20150319\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"cloudtrail.amazonaws.com\" }, \"Action\": \"s3:PutObject\", \"Resource\": \"arn:aws:s3:::[bucket]/AWSLogs/[organization id]/*\", \"Condition\": { \"StringEquals\": { \"s3:x-amz-acl\": \"bucket-owner-full-control\" } } } ] } (Optional) Next we will add a lifecycle policy to clean up old logs. Navigate to Management (Optional) Add a lifecycle rule named Delete old logs , press Next (Optional) Add a transition rule for both the current and previous versions to move to Glacier after 32 days. Press Next (Optional) Select the current and previous versions and set them to delete after 365 days","title":"2. Create the bucket for CloudTrail logs"},{"location":"Security/100_Create_a_Data_Bunker/README.html#3-highly-recommended-ensure-cross-account-access-is-read-only","text":"These instructions outline how to modify the cross account access created in step 1 is read-only. As with step 1, this will depend on how your organization's policies. The key is that our security team are not able to modify data in our data bunker. Human access should only be in a break-glass emergency situation. Note: Following these steps will prevent OrganizationAccountAccessRole from making further changes to this account. Ensure other services such as Amazon Guard Duty and AWS Security Hub are configured before proceeding. If further changes are needed you will have to reset the root credentials for the securtity account. Navigate to IAM and select Roles Select the organizations account access role for your orgainzation: Note: the default is OrganizationAccountAccessRole Press Attach Policy and attach the AWS managed ReadOnlyAccess Policy Navigate back to the OrganizationAccountAccessRole and press the X to remove the AdministratorAccess policy","title":"3. (Highly recommended) Ensure cross account access is read-only"},{"location":"Security/100_Create_a_Data_Bunker/README.html#4-turn-on-cloudtrail-from-the-root-account","text":"Switch back to the root account Navigate to CloudTrail Select Trails from the menu on the left Press Create Trail Enter a name for the trail such as OrganizationTrail Select Yes next to Apply trail to my organization Under Storage location , select No for Create new S3 bucket and enter the bucket name of the bucket created in step 2","title":"4. Turn on CloudTrail from the root account"},{"location":"Security/100_Create_a_Data_Bunker/README.html#verification","text":"Switch back to the Security account Navigate to the S3 bucket previously created (Optional) You can start to explore the logs using CloudTrail","title":"Verification"},{"location":"Security/100_Create_a_Data_Bunker/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_Enable_Security_Hub/README.html","text":"Level 100: Enable Security Hub AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings. Goals Enable AWS Security Hub Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/100_Enable_Security_Hub/README.html#level-100-enable-security-hub","text":"AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.","title":"Level 100: Enable Security Hub"},{"location":"Security/100_Enable_Security_Hub/README.html#goals","text":"Enable AWS Security Hub","title":"Goals"},{"location":"Security/100_Enable_Security_Hub/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/100_Enable_Security_Hub/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/100_Enable_Security_Hub/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html","text":"Level 100: Enable AWS Security Hub via AWS Console Authors Pierre Liddle, Principal Security Architect Table of Contents Getting Started 1. Getting Started The AWS console provides a graphical user interface to search and work with the AWS services. We will use the AWS console to enable AWS Security Hub. 1.1 AWS Security Hub Once you have logged into your AWS account you can use the search facility to locate Security Hub. All you need to do is type in Security Hub in the search field. Once Security Hub shows up you can click on Security Hub to go to the Security Hub service. Alternatively you can go directly to the AWS Security Hub Console. AWS Security Hub Console 1.2 Enable AWS Security Hub In the AWS Security Hub service console you can click on the Enable Security Hub orange button to enable AWS Security Hub in your account. AWS Security Hub requires services permissions to run within your account. You can review the service role permissions in the following screen. Remember to click Enable AWS Security Hub 1.3 Explore AWS Security Hub With AWS Security Hub now enabled in your account, you can explore the security insights AWS Security Hub offers. References & useful resources AWS Security Hub License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#level-100-enable-aws-security-hub-via-aws-console","text":"","title":"Level 100: Enable AWS Security Hub via AWS Console"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#authors","text":"Pierre Liddle, Principal Security Architect","title":"Authors"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#table-of-contents","text":"Getting Started","title":"Table of Contents"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#1-getting-started","text":"The AWS console provides a graphical user interface to search and work with the AWS services. We will use the AWS console to enable AWS Security Hub.","title":"1. Getting Started "},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#11-aws-security-hub","text":"Once you have logged into your AWS account you can use the search facility to locate Security Hub. All you need to do is type in Security Hub in the search field. Once Security Hub shows up you can click on Security Hub to go to the Security Hub service. Alternatively you can go directly to the AWS Security Hub Console. AWS Security Hub Console","title":"1.1 AWS Security Hub"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#12-enable-aws-security-hub","text":"In the AWS Security Hub service console you can click on the Enable Security Hub orange button to enable AWS Security Hub in your account. AWS Security Hub requires services permissions to run within your account. You can review the service role permissions in the following screen. Remember to click Enable AWS Security Hub","title":"1.2 Enable AWS Security Hub"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#13-explore-aws-security-hub","text":"With AWS Security Hub now enabled in your account, you can explore the security insights AWS Security Hub offers.","title":"1.3 Explore AWS Security Hub"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#references-useful-resources","text":"AWS Security Hub","title":"References &amp; useful resources"},{"location":"Security/100_Enable_Security_Hub/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html","text":"Level 200: Automated Deployment of Detective Controls Introduction This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Implement detective controls Automate security best practices Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#level-200-automated-deployment-of-detective-controls","text":"","title":"Level 200: Automated Deployment of Detective Controls"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#introduction","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#goals","text":"Implement detective controls Automate security best practices","title":"Goals"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html","text":"Level 200: Automated Deployment of Detective Controls: Lab Guide Authors Ben Potter, Security Lead, Well-Architected Table of Contents Deployment Knowledge Check Tear Down 1. AWS CloudFormation to configure AWS CloudTrail, AWS Config, and Amazon GuardDuty AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise. GuardDuty also detects potentially compromised instances or reconnaissance by attackers. Using AWS CloudFormation , we are going to configure GuardDuty, and configure alerting to your email address. AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. Download the latest version of the cloudtrail-config-guardduty.yaml CloudFormation template from GitHub raw, or by cloning this repository. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following details for each section: General Stack name: The name of this stack. For this lab, use DetectiveControls . CloudTrail: Enable CloudTrail Yes/No. If you already have CloudTrail enabled select No. Config: Enable Config Yes/No. If you already have Config enabled select No. GuardDuty: Enable GuardDuty Yes/No. If you already have GuardDuty enabled select No. Note that GuardDuty will create and leave an IAM role the first time its enabled. S3BucketPolicyExplicitDeny: (Optional) Explicitly deny destructive actions to the bucket. AWS root user will be required to modify this bucket if configured. S3AccessLogsBucketName: (Optional) The name of an existing S3 bucket for storing S3 access logs. CloudTrail CloudTrailBucketName: The name of the new S3 bucket to create for CloudTrail to send logs to. IMPORTANT Specify a bucket name that is unique. CloudTrailCWLogsRetentionTime: Number of days to retain logs in CloudWatch Logs. CloudTrailS3RetentionTime: Number of days to retain logs in the S3 bucket before they are automatically deleted. CloudTrailEncryptS3Logs: (Optional) Use AWS KMS to encrypt logs stored in Amazon S3. A new KMS key will be created. CloudTrailLogS3DataEvents: (Optional) These events provide insight into the resource operations performed on or within S3. Config ConfigBucketName: The name of the new S3 bucket to create for Config to save config snapshots to. IMPORTANT Specify a bucket name that is unique. ConfigSnapshotFrequency: AWS Config configuration snapshot frequency ConfigS3RetentionTime: Number of days to retain logs in the S3 bucket before they are automatically deleted. Guard Duty GuardDutyEmailAddress: The email address you own that will receive the alerts, you must have access to this address for testing. Once you have finished entering the details for the template continue to the bottom of the page and click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up detective controls to log to your buckets and retain events, giving you the ability to search history and later enable pro-active monitoring of your AWS account! You should receive an email to confirm the SNS email subscription, you must confirm this. Note as the email is directly from GuardDuty via SNS is will be JSON format. 2. Knowledge Check The security best practices followed in this lab are: Automate alerting on key indicators AWS Cloudtrail, AWS Config and Amazon GuardDuty provide insights into your environment. Implement new security services and features: New features such as Amazon GuardDuty have been adopted. Automate configuration management: CloudFormation is being used to configure AWS CloudTrail, AWS Config and Amazon GuardDuty. Implement managed services: Managed services are utilized to increase your visibility and control of your environment. 3. Tear down this lab The following instructions will remove the resources that have a cost for running them. Note: If you are planning on doing the lab 300_Incident_Response_with_AWS_Console_and_CLI we recommend you only tear down this stack after completing that lab as their is a dependency on AWS CloudTrail being enabled for the other lab. Delete the stack: Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Select the DetectiveControls stack. Click the Actions button then click Delete Stack. Confirm the stack and then click the Yes, Delete button. Empty and delete the S3 buckets: Sign in to the AWS Management Console, and open the S3 console at https://console.aws.amazon.com/s3/. Select the CloudTrail bucket name you previously created without clicking the name. Click Empty bucket and enter the bucket name in the confirmation box. Click Confirm and the bucket will be emptied when the bottom task bar has 0 operations in progress. With the bucket now empty, click Delete bucket. Enter the bucket name in the confirmation box and click Confirm. Repeat steps 2 to 6 for the Config bucket you created. References & useful resources AWS CloudTrail User Guide AWS CloudFormation User Guide Amazon GuardDuty User Guide AWS Config User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#level-200-automated-deployment-of-detective-controls-lab-guide","text":"","title":"Level 200: Automated Deployment of Detective Controls: Lab Guide"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#table-of-contents","text":"Deployment Knowledge Check Tear Down","title":"Table of Contents"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#1-aws-cloudformation-to-configure-aws-cloudtrail-aws-config-and-amazon-guardduty","text":"AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your AWS infrastructure. CloudTrail provides event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior to help you protect your AWS accounts and workloads. It monitors for activity such as unusual API calls or potentially unauthorized deployments that indicate a possible account compromise. GuardDuty also detects potentially compromised instances or reconnaissance by attackers. Using AWS CloudFormation , we are going to configure GuardDuty, and configure alerting to your email address. AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. AWS Config continuously monitors and records your AWS resource configurations and allows you to automate the evaluation of recorded configurations against desired configurations. Download the latest version of the cloudtrail-config-guardduty.yaml CloudFormation template from GitHub raw, or by cloning this repository. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following details for each section: General Stack name: The name of this stack. For this lab, use DetectiveControls . CloudTrail: Enable CloudTrail Yes/No. If you already have CloudTrail enabled select No. Config: Enable Config Yes/No. If you already have Config enabled select No. GuardDuty: Enable GuardDuty Yes/No. If you already have GuardDuty enabled select No. Note that GuardDuty will create and leave an IAM role the first time its enabled. S3BucketPolicyExplicitDeny: (Optional) Explicitly deny destructive actions to the bucket. AWS root user will be required to modify this bucket if configured. S3AccessLogsBucketName: (Optional) The name of an existing S3 bucket for storing S3 access logs. CloudTrail CloudTrailBucketName: The name of the new S3 bucket to create for CloudTrail to send logs to. IMPORTANT Specify a bucket name that is unique. CloudTrailCWLogsRetentionTime: Number of days to retain logs in CloudWatch Logs. CloudTrailS3RetentionTime: Number of days to retain logs in the S3 bucket before they are automatically deleted. CloudTrailEncryptS3Logs: (Optional) Use AWS KMS to encrypt logs stored in Amazon S3. A new KMS key will be created. CloudTrailLogS3DataEvents: (Optional) These events provide insight into the resource operations performed on or within S3. Config ConfigBucketName: The name of the new S3 bucket to create for Config to save config snapshots to. IMPORTANT Specify a bucket name that is unique. ConfigSnapshotFrequency: AWS Config configuration snapshot frequency ConfigS3RetentionTime: Number of days to retain logs in the S3 bucket before they are automatically deleted. Guard Duty GuardDutyEmailAddress: The email address you own that will receive the alerts, you must have access to this address for testing. Once you have finished entering the details for the template continue to the bottom of the page and click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up detective controls to log to your buckets and retain events, giving you the ability to search history and later enable pro-active monitoring of your AWS account! You should receive an email to confirm the SNS email subscription, you must confirm this. Note as the email is directly from GuardDuty via SNS is will be JSON format.","title":"1. AWS CloudFormation to configure AWS CloudTrail, AWS Config, and Amazon GuardDuty "},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#2-knowledge-check","text":"The security best practices followed in this lab are: Automate alerting on key indicators AWS Cloudtrail, AWS Config and Amazon GuardDuty provide insights into your environment. Implement new security services and features: New features such as Amazon GuardDuty have been adopted. Automate configuration management: CloudFormation is being used to configure AWS CloudTrail, AWS Config and Amazon GuardDuty. Implement managed services: Managed services are utilized to increase your visibility and control of your environment.","title":"2. Knowledge Check "},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#3-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Note: If you are planning on doing the lab 300_Incident_Response_with_AWS_Console_and_CLI we recommend you only tear down this stack after completing that lab as their is a dependency on AWS CloudTrail being enabled for the other lab. Delete the stack: Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Select the DetectiveControls stack. Click the Actions button then click Delete Stack. Confirm the stack and then click the Yes, Delete button. Empty and delete the S3 buckets: Sign in to the AWS Management Console, and open the S3 console at https://console.aws.amazon.com/s3/. Select the CloudTrail bucket name you previously created without clicking the name. Click Empty bucket and enter the bucket name in the confirmation box. Click Confirm and the bucket will be emptied when the bottom task bar has 0 operations in progress. With the bucket now empty, click Delete bucket. Enter the bucket name in the confirmation box and click Confirm. Repeat steps 2 to 6 for the Config bucket you created.","title":"3. Tear down this lab "},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#references-useful-resources","text":"AWS CloudTrail User Guide AWS CloudFormation User Guide Amazon GuardDuty User Guide AWS Config User Guide","title":"References &amp; useful resources"},{"location":"Security/200_Automated_Deployment_of_Detective_Controls/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html","text":"Level 200: Automated Deployment of EC2 Web Application Introduction This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information. Goals EC2 automated deployment Autoscaling and load balancing Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM, Elastic Load Balancing. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. Deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC . Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#level-200-automated-deployment-of-ec2-web-application","text":"","title":"Level 200: Automated Deployment of EC2 Web Application"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#introduction","text":"This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information.","title":"Introduction"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#goals","text":"EC2 automated deployment Autoscaling and load balancing","title":"Goals"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM, Elastic Load Balancing. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. Deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC .","title":"Prerequisites"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html","text":"Level 200: Automated Deployment of EC2 Web Application Authors Ben Potter, Security Lead, Well-Architected Rodney Lester, Reliability Lead, Well-Architected Table of Contents Overview Create Web Stack Knowledge Check Further Considerations Tear Down 1. Overview Overview of wordpress stack architecture: 2. Create Web Stack Please note a prerequisite to this lab is that you have deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC with the default parameters and recommended stack name. This step will create the web application and all components using the example CloudFormation template, inside the VPC you have created previously. An SSH key is not configured in this lab, instead AWS Systems Manager should be used to manage the EC2 instances as a more secure and scalable method. Choose the version of the CloudFormation template and download to your computer, or by cloning this repository: wordpress.yaml to create a WordPress site, including an RDS database. staticwebapp.yaml to create a static web application that simply displays the instance ID for the instance it is running upon. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create Stack , then With new resources (standard) . Click Upload a template file and then click Choose file . Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next . Enter the following details: Stack name: The name of this stack. For this lab, for the WordPress stack use WebApp1-WordPress or for the static web stack use WebApp1-Static and match the case. ALBSGSource: Your current IP address in CIDR notation which will be allowed to connect to the application load balancer, this secures your web application from the public while you are configuring and testing. The remaining parameters may be left as defaults, you can find out more in the description for each. At the bottom of the page click Next . In this lab, we won't add any tags, permissions or advanced options. Click Next . Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a number of minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now created the WordPress stack (well actually CloudFormation did it for you). In the stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created. After you have played and explored with your web application, don't forget to tear it down to save cost. 3. Knowledge Check The security best practices followed in this lab are: Grant access through roles or federation: A role is attached to the auto-scaled instances. Implement dynamic authentication: The role attached to the auto-scaled instances dynamically acquires credentials. Grant least privileges: The role attached to the auto-scaled instances uses minimum privileges to accomplish the task. Implement new security services and features: New features including secrets manager have been adopted. Limit exposure: Security groups restrict network traffic to a minimum. Automate configuration management: CloudFormation is being used to deploy the application automatically. Control traffic at all layers: Traffic is controlled in multiple tiers, using subnets with different route tables. Reduce attack surface: Instances do not allow for SSH, instead Systems Manager may be used for administration. Implement managed services: Managed services are utilized including Secrets Manager, Aurora serverless. Implement secure key management: AWS Key Management Service is used for key management of Aurora database. Provide mechanisms to keep people away from data: SSH to the instances is not allowed, Systems Manager may be used to control access and CloudFormation is used to deploy and update all infrastructure to reduce human error. 4. Further considerations: Enable TLS (SSL) on application load balancer to encrypt communications, using Amazon Certificate Manager. WordPress that is deployed stores the database password in clear text in a configuration file and is not rotated, best practice if supported would be to encrypt and automatically rotate preferably accessing the Secrets Manager API. Encrypting the EC2 AMI for the web instances would automatically enable encrypted volumes. Implementing a Web Application Firewall such as AWS WAF, and a content delivery service such as Amazon CloudFront. Create an automated process for patching the AMI's and scanning for vulnerabilities before updating in production. Create a pipeline that verifies the CloudFormation template for misconfigurations before creating or updating the stack. 5. Tear down this lab The following instructions will remove the resources that you have created in this lab. Delete the WordPress or Static Web Application CloudFormation stack: Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click the radio button on the left of the WebApp1-WordPress or WebApp1-Static stack. Click the Actions button then click Delete stack . Confirm the stack and then click Delete button. Access the Key Management Service (KMS) console https://console.aws.amazon.com/cloudformation/ References & useful resources AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#level-200-automated-deployment-of-ec2-web-application","text":"","title":"Level 200: Automated Deployment of EC2 Web Application"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected Rodney Lester, Reliability Lead, Well-Architected","title":"Authors"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#table-of-contents","text":"Overview Create Web Stack Knowledge Check Further Considerations Tear Down","title":"Table of Contents"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#1-overview","text":"Overview of wordpress stack architecture:","title":"1. Overview "},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#2-create-web-stack","text":"Please note a prerequisite to this lab is that you have deployed the CloudFormation VPC stack in the lab Automated Deployment of VPC with the default parameters and recommended stack name. This step will create the web application and all components using the example CloudFormation template, inside the VPC you have created previously. An SSH key is not configured in this lab, instead AWS Systems Manager should be used to manage the EC2 instances as a more secure and scalable method. Choose the version of the CloudFormation template and download to your computer, or by cloning this repository: wordpress.yaml to create a WordPress site, including an RDS database. staticwebapp.yaml to create a static web application that simply displays the instance ID for the instance it is running upon. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create Stack , then With new resources (standard) . Click Upload a template file and then click Choose file . Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next . Enter the following details: Stack name: The name of this stack. For this lab, for the WordPress stack use WebApp1-WordPress or for the static web stack use WebApp1-Static and match the case. ALBSGSource: Your current IP address in CIDR notation which will be allowed to connect to the application load balancer, this secures your web application from the public while you are configuring and testing. The remaining parameters may be left as defaults, you can find out more in the description for each. At the bottom of the page click Next . In this lab, we won't add any tags, permissions or advanced options. Click Next . Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a number of minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now created the WordPress stack (well actually CloudFormation did it for you). In the stack click the Outputs tab, and open the WebsiteURL value in your web browser, this is how to access what you just created. After you have played and explored with your web application, don't forget to tear it down to save cost.","title":"2. Create Web Stack "},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#3-knowledge-check","text":"The security best practices followed in this lab are: Grant access through roles or federation: A role is attached to the auto-scaled instances. Implement dynamic authentication: The role attached to the auto-scaled instances dynamically acquires credentials. Grant least privileges: The role attached to the auto-scaled instances uses minimum privileges to accomplish the task. Implement new security services and features: New features including secrets manager have been adopted. Limit exposure: Security groups restrict network traffic to a minimum. Automate configuration management: CloudFormation is being used to deploy the application automatically. Control traffic at all layers: Traffic is controlled in multiple tiers, using subnets with different route tables. Reduce attack surface: Instances do not allow for SSH, instead Systems Manager may be used for administration. Implement managed services: Managed services are utilized including Secrets Manager, Aurora serverless. Implement secure key management: AWS Key Management Service is used for key management of Aurora database. Provide mechanisms to keep people away from data: SSH to the instances is not allowed, Systems Manager may be used to control access and CloudFormation is used to deploy and update all infrastructure to reduce human error.","title":"3. Knowledge Check "},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#4-further-considerations","text":"Enable TLS (SSL) on application load balancer to encrypt communications, using Amazon Certificate Manager. WordPress that is deployed stores the database password in clear text in a configuration file and is not rotated, best practice if supported would be to encrypt and automatically rotate preferably accessing the Secrets Manager API. Encrypting the EC2 AMI for the web instances would automatically enable encrypted volumes. Implementing a Web Application Firewall such as AWS WAF, and a content delivery service such as Amazon CloudFront. Create an automated process for patching the AMI's and scanning for vulnerabilities before updating in production. Create a pipeline that verifies the CloudFormation template for misconfigurations before creating or updating the stack.","title":"4. Further considerations: "},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#5-tear-down-this-lab","text":"The following instructions will remove the resources that you have created in this lab. Delete the WordPress or Static Web Application CloudFormation stack: Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click the radio button on the left of the WebApp1-WordPress or WebApp1-Static stack. Click the Actions button then click Delete stack . Confirm the stack and then click Delete button. Access the Key Management Service (KMS) console https://console.aws.amazon.com/cloudformation/","title":"5. Tear down this lab "},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#references-useful-resources","text":"AWS CloudFormation User Guide Amazon EC2 User Guide for Linux Instances","title":"References &amp; useful resources"},{"location":"Security/200_Automated_Deployment_of_EC2_Web_Application/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html","text":"Level 200: Automated Deployment of IAM Groups and Roles Introduction This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure AWS Identity and Access Management (IAM) Groups and roles for cross-account access. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of a new or existing AWS account with IAM best practices. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Fine-grained authorization Automate security best practices Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#level-200-automated-deployment-of-iam-groups-and-roles","text":"","title":"Level 200: Automated Deployment of IAM Groups and Roles"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#introduction","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure AWS Identity and Access Management (IAM) Groups and roles for cross-account access. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of a new or existing AWS account with IAM best practices. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#goals","text":"Fine-grained authorization Automate security best practices","title":"Goals"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html","text":"Level 200: Automated Deployment of IAM Groups and Roles: Lab Guide 1. AWS CloudFormation to Create a Groups, Policies and Roles with MFA Enforced Using AWS CloudFormation we are going to deploy a set of groups, roles, and managed policies that will help with your security \"baseline\" of your AWS account. 1.1 Create AWS CloudFormation Stack Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/baseline-iam.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use baseline-iam . AllowRegion: A single region to restrict access, enter your preferred region. BaselineExportName: The CloudFormation export name prefix used with the resource name for the resources created, for example, Baseline-PrivilegedAdminRole. BaselineNamePrefix: The prefix for roles, groups, and policies created by this stack. IdentityManagementAccount: (optional) AccountId that contains centralized IAM users and is trusted to assume all roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. OrganizationsRootAccount: (optional) AccountId that is trusted to assume Organizations role, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. ToolingManagementAccount: AccountId that is trusted to assume the ReadOnly and StackSet roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a number of managed polices, groups, and roles that you can test to improve your AWS security! 2. Assume Roles from an IAM user We will assume the roles previously created in the web console and command line interface (CLI) using an existing IAM user. 2.1 Use Restricted Administrator Role in Web Console A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don't sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour. Important The permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored. Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias . Alternatively you can paste the link in your browser that you recorded earlier. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. On the Switch Role page, type the account ID number or the account alias and the name of the role that you created for the Administrator in the previous step, for example, arn:aws:iam::account_ID:role/Administrator . (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply choose the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 7. You are now using the role with the granted permissions! To stop using a role In the IAM console, choose your role's Display Name on the right side of the navigation bar. Choose Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored. 2.2 Use Restricted Administrator Role in Command Line Interface (CLI) Coming soon, for now check out: https://docs.aws.amazon.com/cli/latest/userguide/cli-roles.html 3. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that the changes you made to the root login, users, groups, and policies have no charges associated with them. Delete the IAM stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the baseline-iam stack. 3. Click the Actions button then click Delete Stack. 4. Confirm the stack and then click the Yes, Delete button. References & useful resources AWS Identity and Access Management User Guide IAM Best Practices and Use Cases AWS CloudFormation User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#level-200-automated-deployment-of-iam-groups-and-roles-lab-guide","text":"","title":"Level 200: Automated Deployment of IAM Groups and Roles: Lab Guide"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#1-aws-cloudformation-to-create-a-groups-policies-and-roles-with-mfa-enforced","text":"Using AWS CloudFormation we are going to deploy a set of groups, roles, and managed policies that will help with your security \"baseline\" of your AWS account.","title":"1. AWS CloudFormation to Create a Groups, Policies and Roles with MFA Enforced"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#11-create-aws-cloudformation-stack","text":"Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/baseline-iam.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use baseline-iam . AllowRegion: A single region to restrict access, enter your preferred region. BaselineExportName: The CloudFormation export name prefix used with the resource name for the resources created, for example, Baseline-PrivilegedAdminRole. BaselineNamePrefix: The prefix for roles, groups, and policies created by this stack. IdentityManagementAccount: (optional) AccountId that contains centralized IAM users and is trusted to assume all roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. OrganizationsRootAccount: (optional) AccountId that is trusted to assume Organizations role, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. ToolingManagementAccount: AccountId that is trusted to assume the ReadOnly and StackSet roles, or blank for no cross-account trust. Note that the trusted account needs to be appropriately secured. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a number of managed polices, groups, and roles that you can test to improve your AWS security!","title":"1.1 Create AWS CloudFormation Stack"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#2-assume-roles-from-an-iam-user","text":"We will assume the roles previously created in the web console and command line interface (CLI) using an existing IAM user.","title":"2. Assume Roles from an IAM user"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#21-use-restricted-administrator-role-in-web-console","text":"A role specifies a set of permissions that you can use to access AWS resources that you need. In that sense, it is similar to a user in AWS Identity and Access Management (IAM). A benefit of roles is they allow you to enforce the use of an MFA token to help protect your credentials. When you sign in as a user, you get a specific set of permissions. However, you don't sign in to a role, but once signed in (as a user) you can switch to a role. This temporarily sets aside your original user permissions and instead gives you the permissions assigned to the role. The role can be in your own account or any other AWS account. By default, your AWS Management Console session lasts for one hour. Important The permissions of your IAM user and any roles that you switch to are not cumulative. Only one set of permissions is active at a time. When you switch to a role, you temporarily give up your user permissions and work with the permissions that are assigned to the role. When you exit the role, your user permissions are automatically restored. Sign in to the AWS Management Console as an IAM user https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias . Alternatively you can paste the link in your browser that you recorded earlier. Click Switch Role. If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. On the Switch Role page, type the account ID number or the account alias and the name of the role that you created for the Administrator in the previous step, for example, arn:aws:iam::account_ID:role/Administrator . (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. The name and color can help remind you when this role is active, which changes your permissions. For example, for a role that gives you access to the test environment, you might specify a Display Name of Test and select the green Color. For the role that gives you access to production, you might specify a Display Name of Production and select red as the Color. Click Switch Role. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply choose the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 7. You are now using the role with the granted permissions! To stop using a role In the IAM console, choose your role's Display Name on the right side of the navigation bar. Choose Back to UserName. The role and its permissions are deactivated, and the permissions associated with your IAM user and groups are automatically restored.","title":"2.1 Use Restricted Administrator Role in Web Console"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#22-use-restricted-administrator-role-in-command-line-interface-cli","text":"Coming soon, for now check out: https://docs.aws.amazon.com/cli/latest/userguide/cli-roles.html","title":"2.2 Use Restricted Administrator Role in Command Line Interface (CLI)"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#3-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that the changes you made to the root login, users, groups, and policies have no charges associated with them. Delete the IAM stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the baseline-iam stack. 3. Click the Actions button then click Delete Stack. 4. Confirm the stack and then click the Yes, Delete button.","title":"3. Tear down this lab"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#references-useful-resources","text":"AWS Identity and Access Management User Guide IAM Best Practices and Use Cases AWS CloudFormation User Guide","title":"References &amp; useful resources"},{"location":"Security/200_Automated_Deployment_of_IAM_Groups_and_Roles/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html","text":"Level 200: Automated Deployment of VPC Introduction This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: * Application Load Balancer - named ALB1 * Application instances - named App1 * Shared services - named Shared1 * Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Goals VPC security features VPC layered subnet architecture Automated deployments Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. We recommend you clone the Git repository for easy access to the AWS CloudFormation templates. Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#level-200-automated-deployment-of-vpc","text":"","title":"Level 200: Automated Deployment of VPC"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#introduction","text":"This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: * Application Load Balancer - named ALB1 * Application instances - named App1 * Shared services - named Shared1 * Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs.","title":"Introduction"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#goals","text":"VPC security features VPC layered subnet architecture Automated deployments","title":"Goals"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account with full access to CloudFormation, EC2, VPC, IAM. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic understanding of AWS CloudFormation , visit the Getting Started section of the user guide. We recommend you clone the Git repository for easy access to the AWS CloudFormation templates.","title":"Prerequisites"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Automated_Deployment_of_VPC/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html","text":"Level 200: Automated Deployment of VPC Authors Ben Potter, Security Lead, Well-Architected Table of Contents Overview Create VPC Stack Knowledge Check Tear Down 1. Overview 2. Create VPC Stack This step will create the VPC and all components using the example CloudFormation template. Download the latest version of the vpc-alb-app-db.yaml CloudFormation template from file from GitHub raw, or by cloning this repository. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create Stack , then With new resources (standard) . Click Upload a template file and then click Choose file . Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use WebApp1-VPC and match the case. The parameters may be left as defaults, you can find out more in the description for each. If you change the default name take note as you will need to use it for other labs including \"Automated Deployment of EC2 Web Application\". At the bottom of the page click Next . In this lab, we won't add any tags, permissions or advanced options. Click Next . Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now created the VPC stack (well actually CloudFormation did it for you). Now you have a new VPC, check out 200_Automated_Deployment_of_EC2_Web_Application to deploy a web application inside it. 3. Knowledge Check The security best practices followed in this lab are: Grant least privileges: The roles are scoped with minimum privileges to accomplish the task. Implement new security services and features: New features including secrets manager have been adopted. Limit exposure: Security groups restrict network traffic to a minimum. Use of Internet Gateways and NAT Gateways in use to control traffic flows. Automate configuration management: CloudFormation is being used to deploy the networking constructs. Control traffic at all layers: Traffic is controlled in multiple tiers, using subnets with different route tables. 4. Tear down this lab The following instructions will remove the resources that you have created in this lab. Note: If you are planning on completing the lab 200_Automated_Deployment_of_EC2_Web_Application we recommend you only tear down this lab after completing both, as there is a dependency on this VPC. Delete the VPC CloudFormation stack: Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click the radio button on the left of the WebApp1-VPC stack. Click the Actions button then click Delete stack . Confirm the stack and then click Delete button. Delete the CloudWatch Logs: Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudwatch/ . Click Logs in the left navigation. Click the radio button on the left of the WebApp1-VPC-VPCFlowLogGroup-\\<some unique ID> . Click the Actions Button then click Delete Log Group . Verify the log group name then click Yes, Delete . References & useful resources AWS CloudFormation User Guide Amazon VPC User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#level-200-automated-deployment-of-vpc","text":"","title":"Level 200: Automated Deployment of VPC"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#table-of-contents","text":"Overview Create VPC Stack Knowledge Check Tear Down","title":"Table of Contents"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#1-overview","text":"","title":"1. Overview "},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#2-create-vpc-stack","text":"This step will create the VPC and all components using the example CloudFormation template. Download the latest version of the vpc-alb-app-db.yaml CloudFormation template from file from GitHub raw, or by cloning this repository. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create Stack , then With new resources (standard) . Click Upload a template file and then click Choose file . Choose the CloudFormation template you downloaded in step 1, return to the CloudFormation console page and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use WebApp1-VPC and match the case. The parameters may be left as defaults, you can find out more in the description for each. If you change the default name take note as you will need to use it for other labs including \"Automated Deployment of EC2 Web Application\". At the bottom of the page click Next . In this lab, we won't add any tags, permissions or advanced options. Click Next . Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, check I acknowledge that AWS CloudFormation might create IAM resources with custom names then click Create stack . After a few minutes the final stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now created the VPC stack (well actually CloudFormation did it for you). Now you have a new VPC, check out 200_Automated_Deployment_of_EC2_Web_Application to deploy a web application inside it.","title":"2. Create VPC Stack "},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#3-knowledge-check","text":"The security best practices followed in this lab are: Grant least privileges: The roles are scoped with minimum privileges to accomplish the task. Implement new security services and features: New features including secrets manager have been adopted. Limit exposure: Security groups restrict network traffic to a minimum. Use of Internet Gateways and NAT Gateways in use to control traffic flows. Automate configuration management: CloudFormation is being used to deploy the networking constructs. Control traffic at all layers: Traffic is controlled in multiple tiers, using subnets with different route tables.","title":"3. Knowledge Check "},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#4-tear-down-this-lab","text":"The following instructions will remove the resources that you have created in this lab. Note: If you are planning on completing the lab 200_Automated_Deployment_of_EC2_Web_Application we recommend you only tear down this lab after completing both, as there is a dependency on this VPC. Delete the VPC CloudFormation stack: Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/ . Click the radio button on the left of the WebApp1-VPC stack. Click the Actions button then click Delete stack . Confirm the stack and then click Delete button. Delete the CloudWatch Logs: Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudwatch/ . Click Logs in the left navigation. Click the radio button on the left of the WebApp1-VPC-VPCFlowLogGroup-\\<some unique ID> . Click the Actions Button then click Delete Log Group . Verify the log group name then click Yes, Delete .","title":"4. Tear down this lab "},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#references-useful-resources","text":"AWS CloudFormation User Guide Amazon VPC User Guide","title":"References &amp; useful resources"},{"location":"Security/200_Automated_Deployment_of_VPC/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019-2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html","text":"Level 200: Automated Deployment of Web Application Firewall Introduction This hands-on lab will guide you through the steps to protect a workload from network based attacks using AWS Web Application Firewall (WAF) integrated with Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy AWS Web Application Firewall (WAF) with CloudFront integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#level-200-automated-deployment-of-web-application-firewall","text":"","title":"Level 200: Automated Deployment of Web Application Firewall"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#introduction","text":"This hands-on lab will guide you through the steps to protect a workload from network based attacks using AWS Web Application Firewall (WAF) integrated with Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy AWS Web Application Firewall (WAF) with CloudFront integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#goals","text":"Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection","title":"Goals"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab.","title":"Prerequisites"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html","text":"Level 200: Automated Deployment of Web Application Firewall: Lab Guide Authors Ben Potter, Security Lead, Well-Architected Table of Contents Configure WAF Configure CloudFront for WAF Tear Down 1. Configure AWS WAF Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use waf . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1 . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1 . The remainder of the parameters can be left as defaults. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a basic AWS WAF configuration ready for CloudFront to use! 2. Configure Amazon CloudFront Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, choose Create Distribution. Click Get Started in the Web section. Specify the following settings for the distribution: In Origin Domain Name enter the DNS or domain name from your elastic load balancer or EC2 instance. In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously. Click Create Distrubution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. You have now configured Amazon CloudFront with basic settings and AWS WAF. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation. 3. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. Delete the AWS WAF stack: Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Select the waf-cloudfront stack. Click the Actions button, and then click Delete Stack. Confirm the stack, and then click the Yes, Delete button. References & useful resources Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#level-200-automated-deployment-of-web-application-firewall-lab-guide","text":"","title":"Level 200: Automated Deployment of Web Application Firewall: Lab Guide"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#table-of-contents","text":"Configure WAF Configure CloudFront for WAF Tear Down","title":"Table of Contents"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#1-configure-aws-waf","text":"Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use waf . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1 . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1 . The remainder of the parameters can be left as defaults. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a basic AWS WAF configuration ready for CloudFront to use!","title":"1. Configure AWS WAF "},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#2-configure-amazon-cloudfront","text":"Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, choose Create Distribution. Click Get Started in the Web section. Specify the following settings for the distribution: In Origin Domain Name enter the DNS or domain name from your elastic load balancer or EC2 instance. In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously. Click Create Distrubution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. You have now configured Amazon CloudFront with basic settings and AWS WAF. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.","title":"2. Configure Amazon CloudFront "},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#3-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. Delete the AWS WAF stack: Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Select the waf-cloudfront stack. Click the Actions button, and then click Delete Stack. Confirm the stack, and then click the Yes, Delete button.","title":"3. Tear down this lab "},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#references-useful-resources","text":"Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide","title":"References &amp; useful resources"},{"location":"Security/200_Automated_Deployment_of_Web_Application_Firewall/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_IAM_User_Cleanup/README.html","text":"Level 200: Automated IAM User Cleanup Introduction This hands-on lab will guide you through the steps to deploy a AWS Lambda function with AWS Serverless Application Model (SAM) to provide regular insights on IAM User/s and AWS Access Key usage within your account. You will use the AWS SAM CLI to package your deployment. Skills learned will help you secure your AWS account in alignment with the AWS Well-Architected Framework . Goals Identify orphaned IAM Users and AWS Access Keys Take action to automatically remove IAM Users and AWS Access Keys no longer needed Reduce identity sprawl Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Select region with support for AWS Lambda from the list: AWS Regions and Endpoints . AWS Serverless Application Model (SAM) installed and configured. The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax, enabling you to build serverless applications faster. Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/200_Automated_IAM_User_Cleanup/README.html#level-200-automated-iam-user-cleanup","text":"","title":"Level 200: Automated IAM User Cleanup"},{"location":"Security/200_Automated_IAM_User_Cleanup/README.html#introduction","text":"This hands-on lab will guide you through the steps to deploy a AWS Lambda function with AWS Serverless Application Model (SAM) to provide regular insights on IAM User/s and AWS Access Key usage within your account. You will use the AWS SAM CLI to package your deployment. Skills learned will help you secure your AWS account in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_Automated_IAM_User_Cleanup/README.html#goals","text":"Identify orphaned IAM Users and AWS Access Keys Take action to automatically remove IAM Users and AWS Access Keys no longer needed Reduce identity sprawl","title":"Goals"},{"location":"Security/200_Automated_IAM_User_Cleanup/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Select region with support for AWS Lambda from the list: AWS Regions and Endpoints . AWS Serverless Application Model (SAM) installed and configured. The AWS Serverless Application Model (SAM) is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML. During deployment, SAM transforms and expands the SAM syntax into AWS CloudFormation syntax, enabling you to build serverless applications faster.","title":"Prerequisites"},{"location":"Security/200_Automated_IAM_User_Cleanup/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Automated_IAM_User_Cleanup/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Automated_IAM_User_Cleanup/Lab_Guide.html","text":"Level 200: Automated IAM User Cleanup: Lab Guide Authors Pierre Liddle, Principal Security Architect Byron Pogson, Solutions Architect Table of Contents Architecture Overview Deploying IAM Lambda Cleanup with AWS SAM 1. Architecture Overview The AWS Lambda function is triggered by a regular scheduled event in Amazon CloudWatch Events. Once the Lambda function runs to check the status of the AWS IAM Users and associated IAM Access Keys the results are sent the designated email contact via Amazon SNS. A check is also performed for unused roles. The logs from the AWS Lambda function are captured in Amazon CloudWatch Logs for review and trouble shooting purposes. 2. Deploying IAM Lambda Cleanup with AWS SAM Download the latest version of the templates from the GitHub code folder as raw objects, or by cloning this repository. Create an Amazon S3 bucket if you don't already have one, it needs to be in the same AWS region being deployed into. Now that you have the S3 bucket created and the files downloaded to your machine. You can start to create your deployment package on the command line with AWS SAM. Make sure you are working in the folder where where you have downloaded the files to. Run the following command to prepare your deployment package: Once you have finished preparing the package you can deploy the CloudFormation with AWS SAM: NOTE: The template file to use here is the output file from the previous command: aws cloudformation deploy --template-file output-template.yaml --stack-name IAM-User-Cleanup --capabilities CAPABILITY_IAM --parameter-overrides NotificationEmail=<replace_with_your_email_address> Once you have completed the deployment of your AWS Lambda function, test the function by going to the AWS Lambda function in your AWS account and create a dummy event by selecting test. If your test runs successfully you should receive an email from: AWS Notifications no-reply@sns.amazonaws.com with the subject line of: IAM user cleanup from and the body of the email will have a status report from the findings. E.g. IAM Users and AWS Access Keys which require a cleanup IAM user cleanup successfully ran. User John Doe has not logged in since 2018-04-19 08:36:18+00:00 and needs cleanup User John Doe has not used access key AKIAIOSFODNN7EXAMPLE in since 2018-04-22 21:32: 00+00:00 and needs cleanup User John Doe has not used access key AKIAIOSFODNN7EXAMPLE in since 2018-04-22 20:08:00+00:00 and needs cleanup References & useful resources AWS Identity and Access Management User Guide IAM Best Practices and Use Cases AWS SAM CLI AWS Serverless Application Model (SAM) License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/200_Automated_IAM_User_Cleanup/Lab_Guide.html#level-200-automated-iam-user-cleanup-lab-guide","text":"","title":"Level 200: Automated IAM User Cleanup: Lab Guide"},{"location":"Security/200_Automated_IAM_User_Cleanup/Lab_Guide.html#authors","text":"Pierre Liddle, Principal Security Architect Byron Pogson, Solutions Architect","title":"Authors"},{"location":"Security/200_Automated_IAM_User_Cleanup/Lab_Guide.html#table-of-contents","text":"Architecture Overview Deploying IAM Lambda Cleanup with AWS SAM","title":"Table of Contents"},{"location":"Security/200_Automated_IAM_User_Cleanup/Lab_Guide.html#1-architecture-overview","text":"The AWS Lambda function is triggered by a regular scheduled event in Amazon CloudWatch Events. Once the Lambda function runs to check the status of the AWS IAM Users and associated IAM Access Keys the results are sent the designated email contact via Amazon SNS. A check is also performed for unused roles. The logs from the AWS Lambda function are captured in Amazon CloudWatch Logs for review and trouble shooting purposes.","title":"1. Architecture Overview "},{"location":"Security/200_Automated_IAM_User_Cleanup/Lab_Guide.html#2-deploying-iam-lambda-cleanup-with-aws-sam","text":"Download the latest version of the templates from the GitHub code folder as raw objects, or by cloning this repository. Create an Amazon S3 bucket if you don't already have one, it needs to be in the same AWS region being deployed into. Now that you have the S3 bucket created and the files downloaded to your machine. You can start to create your deployment package on the command line with AWS SAM. Make sure you are working in the folder where where you have downloaded the files to. Run the following command to prepare your deployment package: Once you have finished preparing the package you can deploy the CloudFormation with AWS SAM: NOTE: The template file to use here is the output file from the previous command: aws cloudformation deploy --template-file output-template.yaml --stack-name IAM-User-Cleanup --capabilities CAPABILITY_IAM --parameter-overrides NotificationEmail=<replace_with_your_email_address> Once you have completed the deployment of your AWS Lambda function, test the function by going to the AWS Lambda function in your AWS account and create a dummy event by selecting test. If your test runs successfully you should receive an email from: AWS Notifications no-reply@sns.amazonaws.com with the subject line of: IAM user cleanup from and the body of the email will have a status report from the findings. E.g. IAM Users and AWS Access Keys which require a cleanup IAM user cleanup successfully ran. User John Doe has not logged in since 2018-04-19 08:36:18+00:00 and needs cleanup User John Doe has not used access key AKIAIOSFODNN7EXAMPLE in since 2018-04-22 21:32: 00+00:00 and needs cleanup User John Doe has not used access key AKIAIOSFODNN7EXAMPLE in since 2018-04-22 20:08:00+00:00 and needs cleanup","title":"2. Deploying IAM Lambda Cleanup with AWS SAM "},{"location":"Security/200_Automated_IAM_User_Cleanup/Lab_Guide.html#references-useful-resources","text":"AWS Identity and Access Management User Guide IAM Best Practices and Use Cases AWS SAM CLI AWS Serverless Application Model (SAM)","title":"References &amp; useful resources"},{"location":"Security/200_Automated_IAM_User_Cleanup/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html","text":"Level 200: EC2 Web Infrastructure Protection Introduction This hands-on lab will guide you through the introductory steps to protect an Amazon EC2 workload from network based attacks. You will use the AWS Management Console and AWS CloudFormation to guide you through how to secure an Amazon EC2 based web application with defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Select region with support for AWS WAF for Application Load Balancers from list: AWS Regions and Endpoints . Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#level-200-ec2-web-infrastructure-protection","text":"","title":"Level 200: EC2 Web Infrastructure Protection"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#introduction","text":"This hands-on lab will guide you through the introductory steps to protect an Amazon EC2 workload from network based attacks. You will use the AWS Management Console and AWS CloudFormation to guide you through how to secure an Amazon EC2 based web application with defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#goals","text":"Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection","title":"Goals"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Select region with support for AWS WAF for Application Load Balancers from list: AWS Regions and Endpoints .","title":"Prerequisites"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html","text":"Level 200: EC2 Web Infrastructure Protection: Lab Guide 1. Launch Instance For launching your first instance, we are going to use the launch wizard in the Amazon EC2 console. 1.1 Launch Single Linux Instance You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn't cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Launch Instance. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI (not Amazon Linux 2). On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. On the Configure Instance Details page, make the following changes: 5.1 Select Create new IAM role. 5.2 In the new tab that opens, select Create role. 5.3 With AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions. 5.4 Enter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account. 5.5 Enter a role name, such as ec2-s3-read-only-role , and then click Create role. 5.6 Back on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created. 5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched: ``` #!/bin/bash yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + ``` Accept defaults and Choose Next: Add tags. Click Next: Configure Security Group. 7.1 On type SSH, select Source as My IP 7.2 Click Add Rule, select Type as HTTP and source as Anywhere Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed. However, for simplicity in this lab, we are opening the access to anywhere. Later modules will secure access with Elastic Load Balancer. 7.3 Select Add Rule to add both SSH and HTTP, and on source, select My IP . ![Security Group](Images/ec2-launch-wizard-security-group.png) 7.4 Click Review and Launch. ![ec2-launch-wizard](Images/ec2-launch-wizard-launch.png) On the Review Instance Launch page, check the details, and then click Launch. If you do not have an existing key pair for access instances, a prompt will appear. Click Create New, then type a name such as lab , click Download Key Pair, and then click Launch Instances. **Important** This is the only chance to save the private key file. You'll need to provide the name of your key pair when you launch an instance, and you'll provide the corresponding private key each time you connect to the instance. Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value. 2. Create AWS WAF Rules 2.1 AWS CloudFormation to create AWS WAF ACL for Application Load Balancer Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with Application Load Balancer. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create New Stack. Select Specify an Amazon S3 template URL and enter the following URL for the template: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-regional.yaml and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use lab-waf-regional . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use WAFLabReg . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use WAFLabReg . The remainder of the parameters can be left as defaults. Click Next. In this scenario, we won't add any tags or other options. Click Next. Review the information for the stack. When you're satisfied with the settings, click Create. After a few minutes, the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a basic AWS WAF configuration ready for Application Load Balancer to use! 3. Create Application Load Balancer with WAF integration Using the AWS Management Console, we will create an Application Load Balancer, link it to the AWS WAF ACL we previously created and test. 3.1 Create Application Load Balancer Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Load Balancers from the Load Balancing section. Click Create Load Balancer. Click Create under the Application Load Balancer section. Enter Name for Application Load Balancer such as lab-alb . Select all availability zones in your region then click Next. You will need to click Next again to accept your load balancer is using insecure listener. Click Create a new security group and enter name and description such as lab-alb and accept default of open to internet. Accept defaults and enter Name such as lab-alb and click Next. From the list of instances click the check box and then Add to registered button. Then click Next. Review the details and click Create. A successful message should appear, click Close. Take not of the DNS name under the Description tab, you will need this for testing. 3.2 Configure Application Load Balancer with WAF Open the AWS WAF console at https://console.aws.amazon.com/waf/. In the navigation pane, choose Web ACLs. Choose the web ACL that you want to associate with the Application Load Balancer. On the Rules tab, under AWS resources using this web ACL, choose Add association. When prompted, use the Resource list to choose the Application Load Balancer that you want to associate this web ACL such as lab-alb and click Add. The Application Load Balancer should now appear under resources using. You can now test access by entering the DNS name of your load balancer in a web browser. 4. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Terminate the instance: Sign in to the AWS Management Console, and open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the left console instance menu, select Instances. Select the instance you created to terminate. From the Actions button (or right click) select Instance State > Terminate. Verify this is the instance you want terminated, then click the Yes, Terminate button. Delete the Application Load Balancer: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Load Balancers from the Load Balancing section. Choose the load balancer you created previously such as lab-alb and click Actions, then Delete. Confirm by clicking Yes, Delete. From the console dashboard, choose Target Groups from the Load Balancing section. Choose the target group you created previously such as lab-alb and click Actions, then Delete. Delete the AWS WAF stack: Open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Select the lab-waf-regional stack. Click the Actions button, and then click Delete Stack. Confirm the stack, and then click the Yes, Delete button. References & useful resources Amazon Elastic Compute Cloud User Guide for Linux Instances Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#level-200-ec2-web-infrastructure-protection-lab-guide","text":"","title":"Level 200: EC2 Web Infrastructure Protection: Lab Guide"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#1-launch-instance","text":"For launching your first instance, we are going to use the launch wizard in the Amazon EC2 console.","title":"1. Launch Instance"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#11-launch-single-linux-instance","text":"You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn't cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Launch Instance. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI (not Amazon Linux 2). On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. On the Configure Instance Details page, make the following changes: 5.1 Select Create new IAM role. 5.2 In the new tab that opens, select Create role. 5.3 With AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions. 5.4 Enter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account. 5.5 Enter a role name, such as ec2-s3-read-only-role , and then click Create role. 5.6 Back on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created. 5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched: ``` #!/bin/bash yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + ``` Accept defaults and Choose Next: Add tags. Click Next: Configure Security Group. 7.1 On type SSH, select Source as My IP 7.2 Click Add Rule, select Type as HTTP and source as Anywhere Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed. However, for simplicity in this lab, we are opening the access to anywhere. Later modules will secure access with Elastic Load Balancer. 7.3 Select Add Rule to add both SSH and HTTP, and on source, select My IP . ![Security Group](Images/ec2-launch-wizard-security-group.png) 7.4 Click Review and Launch. ![ec2-launch-wizard](Images/ec2-launch-wizard-launch.png) On the Review Instance Launch page, check the details, and then click Launch. If you do not have an existing key pair for access instances, a prompt will appear. Click Create New, then type a name such as lab , click Download Key Pair, and then click Launch Instances. **Important** This is the only chance to save the private key file. You'll need to provide the name of your key pair when you launch an instance, and you'll provide the corresponding private key each time you connect to the instance. Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value.","title":"1.1 Launch Single Linux Instance"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#2-create-aws-waf-rules","text":"","title":"2. Create AWS WAF Rules"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#21-aws-cloudformation-to-create-aws-waf-acl-for-application-load-balancer","text":"Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with Application Load Balancer. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create New Stack. Select Specify an Amazon S3 template URL and enter the following URL for the template: https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-regional.yaml and click Next. Enter the following details: Stack name: The name of this stack. For this lab, use lab-waf-regional . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use WAFLabReg . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use WAFLabReg . The remainder of the parameters can be left as defaults. Click Next. In this scenario, we won't add any tags or other options. Click Next. Review the information for the stack. When you're satisfied with the settings, click Create. After a few minutes, the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE. You have now set up a basic AWS WAF configuration ready for Application Load Balancer to use!","title":"2.1 AWS CloudFormation to create AWS WAF ACL for Application Load Balancer"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#3-create-application-load-balancer-with-waf-integration","text":"Using the AWS Management Console, we will create an Application Load Balancer, link it to the AWS WAF ACL we previously created and test.","title":"3. Create Application Load Balancer with WAF integration"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#31-create-application-load-balancer","text":"Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Load Balancers from the Load Balancing section. Click Create Load Balancer. Click Create under the Application Load Balancer section. Enter Name for Application Load Balancer such as lab-alb . Select all availability zones in your region then click Next. You will need to click Next again to accept your load balancer is using insecure listener. Click Create a new security group and enter name and description such as lab-alb and accept default of open to internet. Accept defaults and enter Name such as lab-alb and click Next. From the list of instances click the check box and then Add to registered button. Then click Next. Review the details and click Create. A successful message should appear, click Close. Take not of the DNS name under the Description tab, you will need this for testing.","title":"3.1 Create Application Load Balancer"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#32-configure-application-load-balancer-with-waf","text":"Open the AWS WAF console at https://console.aws.amazon.com/waf/. In the navigation pane, choose Web ACLs. Choose the web ACL that you want to associate with the Application Load Balancer. On the Rules tab, under AWS resources using this web ACL, choose Add association. When prompted, use the Resource list to choose the Application Load Balancer that you want to associate this web ACL such as lab-alb and click Add. The Application Load Balancer should now appear under resources using. You can now test access by entering the DNS name of your load balancer in a web browser.","title":"3.2 Configure Application Load Balancer with WAF"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#4-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Terminate the instance: Sign in to the AWS Management Console, and open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the left console instance menu, select Instances. Select the instance you created to terminate. From the Actions button (or right click) select Instance State > Terminate. Verify this is the instance you want terminated, then click the Yes, Terminate button. Delete the Application Load Balancer: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Load Balancers from the Load Balancing section. Choose the load balancer you created previously such as lab-alb and click Actions, then Delete. Confirm by clicking Yes, Delete. From the console dashboard, choose Target Groups from the Load Balancing section. Choose the target group you created previously such as lab-alb and click Actions, then Delete. Delete the AWS WAF stack: Open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Select the lab-waf-regional stack. Click the Actions button, and then click Delete Stack. Confirm the stack, and then click the Yes, Delete button.","title":"4. Tear down this lab"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#references-useful-resources","text":"Amazon Elastic Compute Cloud User Guide for Linux Instances Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide","title":"References &amp; useful resources"},{"location":"Security/200_Basic_EC2_with_WAF_Protection/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/README.html","text":"Level 200: AWS Certificate Manager Request Public Certificate AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the Internet as well as resources on private networks. AWS Certificate Manager removes the time-consuming manual process of purchasing, uploading, and renewing SSL/TLS certificates. Goals Request AWS Certificate Manager public certificate Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . See pricing for further information on AWS Certificate Manager. Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: AWS Certificate Manager Request Public Certificate"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/README.html#level-200-aws-certificate-manager-request-public-certificate","text":"AWS Certificate Manager is a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the Internet as well as resources on private networks. AWS Certificate Manager removes the time-consuming manual process of purchasing, uploading, and renewing SSL/TLS certificates.","title":"Level 200: AWS Certificate Manager Request Public Certificate"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/README.html#goals","text":"Request AWS Certificate Manager public certificate","title":"Goals"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . See pricing for further information on AWS Certificate Manager.","title":"Prerequisites"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/Lab_Guide.html","text":"Level 200: AWS Certificate Manager Request Public Certificate Authors Ben Potter, Security Lead, Well-Architected Table of Contents Requesting a public certificate using the console Tear Down 1. Request Certificate Sign into the AWS Management Console and open the ACM console at https://console.aws.amazon.com/acm/home . Select your prefferred region for regional certificates including Elastic Load Balancing, or US East (N. Virginia) for global services including Amazon CloudFront. If you see a welcome page, click Get started under provision certificates area. On the Request a certificate page, click Request a public certificate , then click Request a certificate . Type your domain name. You can use a fully qualified domain name (FQDN) such as www.example.com or a bare or apex domain name such as example.com . You can also use an asterisk * as a wildcard in the leftmost position to protect several site names in the same domain. For example, *.example.com protects corp.example.com , and images.example.com . The wildcard name will appear in the Subject field and the Subject Alternative Name extension of the ACM certificate. Note: When you request a wildcard certificate, the asterisk * must be in the leftmost position of the domain name and can protect only one subdomain level. For example, *.example.com can protect login.example.com , and test.example.com , but it cannot protect test.login.example.com . Also note that *.example.com protects only the subdomains of example.com , it does not protect the bare or apex domain example.com . To protect both, see the next step. To add more domain names to the ACM certificate, choose Add another name to this certificate and type another domain name in the text box that opens. This is useful for protecting both a bare or apex domain (like example.com ) and its subdomains *.example.com . After you have typed valid domain names, choose Next . Before ACM issues a certificate, it validates that you own or control the domain names in your certificate request. You can use either email validation or DNS validation. If you choose email validation, ACM sends validation email to three contact addresses registered in the WHOIS database and to five common system administration addresses for each domain name. You or an authorized representative must approve one of these email messages. If you use DNS validation, you simply create a CNAME record provided by ACM to your DNS configuration. Choose your option, then click Review . Note: If you are able to edit your DNS configuration, we recommend that you use DNS domain validation rather than email validation. DNS validation has multiple benefits over email validation. See Use DNS to Validate Domain Ownership. If the review page correctly contains the information that you provided for your request, choose Confirm and request . The following page shows that your request status is pending validation. You must approve the request either through email link or DNS record. Important: Unless you choose to opt out, your certificate will be automatically recorded in at least two public certificate transparency databases. You cannot currently use the console to opt out. You must use the AWS CLI or the API. For more information, see Opting Out of Certificate Transparency Logging . For general information about transparency logs, see Certificate Transparency Logging . 8. Your certificate is now ready to associate with a supported service . 2. Tear down this lab The following instructions will remove the certificate you have created. Sign into the AWS Management Console and open the ACM console at https://console.aws.amazon.com/acm/home . Select the region where you created the certificate. Click the check box for the domain name of the certificate to delete. Click Actions then Delete . Verify this is the certificate to delete and click Delete . Note: You cannot delete an ACM Certificate that is being used by another AWS service. To delete a certificate that is in use, you must first remove the certificate association. References & useful resources AWS Certificate Manager Create an HTTPS Listener for Your Application Load Balancer License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: AWS Certificate Manager Request Public Certificate"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/Lab_Guide.html#level-200-aws-certificate-manager-request-public-certificate","text":"","title":"Level 200: AWS Certificate Manager Request Public Certificate"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/Lab_Guide.html#table-of-contents","text":"Requesting a public certificate using the console Tear Down","title":"Table of Contents"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/Lab_Guide.html#1-request-certificate","text":"Sign into the AWS Management Console and open the ACM console at https://console.aws.amazon.com/acm/home . Select your prefferred region for regional certificates including Elastic Load Balancing, or US East (N. Virginia) for global services including Amazon CloudFront. If you see a welcome page, click Get started under provision certificates area. On the Request a certificate page, click Request a public certificate , then click Request a certificate . Type your domain name. You can use a fully qualified domain name (FQDN) such as www.example.com or a bare or apex domain name such as example.com . You can also use an asterisk * as a wildcard in the leftmost position to protect several site names in the same domain. For example, *.example.com protects corp.example.com , and images.example.com . The wildcard name will appear in the Subject field and the Subject Alternative Name extension of the ACM certificate. Note: When you request a wildcard certificate, the asterisk * must be in the leftmost position of the domain name and can protect only one subdomain level. For example, *.example.com can protect login.example.com , and test.example.com , but it cannot protect test.login.example.com . Also note that *.example.com protects only the subdomains of example.com , it does not protect the bare or apex domain example.com . To protect both, see the next step. To add more domain names to the ACM certificate, choose Add another name to this certificate and type another domain name in the text box that opens. This is useful for protecting both a bare or apex domain (like example.com ) and its subdomains *.example.com . After you have typed valid domain names, choose Next . Before ACM issues a certificate, it validates that you own or control the domain names in your certificate request. You can use either email validation or DNS validation. If you choose email validation, ACM sends validation email to three contact addresses registered in the WHOIS database and to five common system administration addresses for each domain name. You or an authorized representative must approve one of these email messages. If you use DNS validation, you simply create a CNAME record provided by ACM to your DNS configuration. Choose your option, then click Review . Note: If you are able to edit your DNS configuration, we recommend that you use DNS domain validation rather than email validation. DNS validation has multiple benefits over email validation. See Use DNS to Validate Domain Ownership. If the review page correctly contains the information that you provided for your request, choose Confirm and request . The following page shows that your request status is pending validation. You must approve the request either through email link or DNS record. Important: Unless you choose to opt out, your certificate will be automatically recorded in at least two public certificate transparency databases. You cannot currently use the console to opt out. You must use the AWS CLI or the API. For more information, see Opting Out of Certificate Transparency Logging . For general information about transparency logs, see Certificate Transparency Logging . 8. Your certificate is now ready to associate with a supported service .","title":"1. Request Certificate "},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/Lab_Guide.html#2-tear-down-this-lab","text":"The following instructions will remove the certificate you have created. Sign into the AWS Management Console and open the ACM console at https://console.aws.amazon.com/acm/home . Select the region where you created the certificate. Click the check box for the domain name of the certificate to delete. Click Actions then Delete . Verify this is the certificate to delete and click Delete . Note: You cannot delete an ACM Certificate that is being used by another AWS service. To delete a certificate that is in use, you must first remove the certificate association.","title":"2. Tear down this lab "},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/Lab_Guide.html#references-useful-resources","text":"AWS Certificate Manager Create an HTTPS Listener for Your Application Load Balancer","title":"References &amp; useful resources"},{"location":"Security/200_Certificate_Manager_Request_Public_Certificate/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_CloudFront_for_Web_Application/README.html","text":"Level 200: CloudFront for Web Application Introduction This hands-on lab will guide you through the steps to help protect a web application from network based attacks using Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. A web application to configure as the origin to CloudFront. Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: CloudFront for Web Application"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#level-200-cloudfront-for-web-application","text":"","title":"Level 200: CloudFront for Web Application"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#introduction","text":"This hands-on lab will guide you through the steps to help protect a web application from network based attacks using Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#goals","text":"Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection","title":"Goals"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. A web application to configure as the origin to CloudFront.","title":"Prerequisites"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_CloudFront_for_Web_Application/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html","text":"Level 200: CloudFront for Web Application: Lab Guide Authors Ben Potter, Security Lead, Well-Architected Table of Contents Configure CloudFront - EC2 or Load Balancer Tear Down 1. Configure Amazon CloudFront for EC2 or Elastic Load Balancer Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, choose Create Distribution. Click Get Started in the Web section. Specify the following settings for the distribution: In Origin Domain Name enter the DNS or domain name from your elastic load balancer or EC2 instance. Click Create Distrubution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation. 2. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. References & useful resources Amazon CloudFront Developer Guide AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Level 200: CloudFront for Web Application: Lab Guide"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#level-200-cloudfront-for-web-application-lab-guide","text":"","title":"Level 200: CloudFront for Web Application: Lab Guide"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#table-of-contents","text":"Configure CloudFront - EC2 or Load Balancer Tear Down","title":"Table of Contents"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#1-configure-amazon-cloudfront-for-ec2-or-elastic-load-balancer","text":"Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, choose Create Distribution. Click Get Started in the Web section. Specify the following settings for the distribution: In Origin Domain Name enter the DNS or domain name from your elastic load balancer or EC2 instance. Click Create Distrubution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.","title":"1. Configure Amazon CloudFront for EC2 or Elastic Load Balancer"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#2-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button.","title":"2. Tear down this lab "},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#references-useful-resources","text":"Amazon CloudFront Developer Guide AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide","title":"References &amp; useful resources"},{"location":"Security/200_CloudFront_for_Web_Application/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html","text":"Level 200: CloudFront with WAF Protection Introduction This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab. Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#level-200-cloudfront-with-waf-protection","text":"","title":"Level 200: CloudFront with WAF Protection"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#introduction","text":"This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#goals","text":"Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection","title":"Goals"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab.","title":"Prerequisites"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/200_CloudFront_with_WAF_Protection/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html","text":"Level 200: CloudFront with WAF Protection: Lab Guide Authors Ben Potter, Security Lead, Well-Architected Table of Contents Launch Instance Configure WAF Configure CloudFront Tear Down 1. Launch Instance You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn't cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Launch Instance. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI, either version. On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. On the Configure Instance Details page, make the following changes: 5.1 Select Create new IAM role. 5.2 In the new tab that opens, select Create role. 5.3 With AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions. 5.4 Enter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account. 5.5 Enter a role name, such as ec2-s3-read-only-role , and then click Create role. 5.6 Back on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created. 5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched: #!/bin/bash yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + Accept defaults and click Next: Add tags . Click Next: Configure Security Group . 7.1 Accept default option Create a new security group . 7.2 On the line of the first default entry SSH , select Source as My IP . 7.3 Click Add Rule , select Type as HTTP and Source as Anywhere . Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed to the internet. However, for simplicity in this lab, we are opening the access to anywhere. Other lab modules secure access with Elastic Load Balancer. 7.5 Click Review and Launch. On the Review Instance Launch page, check the details, and then click Launch. If you do not have an existing key pair for access instances, a prompt will appear. Click Create New,then type a name such as lab , click Download Key Pair, and then click Launch Instances. **Important** This is the only chance to save the private key file. You'll need to provide the name of your key pair when you launch an instance, and you'll provide the corresponding private key each time you connect to the instance. Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value. 2. Configure AWS WAF Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use waf . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1 . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1 . The remainder of the parameters can be left as defaults. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a basic AWS WAF configuration ready for CloudFront to use! 3. Configure Amazon CloudFront Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, choose Create Distribution. Click Get Started in the Web section. Specify the following settings for the distribution: In Origin Domain Name enter the EC2 public DNS name you recorded from your instance launch. In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously. Click Create Distrubution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings and AWS WAF. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation. 3. Tear down this lab The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. Delete the AWS WAF stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the waf-cloudfront stack. 3. Click the Actions button, and then click Delete Stack. 4. Confirm the stack, and then click the Yes, Delete button. References & useful resources Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#level-200-cloudfront-with-waf-protection-lab-guide","text":"","title":"Level 200: CloudFront with WAF Protection: Lab Guide"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#table-of-contents","text":"Launch Instance Configure WAF Configure CloudFront Tear Down","title":"Table of Contents"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#1-launch-instance","text":"You can launch a Linux instance using the AWS Management Console. This tutorial is intended to help you launch your first instance quickly, so it doesn't cover all possible options. For more information about the advanced options, see Launching an Instance . Launch an instance: Open the Amazon EC2 console at https://console.aws.amazon.com/ec2/. From the console dashboard, choose Launch Instance. The choose an Amazon Machine Image (AMI) page displays a list of basic configurations, called Amazon Machine Images (AMIs), that serve as templates for your instance. Select the HVM edition of the Amazon Linux AMI, either version. On the Choose an Instance Type page, you can select the hardware configuration of your instance. Select the t2.micro type, which is selected by default. Notice that this instance type is eligible for the free tier. Then select Next: Configure Instance Details. On the Configure Instance Details page, make the following changes: 5.1 Select Create new IAM role. 5.2 In the new tab that opens, select Create role. 5.3 With AWS service pre-selected, select EC2 from the top of the list, then click Next: Permissions. 5.4 Enter s3 in the search and select AmazonS3ReadOnlyAccess from the list of policies, then click Next: Review. This policy will give this EC2 instance access to read and list any objects in Amazon S3 within your AWS account. 5.5 Enter a role name, such as ec2-s3-read-only-role , and then click Create role. 5.6 Back on the EC2 launch web browser tab, select the refresh button next to Create new IAM role, and click the role you just created. 5.7 Scroll down and expand the Advanced Details section. Enter the following in the User Data test box to automatically install Apache web server and apply basic configuration when the instance is launched: #!/bin/bash yum update -y yum install -y httpd service httpd start chkconfig httpd on groupadd www usermod -a -G www ec2-user chown -R root:www /var/www chmod 2775 /var/www find /var/www -type d -exec chmod 2775 {} + find /var/www -type f -exec chmod 0664 {} + Accept defaults and click Next: Add tags . Click Next: Configure Security Group . 7.1 Accept default option Create a new security group . 7.2 On the line of the first default entry SSH , select Source as My IP . 7.3 Click Add Rule , select Type as HTTP and Source as Anywhere . Note that best practice is to have an Elastic Load Balancer inline or the EC2 instance not directly exposed to the internet. However, for simplicity in this lab, we are opening the access to anywhere. Other lab modules secure access with Elastic Load Balancer. 7.5 Click Review and Launch. On the Review Instance Launch page, check the details, and then click Launch. If you do not have an existing key pair for access instances, a prompt will appear. Click Create New,then type a name such as lab , click Download Key Pair, and then click Launch Instances. **Important** This is the only chance to save the private key file. You'll need to provide the name of your key pair when you launch an instance, and you'll provide the corresponding private key each time you connect to the instance. Click View Instances. When your instance is launched, its status will change to running, and it will need a few minutes to apply patches and install Apache web server. You can connect to the Apache test page by entering the public DNS, which you can find on the description tab or instances list. Take note of this public DNS value.","title":"1. Launch Instance "},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#2-configure-aws-waf","text":"Using AWS CloudFormation , we are going to deploy a basic example AWS WAF configuration for use with CloudFront. Sign in to the AWS Management Console, select your preferred region, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. Note if your CloudFormation console does not look the same, you can enable the redesigned console by clicking New Console in the CloudFormation menu. Click Create stack . Enter the following Amazon S3 URL : https://s3-us-west-2.amazonaws.com/aws-well-architected-labs/Security/Code/waf-global.yaml and click Next . Enter the following details: Stack name: The name of this stack. For this lab, use waf . WAFName: Enter the base name to be used for resource and export names for this stack. For this lab, you can use Lab1 . WAFCloudWatchPrefix: Enter the name of the CloudWatch prefix to use for each rule using alphanumeric characters only. For this lab, you can use Lab1 . The remainder of the parameters can be left as defaults. At the bottom of the page click Next . In this lab, we won't add any tags or other options. Click Next. Tags, which are key-value pairs, can help you identify your stacks. For more information, see Adding Tags to Your AWS CloudFormation Stack . Review the information for the stack. When you're satisfied with the configuration, click Create stack . After a few minutes the stack status should change from CREATE_IN_PROGRESS to CREATE_COMPLETE . You have now set up a basic AWS WAF configuration ready for CloudFront to use!","title":"2. Configure AWS WAF "},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#3-configure-amazon-cloudfront","text":"Using the AWS Management Console, we will create a CloudFront distribution, and link it to the AWS WAF ACL we previously created. Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, choose Create Distribution. Click Get Started in the Web section. Specify the following settings for the distribution: In Origin Domain Name enter the EC2 public DNS name you recorded from your instance launch. In the distribution Settings section, click AWS WAF Web ACL, and select the one you created previously. Click Create Distrubution. For more information on the other configuration options, see Values That You Specify When You Create or Update a Web Distribution in the CloudFront documentation. After CloudFront creates your distribution, the value of the Status column for your distribution will change from In Progress to Deployed. When your distribution is deployed, confirm that you can access your content using your new CloudFront URL or CNAME. Copy the Domain Name into a web browser to test. For more information, see Testing a Web Distribution in the CloudFront documentation. 7. You have now configured Amazon CloudFront with basic settings and AWS WAF. For more information on configuring CloudFront, see Viewing and Updating CloudFront Distributions in the CloudFront documentation.","title":"3. Configure Amazon CloudFront "},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#3-tear-down-this-lab","text":"The following instructions will remove the resources that have a cost for running them. Please note that Security Groups and SSH key will exist. You may remove these also or leave for future use. Delete the CloudFront distribution: Open the Amazon CloudFront console at https://console.aws.amazon.com/cloudfront/home. From the console dashboard, select the distribution you created earlier and click the Disable button. To confirm, click the Yes, Disable button. After approximately 15 minutes when the status is Deployed, select the distribution and click the Delete button, and then to confirm click the Yes, Delete button. Delete the AWS WAF stack: 1. Sign in to the AWS Management Console, and open the CloudFormation console at https://console.aws.amazon.com/cloudformation/. 2. Select the waf-cloudfront stack. 3. Click the Actions button, and then click Delete Stack. 4. Confirm the stack, and then click the Yes, Delete button.","title":"3. Tear down this lab "},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#references-useful-resources","text":"Amazon Elastic Compute Cloud User Guide for Linux Instances Amazon CloudFront Developer Guide Tutorial: Configure Apache Web Server on Amazon Linux 2 to Use SSL/TLS AWS WAF, AWS Firewall Manager, and AWS Shield Advanced Developer Guide","title":"References &amp; useful resources"},{"location":"Security/200_CloudFront_with_WAF_Protection/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html","text":"Level 300: IAM Permission Boundaries Delegating Role Creation Introduction This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals IAM permission boundaries IAM policy conditions Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#level-300-iam-permission-boundaries-delegating-role-creation","text":"","title":"Level 300: IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#introduction","text":"This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#goals","text":"IAM permission boundaries IAM policy conditions","title":"Goals"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html","text":"Level 300: IAM Permission Boundaries Delegating Role Creation Authors Ben Potter, Security Lead, Well-Architected Table of Contents Create IAM Policies Create and Test Developer Role Create and Test Region Restricted User Role Knowledge Check Tear Down The following image shows what you will be doing in this lab. 1. Create IAM policies 1.1 Create policy for permission boundary This policy will be used for the permission boundary when the developer role creates their own user role with their delegated permissions. In this lab using AWS IAM we are only going to allow the us-east-1 (North Virginia) and us-west-1 (North California) regions, optionally you can change these to your favourite regions and add / remove as many as you need. The only service actions we are going to allow in these regions are AWS EC2 and AWS Lambda, note that these services require additional supporting actions if you were to re-use this policy after this lab, depending on your requirements. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . If you need to enable MFA follow the IAM User Guide . You will need to log out and back in again with MFA so your session has MFA active. In the navigation pane, click Policies and then click Create policy . On the Create policy page click the JSON tab. Replace the example start of the policy that is already in the editor with the policy below. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"EC2RestrictRegion\", \"Effect\": \"Allow\", \"Action\": \"ec2:*\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"aws:RequestedRegion\": [ \"us-east-1\", \"us-west-1\" ] } } }, { \"Sid\": \"LambdaRestrictRegion\", \"Effect\": \"Allow\", \"Action\": \"lambda:*\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"aws:RequestedRegion\": [ \"us-east-1\", \"us-west-1\" ] } } } ] } Click Review policy . Enter the name of restrict-region-boundary and any description to help you identify the policy, verify the summary and then click Create policy . 1.2 Create developer IAM restricted policy This policy will be attached to the developer role, and will allow the developer to create policies and roles with a name prefix of app1 , and only if the permission boundary restrict-region-boundary is attached. You will need to change the account id placeholders of 123456789012 to your account number in 5 places. You can find your account id by navigating to https://console.aws.amazon.com/billing/home?#/account in the console. Naming prefixes are useful when you have different teams or in this case different applications running in the same AWS account. They can be used to keep your resources looking tidy, and also in IAM policy as the resource as we are doing here. Create a managed policy using the JSON policy below and name of createrole-restrict-region-boundary . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"CreatePolicy\", \"Effect\": \"Allow\", \"Action\": [ \"iam:CreatePolicy\", \"iam:CreatePolicyVersion\", \"iam:DeletePolicyVersion\" ], \"Resource\": \"arn:aws:iam::123456789012:policy/app1*\" }, { \"Sid\": \"CreateRole\", \"Effect\": \"Allow\", \"Action\": [ \"iam:CreateRole\" ], \"Resource\": \"arn:aws:iam::123456789012:role/app1*\", \"Condition\": { \"StringEquals\": { \"iam:PermissionsBoundary\": \"arn:aws:iam::123456789012:policy/restrict-region-boundary\" } } }, { \"Sid\": \"AttachDetachRolePolicy\", \"Effect\": \"Allow\", \"Action\": [ \"iam:DetachRolePolicy\", \"iam:AttachRolePolicy\" ], \"Resource\": \"arn:aws:iam::123456789012:role/app1*\", \"Condition\": { \"ArnEquals\": { \"iam:PolicyARN\": [ \"arn:aws:iam::123456789012:policy/*\", \"arn:aws:iam::aws:policy/*\" ] } } } ] } 1.3 Create developer IAM console access policy This policy allows list and read type IAM service actions so you can see what you have created using the console. Note that it is not a requirement if you simply wanted to create the role and policy, or if you were using the Command Line Interface (CLI) or CloudFormation. Create a managed policy using the JSON policy below and name of iam-restricted-list-read . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Get\", \"Effect\": \"Allow\", \"Action\": [ \"iam:ListPolicies\", \"iam:GetRole\", \"iam:GetPolicyVersion\", \"iam:ListRoleTags\", \"iam:GetPolicy\", \"iam:ListPolicyVersions\", \"iam:ListAttachedRolePolicies\", \"iam:ListRoles\", \"iam:ListRolePolicies\", \"iam:GetRolePolicy\" ], \"Resource\": \"*\" } ] } 2. Create and Test Developer Role 2.1 Create Developer Role Create a role for developers that will have permission to create roles and policies, with the permission boundary and naming prefix enforced: Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles and then click Create role . Click Another AWS account, then enter your account ID and tick Require MFA, then click Next: Permissions . We enforce MFA here as it is a best practice. In the search field start typing createrole then check the box next to the createrole-restrict-region-boundary policy. Erase your previous search and start typing iam-res then check the box next to the iam-restricted-list-read policy and then click Next: Tags . For this lab we will not use IAM tags, click Next: Review . Enter the name of developer-restricted-iam for the Role name and click Create role . Check the role you have created by clicking on developer-restricted-iam in the list. Record both the Role ARN and the link to the console. The role is now created, ready to test! 2.2. Test Developer Role Now you will use an existing IAM user with MFA enabled to assume the new developer-restricted-iam role. Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role . Alternatively you can paste the link in your browser that you recorded earlier. On the Switch Role page, type the account ID number or the account alias and the name of the role developer-restricted-iam that you created in the previous step. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. Click Switch Role . If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 6. You are now using the developer role with the granted permissions, stay logged in using the role for the next section. 3. Create and Test User Role 3.1 Create User Role While you are still assuming the developer-restricted-iam role you created in the previous step, create a new user role with the boundary policy attached and name it with the prefix. We will use AWS managed policies for this user role, however the createrole-restrict-region-boundary policy will allow us to create and attach our own policies, only if they have a prefix of app1 . Verify that you are Using the developer role previously created by checking the top bar it should look like and open the IAM console at https://console.aws.amazon.com/iam/ . You will notice a number of permission denied messages as this developer role is restricted. Least privilege is a best practice! In the navigation pane, click Roles and then click Create role . Click Another AWS account , then enter your account ID that you have been using for this lab and tick Require MFA , then click Next: Permissions . In the search field start typing ec2full then check the box next to the AmazonEC2FullAccess policy. Erase your previous search and start typing lambda then check the box next to the AWSLambdaFullAccess policy. Expand the bottom section Set permissions boundary and click Use a permissions boundary to control the maximum role permissions . In the search field start typing boundary then click the radio button for restrict-region-boundary and then click Next: Tags . For this lab we will not use IAM tags, click Next: Review . Enter the Role name of app1-user-region-restricted-services for the role and click Create role . The role should create successfully if you followed all the steps. Record both the Role ARN and the link to the console. If you receive an error message a common mistake is not changing the account number in the policies in the previous steps. 3.2 Test User Role Now you will use an existing IAM user to assume the new app1-user-region-restricted-services role, as if you were a user who only needs to administer EC2 and Lambda in your allowed regions. In the console, click your role's Display Name on the right side of the navigation bar. Click Back to your previous username . You are now back to using your original IAM user. In the console, click your user name on the navigation bar in the upper right. Alternatively you can paste the link in your browser that you recorded earlier for the app1-user-region-restricted-services role. On the Switch Role page, type the account ID number or the account alias and the name of the role app1-user-region-restricted-services that you created in the previous step. Select a different color to before, otherwise it will overwrite that profile in your browser. Click Switch Role . The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. You are now using the user role with the only actions allowed of EC2 and Lambda in us-east-1 (North Virginia) and us-west-1 (North California) regions! Navigate to the EC2 Management Console in the us-east-1 region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. Navigate to the EC2 Management Console in a region that is not allowed, such as ap-southeast-2 (Sydney) https://ap-southeast-2.console.aws.amazon.com/ec2/v2/home?region=ap-southeast-2 . The EC2 Dashboard should display a number of unauthorized error messages. Congratulations! You have now learnt about IAM permission boundaries and have one working! 4. Knowledge Check The security best practices followed in this lab are: Manage credentials and authentication Use of MFA for access to provide additional access control. Grant access through roles or federation: Roles with associated policies have been used to define appropriate permission boundaries. Grant least privileges: The roles are scoped with minimum privileges to accomplish the task. 5. Tear down this lab Please note that the changes you made to the users, groups, and roles have no charges associated with them. Using the original IAM user, for each of the roles you created select them in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role . The roles created are: app1-user-region-restricted-services developer-restricted-iam For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete . The policies created are: restrict-region-boundary createrole-restrict-region-boundary iam-restricted-list-read References & useful resources Permissions Boundaries for IAM Entities AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#level-300-iam-permission-boundaries-delegating-role-creation","text":"","title":"Level 300: IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#table-of-contents","text":"Create IAM Policies Create and Test Developer Role Create and Test Region Restricted User Role Knowledge Check Tear Down The following image shows what you will be doing in this lab.","title":"Table of Contents"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#1-create-iam-policies","text":"","title":"1. Create IAM policies "},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#11-create-policy-for-permission-boundary","text":"This policy will be used for the permission boundary when the developer role creates their own user role with their delegated permissions. In this lab using AWS IAM we are only going to allow the us-east-1 (North Virginia) and us-west-1 (North California) regions, optionally you can change these to your favourite regions and add / remove as many as you need. The only service actions we are going to allow in these regions are AWS EC2 and AWS Lambda, note that these services require additional supporting actions if you were to re-use this policy after this lab, depending on your requirements. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . If you need to enable MFA follow the IAM User Guide . You will need to log out and back in again with MFA so your session has MFA active. In the navigation pane, click Policies and then click Create policy . On the Create policy page click the JSON tab. Replace the example start of the policy that is already in the editor with the policy below. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"EC2RestrictRegion\", \"Effect\": \"Allow\", \"Action\": \"ec2:*\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"aws:RequestedRegion\": [ \"us-east-1\", \"us-west-1\" ] } } }, { \"Sid\": \"LambdaRestrictRegion\", \"Effect\": \"Allow\", \"Action\": \"lambda:*\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"aws:RequestedRegion\": [ \"us-east-1\", \"us-west-1\" ] } } } ] } Click Review policy . Enter the name of restrict-region-boundary and any description to help you identify the policy, verify the summary and then click Create policy .","title":"1.1 Create policy for permission boundary"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#12-create-developer-iam-restricted-policy","text":"This policy will be attached to the developer role, and will allow the developer to create policies and roles with a name prefix of app1 , and only if the permission boundary restrict-region-boundary is attached. You will need to change the account id placeholders of 123456789012 to your account number in 5 places. You can find your account id by navigating to https://console.aws.amazon.com/billing/home?#/account in the console. Naming prefixes are useful when you have different teams or in this case different applications running in the same AWS account. They can be used to keep your resources looking tidy, and also in IAM policy as the resource as we are doing here. Create a managed policy using the JSON policy below and name of createrole-restrict-region-boundary . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"CreatePolicy\", \"Effect\": \"Allow\", \"Action\": [ \"iam:CreatePolicy\", \"iam:CreatePolicyVersion\", \"iam:DeletePolicyVersion\" ], \"Resource\": \"arn:aws:iam::123456789012:policy/app1*\" }, { \"Sid\": \"CreateRole\", \"Effect\": \"Allow\", \"Action\": [ \"iam:CreateRole\" ], \"Resource\": \"arn:aws:iam::123456789012:role/app1*\", \"Condition\": { \"StringEquals\": { \"iam:PermissionsBoundary\": \"arn:aws:iam::123456789012:policy/restrict-region-boundary\" } } }, { \"Sid\": \"AttachDetachRolePolicy\", \"Effect\": \"Allow\", \"Action\": [ \"iam:DetachRolePolicy\", \"iam:AttachRolePolicy\" ], \"Resource\": \"arn:aws:iam::123456789012:role/app1*\", \"Condition\": { \"ArnEquals\": { \"iam:PolicyARN\": [ \"arn:aws:iam::123456789012:policy/*\", \"arn:aws:iam::aws:policy/*\" ] } } } ] }","title":"1.2 Create developer IAM restricted policy"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#13-create-developer-iam-console-access-policy","text":"This policy allows list and read type IAM service actions so you can see what you have created using the console. Note that it is not a requirement if you simply wanted to create the role and policy, or if you were using the Command Line Interface (CLI) or CloudFormation. Create a managed policy using the JSON policy below and name of iam-restricted-list-read . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Get\", \"Effect\": \"Allow\", \"Action\": [ \"iam:ListPolicies\", \"iam:GetRole\", \"iam:GetPolicyVersion\", \"iam:ListRoleTags\", \"iam:GetPolicy\", \"iam:ListPolicyVersions\", \"iam:ListAttachedRolePolicies\", \"iam:ListRoles\", \"iam:ListRolePolicies\", \"iam:GetRolePolicy\" ], \"Resource\": \"*\" } ] }","title":"1.3 Create developer IAM console access policy"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#2-create-and-test-developer-role","text":"","title":"2. Create and Test Developer Role "},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#21-create-developer-role","text":"Create a role for developers that will have permission to create roles and policies, with the permission boundary and naming prefix enforced: Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles and then click Create role . Click Another AWS account, then enter your account ID and tick Require MFA, then click Next: Permissions . We enforce MFA here as it is a best practice. In the search field start typing createrole then check the box next to the createrole-restrict-region-boundary policy. Erase your previous search and start typing iam-res then check the box next to the iam-restricted-list-read policy and then click Next: Tags . For this lab we will not use IAM tags, click Next: Review . Enter the name of developer-restricted-iam for the Role name and click Create role . Check the role you have created by clicking on developer-restricted-iam in the list. Record both the Role ARN and the link to the console. The role is now created, ready to test!","title":"2.1 Create Developer Role"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#22-test-developer-role","text":"Now you will use an existing IAM user with MFA enabled to assume the new developer-restricted-iam role. Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role . Alternatively you can paste the link in your browser that you recorded earlier. On the Switch Role page, type the account ID number or the account alias and the name of the role developer-restricted-iam that you created in the previous step. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. Click Switch Role . If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 6. You are now using the developer role with the granted permissions, stay logged in using the role for the next section.","title":"2.2. Test Developer Role"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#3-create-and-test-user-role","text":"","title":"3. Create and Test User Role "},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#31-create-user-role","text":"While you are still assuming the developer-restricted-iam role you created in the previous step, create a new user role with the boundary policy attached and name it with the prefix. We will use AWS managed policies for this user role, however the createrole-restrict-region-boundary policy will allow us to create and attach our own policies, only if they have a prefix of app1 . Verify that you are Using the developer role previously created by checking the top bar it should look like and open the IAM console at https://console.aws.amazon.com/iam/ . You will notice a number of permission denied messages as this developer role is restricted. Least privilege is a best practice! In the navigation pane, click Roles and then click Create role . Click Another AWS account , then enter your account ID that you have been using for this lab and tick Require MFA , then click Next: Permissions . In the search field start typing ec2full then check the box next to the AmazonEC2FullAccess policy. Erase your previous search and start typing lambda then check the box next to the AWSLambdaFullAccess policy. Expand the bottom section Set permissions boundary and click Use a permissions boundary to control the maximum role permissions . In the search field start typing boundary then click the radio button for restrict-region-boundary and then click Next: Tags . For this lab we will not use IAM tags, click Next: Review . Enter the Role name of app1-user-region-restricted-services for the role and click Create role . The role should create successfully if you followed all the steps. Record both the Role ARN and the link to the console. If you receive an error message a common mistake is not changing the account number in the policies in the previous steps.","title":"3.1 Create User Role"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#32-test-user-role","text":"Now you will use an existing IAM user to assume the new app1-user-region-restricted-services role, as if you were a user who only needs to administer EC2 and Lambda in your allowed regions. In the console, click your role's Display Name on the right side of the navigation bar. Click Back to your previous username . You are now back to using your original IAM user. In the console, click your user name on the navigation bar in the upper right. Alternatively you can paste the link in your browser that you recorded earlier for the app1-user-region-restricted-services role. On the Switch Role page, type the account ID number or the account alias and the name of the role app1-user-region-restricted-services that you created in the previous step. Select a different color to before, otherwise it will overwrite that profile in your browser. Click Switch Role . The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you. You are now using the user role with the only actions allowed of EC2 and Lambda in us-east-1 (North Virginia) and us-west-1 (North California) regions! Navigate to the EC2 Management Console in the us-east-1 region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. Navigate to the EC2 Management Console in a region that is not allowed, such as ap-southeast-2 (Sydney) https://ap-southeast-2.console.aws.amazon.com/ec2/v2/home?region=ap-southeast-2 . The EC2 Dashboard should display a number of unauthorized error messages. Congratulations! You have now learnt about IAM permission boundaries and have one working!","title":"3.2 Test User Role"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#4-knowledge-check","text":"The security best practices followed in this lab are: Manage credentials and authentication Use of MFA for access to provide additional access control. Grant access through roles or federation: Roles with associated policies have been used to define appropriate permission boundaries. Grant least privileges: The roles are scoped with minimum privileges to accomplish the task.","title":"4. Knowledge Check "},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#5-tear-down-this-lab","text":"Please note that the changes you made to the users, groups, and roles have no charges associated with them. Using the original IAM user, for each of the roles you created select them in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role . The roles created are: app1-user-region-restricted-services developer-restricted-iam For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete . The policies created are: restrict-region-boundary createrole-restrict-region-boundary iam-restricted-list-read","title":"5. Tear down this lab "},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#references-useful-resources","text":"Permissions Boundaries for IAM Entities AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management","title":"References &amp; useful resources"},{"location":"Security/300_IAM_Permission_Boundaries_Delegating_Role_Creation/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html","text":"Level 300: IAM Tag Based Access Control for EC2 Introduction This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals IAM least privilege IAM policy conditions Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#level-300-iam-tag-based-access-control-for-ec2","text":"","title":"Level 300: IAM Tag Based Access Control for EC2"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#introduction","text":"This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#goals","text":"IAM least privilege IAM policy conditions","title":"Goals"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html","text":"Level 300: IAM Tag Based Access Control for EC2 Authors Ben Potter, Security Lead, Well-Architected Table of Contents Create IAM Policies Create IAM Role Test Knowledge Check Tear Down In this lab we use the RequestTag condition key to require specific tag value during create actions in the EC2 service. This allows users to create tags when creating resources only if they meet specific requirements. To control which existing resources users can modify tags on we use a combination of RequestTag and ResourceTag conditions. To control resources users can manage based on tag values we use ResourceTag based on a tag that exists on a resource. You can think of RequestTag condition key is for new resources when you are creating, and ResourceTag is the tag that already exists on the resource. 1. Create IAM managed policies The policies are split into five different functions for demonstration purposes, you may like to modify and combine them to use after this lab to your exact requirements. In addition to enforcing tags, a region restriction only allow regions us-east-1 (North Virginia) and us-west-1 (North California). 1.1 Create policy named ec2-list-read This policy allows read only permissions with a region condition. The only service actions we are going to allow are EC2, note that you typically require additional supporting actions such as Elastic Load Balancing if you were to re-use this policy after this lab, depending on your requirements. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . If you need to enable MFA follow the IAM User Guide . You will need to log out and back in again with MFA so your session has MFA active. In the navigation pane, click Policies and then click Create policy . On the Create policy page click the JSON tab. Replace the example start of the policy that is already in the editor with the policy below. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"ec2listread\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:Describe*\", \"ec2:Get*\" ], \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"aws:RequestedRegion\": [ \"us-east-1\", \"us-west-1\" ] } } } ] } Click Review policy . Enter the name of ec2-list-read and any description to help you identify the policy, verify the summary and then click Create policy . 1.2 Create policy named ec2-create-tags This policy allows the creation of tags for EC2, with a condition of the action being RunInstances , which is launching an instance. Create a managed policy using the JSON policy below and name of ec2-create-tags . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"ec2createtags\", \"Effect\": \"Allow\", \"Action\": \"ec2:CreateTags\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"ec2:CreateAction\": \"RunInstances\" } } } ] } 1.3 Create policy named ec2-create-tags-existing This policy allows creation (and overwriting) of EC2 tags only if the resources are already tagged Team / Alpha . Create a managed policy using the JSON policy below and name of ec2-create-tags-existing . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"ec2createtagsexisting\", \"Effect\": \"Allow\", \"Action\": \"ec2:CreateTags\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"ec2:ResourceTag/Team\": \"Alpha\" }, \"ForAllValues:StringEquals\": { \"aws:TagKeys\": [ \"Team\", \"Name\" ] }, \"StringEqualsIfExists\": { \"aws:RequestTag/Team\": \"Alpha\" } } } ] } 1.4 Create policy named ec2-run-instances This first section of this policy allows instances to be launched, only if the conditions of region and specific tag keys are matched. The second section allows other resources to be created at instance launch time with region condition. Create a managed policy using the JSON policy below and name of ec2-run-instances . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"ec2runinstances\", \"Effect\": \"Allow\", \"Action\": \"ec2:RunInstances\", \"Resource\": \"arn:aws:ec2:*:*:instance/*\", \"Condition\": { \"StringEquals\": { \"aws:RequestedRegion\": [ \"us-east-1\", \"us-west-1\" ], \"aws:RequestTag/Team\": \"Alpha\" }, \"ForAllValues:StringEquals\": { \"aws:TagKeys\": [ \"Name\", \"Team\" ] } } }, { \"Sid\": \"ec2runinstancesother\", \"Effect\": \"Allow\", \"Action\": \"ec2:RunInstances\", \"Resource\": [ \"arn:aws:ec2:*:*:subnet/*\", \"arn:aws:ec2:*:*:key-pair/*\", \"arn:aws:ec2:*::snapshot/*\", \"arn:aws:ec2:*:*:launch-template/*\", \"arn:aws:ec2:*:*:volume/*\", \"arn:aws:ec2:*:*:security-group/*\", \"arn:aws:ec2:*:*:placement-group/*\", \"arn:aws:ec2:*:*:network-interface/*\", \"arn:aws:ec2:*::image/*\" ], \"Condition\": { \"StringEquals\": { \"aws:RequestedRegion\": [ \"us-east-1\", \"us-west-1\" ] } } } ] } 1.5 Create policy named ec2-manage-instances This policy allows reboot, terminate, start and stop of instances, with a condition of the key Team is Alpha and region. Create a managed policy using the JSON policy below and name of ec2-manage-instances . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"ec2manageinstances\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:RebootInstances\", \"ec2:TerminateInstances\", \"ec2:StartInstances\", \"ec2:StopInstances\" ], \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"ec2:ResourceTag/Team\": \"Alpha\", \"aws:RequestedRegion\": [ \"us-east-1\", \"us-west-1\" ] } } } ] } 2. Create Role Create a role for EC2 administrators, and attach the managed policies previously created. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles and then click Create role . Click Another AWS account, then enter the account ID of the account you are using now and tick Require MFA, then click Next: Permissions . We enforce MFA here as it is a best practice. In the search field start typing ec2- then check the box next to the policies you just created: ec2-create-tags , ec2-create-tags-existing , ec2-list-read , ec2-manage-instances , ec2-run-instances . and then click Next: Tags . For this lab we will not use IAM tags, click Next: Review . Enter the name of ec2-admin-team-alpha for the Role name and click Create role . Check the role you have created by clicking on ec2-admin-team-alpha in the list. Record both the Role ARN and the link to the console. The role is now created, ready to test! 3. Test Role 3.1 Assume ec2-admin-team-alpha Role Now you will use an existing IAM user with MFA enabled to assume the new ec2-admin-team-alpha role. Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role . Alternatively you can paste the link in your browser that you recorded earlier. On the Switch Role page, type you account ID number in the Account field, and the name of the role ec2-admin-team-alpha that you created in the previous step in the Role field. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. Click Switch Role . If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu. 3.2 Launch Instance With & Without Tags Navigate to the EC2 Management Console in the us-east-2 (Ohio) region https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2 . The EC2 Dashboard should display a list of errors including You are not authorized . This is the first test passed, as us-east-2 region is not allowed. Navigate to the EC2 Management Console in the us-east-1 (North Virginia) region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. Click Launch Instance button to start the wizard. Click Select next to the first Amazon Linux 2 Amazon Machine Image to launch. Accept the default instance size by clicking Next: Configure Instance Details . Accept default details by clicking Next: Add Storage . Accept default storage options by clicking Next: Add Tags . Lets add an incorrect tag now that will fail to launch. Click Add Tag enter Key of Name and Value of Example . Repeat to add Key of Team and Value of Beta . Note: Keys and values are case sensitive! Click Next: Configure Security Group . Click Select an existing security group , click the check box next to security group with name default , then click Review and Launch . Click Launch then click the option to Proceed without a key pair . Tick the I acknowledge box then click Launch Instances . The launch should fail, if it succeeded verify the role you are using and the managed roles you have attached as per previous steps. Click Back to Review Screen then click Edit tags to modify the tags. Change the Team key to a value of Alpha which matches the IAM policy previously created then click Review and Launch . On the review launch page once again click Launch then click the option to Proceed without a key pair . Tick the I acknowledge box then click Launch Instances . You should see a message that the instance is now launching. Click View Instances and do not terminate it just yet. 3.3 Modify Tags On Instances Continuing from 3.2 in the EC2 Management Console instances view, click the check box next to the instance named Example then the Tags tab. Click Add/Edit Tags , try changing the Team key to a value of Test then click Save . An error message should appear. Change the Team key back to Alpha, and edit the Name key to a value of Test and click Save . The request should succeed. 3.4 Manage Instances Continuing from 3.3 in the EC2 Management Console instances view, click the check box next to the instance named Test . Click Actions button then expand out Instance State then Terminate . Check the instance is the one you wish to terminate by it's name and click Yes, Terminate . The instance should now terminate. Congratulations! You have now learnt about IAM tag based permissions for EC2! 4. Knowledge Check The security best practices followed in this lab are: * Grant least privileges: The roles are scoped with minimum privileges to accomplish the task. 5. Tear down this lab Please note that the changes you made to the policies and roles have no charges associated with them. Using the original IAM user, select the ec2-admin-team-alpha role in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role . For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete . The policies created are: ec2-create-tags ec2-create-tags-existing ec2-list-read ec2-manage-instances ec2-run-instances References & useful resources AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#level-300-iam-tag-based-access-control-for-ec2","text":"","title":"Level 300: IAM Tag Based Access Control for EC2"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#table-of-contents","text":"Create IAM Policies Create IAM Role Test Knowledge Check Tear Down In this lab we use the RequestTag condition key to require specific tag value during create actions in the EC2 service. This allows users to create tags when creating resources only if they meet specific requirements. To control which existing resources users can modify tags on we use a combination of RequestTag and ResourceTag conditions. To control resources users can manage based on tag values we use ResourceTag based on a tag that exists on a resource. You can think of RequestTag condition key is for new resources when you are creating, and ResourceTag is the tag that already exists on the resource.","title":"Table of Contents"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#1-create-iam-managed-policies","text":"The policies are split into five different functions for demonstration purposes, you may like to modify and combine them to use after this lab to your exact requirements. In addition to enforcing tags, a region restriction only allow regions us-east-1 (North Virginia) and us-west-1 (North California).","title":"1. Create IAM managed policies "},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#11-create-policy-named-ec2-list-read","text":"This policy allows read only permissions with a region condition. The only service actions we are going to allow are EC2, note that you typically require additional supporting actions such as Elastic Load Balancing if you were to re-use this policy after this lab, depending on your requirements. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . If you need to enable MFA follow the IAM User Guide . You will need to log out and back in again with MFA so your session has MFA active. In the navigation pane, click Policies and then click Create policy . On the Create policy page click the JSON tab. Replace the example start of the policy that is already in the editor with the policy below. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"ec2listread\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:Describe*\", \"ec2:Get*\" ], \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"aws:RequestedRegion\": [ \"us-east-1\", \"us-west-1\" ] } } } ] } Click Review policy . Enter the name of ec2-list-read and any description to help you identify the policy, verify the summary and then click Create policy .","title":"1.1 Create policy named ec2-list-read"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#12-create-policy-named-ec2-create-tags","text":"This policy allows the creation of tags for EC2, with a condition of the action being RunInstances , which is launching an instance. Create a managed policy using the JSON policy below and name of ec2-create-tags . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"ec2createtags\", \"Effect\": \"Allow\", \"Action\": \"ec2:CreateTags\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"ec2:CreateAction\": \"RunInstances\" } } } ] }","title":"1.2 Create policy named ec2-create-tags"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#13-create-policy-named-ec2-create-tags-existing","text":"This policy allows creation (and overwriting) of EC2 tags only if the resources are already tagged Team / Alpha . Create a managed policy using the JSON policy below and name of ec2-create-tags-existing . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"ec2createtagsexisting\", \"Effect\": \"Allow\", \"Action\": \"ec2:CreateTags\", \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"ec2:ResourceTag/Team\": \"Alpha\" }, \"ForAllValues:StringEquals\": { \"aws:TagKeys\": [ \"Team\", \"Name\" ] }, \"StringEqualsIfExists\": { \"aws:RequestTag/Team\": \"Alpha\" } } } ] }","title":"1.3 Create policy named ec2-create-tags-existing"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#14-create-policy-named-ec2-run-instances","text":"This first section of this policy allows instances to be launched, only if the conditions of region and specific tag keys are matched. The second section allows other resources to be created at instance launch time with region condition. Create a managed policy using the JSON policy below and name of ec2-run-instances . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"ec2runinstances\", \"Effect\": \"Allow\", \"Action\": \"ec2:RunInstances\", \"Resource\": \"arn:aws:ec2:*:*:instance/*\", \"Condition\": { \"StringEquals\": { \"aws:RequestedRegion\": [ \"us-east-1\", \"us-west-1\" ], \"aws:RequestTag/Team\": \"Alpha\" }, \"ForAllValues:StringEquals\": { \"aws:TagKeys\": [ \"Name\", \"Team\" ] } } }, { \"Sid\": \"ec2runinstancesother\", \"Effect\": \"Allow\", \"Action\": \"ec2:RunInstances\", \"Resource\": [ \"arn:aws:ec2:*:*:subnet/*\", \"arn:aws:ec2:*:*:key-pair/*\", \"arn:aws:ec2:*::snapshot/*\", \"arn:aws:ec2:*:*:launch-template/*\", \"arn:aws:ec2:*:*:volume/*\", \"arn:aws:ec2:*:*:security-group/*\", \"arn:aws:ec2:*:*:placement-group/*\", \"arn:aws:ec2:*:*:network-interface/*\", \"arn:aws:ec2:*::image/*\" ], \"Condition\": { \"StringEquals\": { \"aws:RequestedRegion\": [ \"us-east-1\", \"us-west-1\" ] } } } ] }","title":"1.4 Create policy named ec2-run-instances"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#15-create-policy-named-ec2-manage-instances","text":"This policy allows reboot, terminate, start and stop of instances, with a condition of the key Team is Alpha and region. Create a managed policy using the JSON policy below and name of ec2-manage-instances . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"ec2manageinstances\", \"Effect\": \"Allow\", \"Action\": [ \"ec2:RebootInstances\", \"ec2:TerminateInstances\", \"ec2:StartInstances\", \"ec2:StopInstances\" ], \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"ec2:ResourceTag/Team\": \"Alpha\", \"aws:RequestedRegion\": [ \"us-east-1\", \"us-west-1\" ] } } } ] }","title":"1.5 Create policy named ec2-manage-instances"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#2-create-role","text":"Create a role for EC2 administrators, and attach the managed policies previously created. Sign in to the AWS Management Console as an IAM user with MFA enabled that can assume roles in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . In the navigation pane, click Roles and then click Create role . Click Another AWS account, then enter the account ID of the account you are using now and tick Require MFA, then click Next: Permissions . We enforce MFA here as it is a best practice. In the search field start typing ec2- then check the box next to the policies you just created: ec2-create-tags , ec2-create-tags-existing , ec2-list-read , ec2-manage-instances , ec2-run-instances . and then click Next: Tags . For this lab we will not use IAM tags, click Next: Review . Enter the name of ec2-admin-team-alpha for the Role name and click Create role . Check the role you have created by clicking on ec2-admin-team-alpha in the list. Record both the Role ARN and the link to the console. The role is now created, ready to test!","title":"2. Create Role "},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#3-test-role","text":"","title":"3. Test Role"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#31-assume-ec2-admin-team-alpha-role","text":"Now you will use an existing IAM user with MFA enabled to assume the new ec2-admin-team-alpha role. Sign in to the AWS Management Console as an IAM user with MFA enabled. https://console.aws.amazon.com . In the console, click your user name on the navigation bar in the upper right. It typically looks like this: username@account_ID_number_or_alias then click Switch Role . Alternatively you can paste the link in your browser that you recorded earlier. On the Switch Role page, type you account ID number in the Account field, and the name of the role ec2-admin-team-alpha that you created in the previous step in the Role field. (Optional) Type text that you want to appear on the navigation bar in place of your user name when this role is active. A name is suggested, based on the account and role information, but you can change it to whatever has meaning for you. You can also select a color to highlight the display name. Click Switch Role . If this is the first time choosing this option, a page appears with more information. After reading it, click Switch Role. If you clear your browser cookies, this page can appear again. The display name and color replace your user name on the navigation bar, and you can start using the permissions that the role grants you replacing the permission that you had as the IAM user. Tip The last several roles that you used appear on the menu. The next time you need to switch to one of those roles, you can simply click the role you want. You only need to type the account and role information manually if the role is not displayed on the Identity menu.","title":"3.1 Assume ec2-admin-team-alpha Role"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#32-launch-instance-with-without-tags","text":"Navigate to the EC2 Management Console in the us-east-2 (Ohio) region https://us-east-2.console.aws.amazon.com/ec2/v2/home?region=us-east-2 . The EC2 Dashboard should display a list of errors including You are not authorized . This is the first test passed, as us-east-2 region is not allowed. Navigate to the EC2 Management Console in the us-east-1 (North Virginia) region https://us-east-1.console.aws.amazon.com/ec2/v2/home?region=us-east-1 . The EC2 Dashboard should display a summary list of resources with the only error being Error retrieving resource count from Elastic Load Balancing as that requires additional permissions. Click Launch Instance button to start the wizard. Click Select next to the first Amazon Linux 2 Amazon Machine Image to launch. Accept the default instance size by clicking Next: Configure Instance Details . Accept default details by clicking Next: Add Storage . Accept default storage options by clicking Next: Add Tags . Lets add an incorrect tag now that will fail to launch. Click Add Tag enter Key of Name and Value of Example . Repeat to add Key of Team and Value of Beta . Note: Keys and values are case sensitive! Click Next: Configure Security Group . Click Select an existing security group , click the check box next to security group with name default , then click Review and Launch . Click Launch then click the option to Proceed without a key pair . Tick the I acknowledge box then click Launch Instances . The launch should fail, if it succeeded verify the role you are using and the managed roles you have attached as per previous steps. Click Back to Review Screen then click Edit tags to modify the tags. Change the Team key to a value of Alpha which matches the IAM policy previously created then click Review and Launch . On the review launch page once again click Launch then click the option to Proceed without a key pair . Tick the I acknowledge box then click Launch Instances . You should see a message that the instance is now launching. Click View Instances and do not terminate it just yet.","title":"3.2 Launch Instance With &amp; Without Tags"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#33-modify-tags-on-instances","text":"Continuing from 3.2 in the EC2 Management Console instances view, click the check box next to the instance named Example then the Tags tab. Click Add/Edit Tags , try changing the Team key to a value of Test then click Save . An error message should appear. Change the Team key back to Alpha, and edit the Name key to a value of Test and click Save . The request should succeed.","title":"3.3 Modify Tags On Instances"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#34-manage-instances","text":"Continuing from 3.3 in the EC2 Management Console instances view, click the check box next to the instance named Test . Click Actions button then expand out Instance State then Terminate . Check the instance is the one you wish to terminate by it's name and click Yes, Terminate . The instance should now terminate. Congratulations! You have now learnt about IAM tag based permissions for EC2!","title":"3.4 Manage Instances"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#4-knowledge-check","text":"The security best practices followed in this lab are: * Grant least privileges: The roles are scoped with minimum privileges to accomplish the task.","title":"4. Knowledge Check "},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#5-tear-down-this-lab","text":"Please note that the changes you made to the policies and roles have no charges associated with them. Using the original IAM user, select the ec2-admin-team-alpha role in the IAM console at https://console.aws.amazon.com/iam/ and click Delete role . For each of the policies you created, one at a time select the radio button then Policy actions drop down menu then Delete . The policies created are: ec2-create-tags ec2-create-tags-existing ec2-list-read ec2-manage-instances ec2-run-instances","title":"5. Tear down this lab "},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#references-useful-resources","text":"AWS Identity and Access Management User Guide IAM Best Practices and Use Cases Become an IAM Policy Master in 60 Minutes or Less Actions, Resources, and Condition Keys for Identity And Access Management","title":"References &amp; useful resources"},{"location":"Security/300_IAM_Tag_Based_Access_Control_for_EC2/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/README.html","text":"Level 300: Incident Response Playbook with Jupyter - AWS IAM Introduction This hands-on lab will guide you through running a basic incident response playbook using Jupyter. It is a best practice to be prepared for an incident, and practice your investigation and response tools and processes. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Identify tooling for incident response Automate containment for incident response Pre-deploy tools for incident response Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . CloudTrail must already be enabled in your account and logging to CloudWatch Logs, follow the Automated Deployment of Detective Controls lab to enable. Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/README.html#level-300-incident-response-playbook-with-jupyter-aws-iam","text":"","title":"Level 300: Incident Response Playbook with Jupyter - AWS IAM"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/README.html#introduction","text":"This hands-on lab will guide you through running a basic incident response playbook using Jupyter. It is a best practice to be prepared for an incident, and practice your investigation and response tools and processes. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/README.html#goals","text":"Identify tooling for incident response Automate containment for incident response Pre-deploy tools for incident response","title":"Goals"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . CloudTrail must already be enabled in your account and logging to CloudWatch Logs, follow the Automated Deployment of Detective Controls lab to enable.","title":"Prerequisites"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/Lab_Guide.html","text":"Level 300: Incident Response Playbook with Jupyter - AWS IAM Authors Ben Potter, Security Lead, Well-Architected Byron Pogson, Solutions Architect, AWS Table of Contents Prerequisites Playbook Run Knowledge Check 1. Prerequisites 1.1 Install Python and Modules Python 3 and a number of Python modules are required. Python downloads Installing pip may be required also. After installing Python, install the following packages by executing the following command in your command line or terminal: pip install boto3 pandas jupyter 1.2 Install the AWS CLI AWS CLI is not directly used for this lab, however it makes configuration of the AWS IAM credentials easier, and is useful for testing and general use. Install AWS CLI: Install the AWS CLI on macOS Install the AWS CLI on Linux Install the AWS CLI on Windows In your command line or terminal run aws configure to configure your credentials. Note the user will require access to the IAM service. A best practice is to enforce the use of MFA, so if you misplace your AWS Management console password and/or access/secret key, there is nothing anyone can do without your MFA credentials. You can follow the instructions here to configure AWS CLI to assume a role with MFA enforced. 2. Playbook Run 2.1 Download Playbook and Helper Download the latest version of the notebook Incident_Response_Playbook_AWS_IAM.ipynb and helper incident_response_helpers.py from file from GitHub raw, or by cloning this repository. 2.2 Run the Playbook In your command line or terminal change directory to where you downloaded or cloned the notebook and helper. Enter jupyter notebook to start the local webserver, and connect to the url provided in the console e.g. The Jupyter Notebook is running at: , a web browser may automatically open to the correct url. Click on the Incident_Response_Playbook_AWS_IAM.ipynb file to execute the playbook. Follow the instructions in the playbook. 3. Knowledge Check The security best practices followed in this lab are: Analyze logs centrally Amazon CloudWatch is used to monitor, store, and access your log files. You can use AWS CloudWatch to analyze your logs centrally. Automate alerting on key indicators AWS CloudTrail, AWS Config,Amazon GuardDuty and Amazon VPC Flow Logs provide insights into your environment. Implement new security services and features: New features such as Amazon VPC Flow Logs have been adopted. Implement managed services: Managed services are utilized to increase your visibility and control of your environment. Identify Tooling Using the AWS Management Console and/or AWS CLI tools with prepared scripts will assist in your investigations. References & useful resources AWS CLI Command Reference AWS Identity and Access Management User Guide CloudWatch Logs Insights Query Syntax Jupyter License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/Lab_Guide.html#level-300-incident-response-playbook-with-jupyter-aws-iam","text":"","title":"Level 300: Incident Response Playbook with Jupyter - AWS IAM"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected Byron Pogson, Solutions Architect, AWS","title":"Authors"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/Lab_Guide.html#table-of-contents","text":"Prerequisites Playbook Run Knowledge Check","title":"Table of Contents"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/Lab_Guide.html#1-prerequisites","text":"","title":"1. Prerequisites "},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/Lab_Guide.html#11-install-python-and-modules","text":"Python 3 and a number of Python modules are required. Python downloads Installing pip may be required also. After installing Python, install the following packages by executing the following command in your command line or terminal: pip install boto3 pandas jupyter","title":"1.1 Install Python and Modules"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/Lab_Guide.html#12-install-the-aws-cli","text":"AWS CLI is not directly used for this lab, however it makes configuration of the AWS IAM credentials easier, and is useful for testing and general use. Install AWS CLI: Install the AWS CLI on macOS Install the AWS CLI on Linux Install the AWS CLI on Windows In your command line or terminal run aws configure to configure your credentials. Note the user will require access to the IAM service. A best practice is to enforce the use of MFA, so if you misplace your AWS Management console password and/or access/secret key, there is nothing anyone can do without your MFA credentials. You can follow the instructions here to configure AWS CLI to assume a role with MFA enforced.","title":"1.2 Install the AWS CLI"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/Lab_Guide.html#2-playbook-run","text":"","title":"2. Playbook Run "},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/Lab_Guide.html#21-download-playbook-and-helper","text":"Download the latest version of the notebook Incident_Response_Playbook_AWS_IAM.ipynb and helper incident_response_helpers.py from file from GitHub raw, or by cloning this repository.","title":"2.1 Download Playbook and Helper"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/Lab_Guide.html#22-run-the-playbook","text":"In your command line or terminal change directory to where you downloaded or cloned the notebook and helper. Enter jupyter notebook to start the local webserver, and connect to the url provided in the console e.g. The Jupyter Notebook is running at: , a web browser may automatically open to the correct url. Click on the Incident_Response_Playbook_AWS_IAM.ipynb file to execute the playbook. Follow the instructions in the playbook.","title":"2.2 Run the Playbook"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/Lab_Guide.html#3-knowledge-check","text":"The security best practices followed in this lab are: Analyze logs centrally Amazon CloudWatch is used to monitor, store, and access your log files. You can use AWS CloudWatch to analyze your logs centrally. Automate alerting on key indicators AWS CloudTrail, AWS Config,Amazon GuardDuty and Amazon VPC Flow Logs provide insights into your environment. Implement new security services and features: New features such as Amazon VPC Flow Logs have been adopted. Implement managed services: Managed services are utilized to increase your visibility and control of your environment. Identify Tooling Using the AWS Management Console and/or AWS CLI tools with prepared scripts will assist in your investigations.","title":"3. Knowledge Check "},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/Lab_Guide.html#references-useful-resources","text":"AWS CLI Command Reference AWS Identity and Access Management User Guide CloudWatch Logs Insights Query Syntax Jupyter","title":"References &amp; useful resources"},{"location":"Security/300_Incident_Response_Playbook_with_Jupyter-AWS_IAM/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2020 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html","text":"Level 300: Incident Response with AWS Console and CLI Introduction This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Identify tooling for incident response Automate containment for incident response Pre-deploy tools for incident response Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . CloudTrail must already be enabled in your account and logging to CloudWatch Logs, follow the Automated Deployment of Detective Controls lab to enable. Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#level-300-incident-response-with-aws-console-and-cli","text":"","title":"Level 300: Incident Response with AWS Console and CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#introduction","text":"This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#goals","text":"Identify tooling for incident response Automate containment for incident response Pre-deploy tools for incident response","title":"Goals"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user or role in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . CloudTrail must already be enabled in your account and logging to CloudWatch Logs, follow the Automated Deployment of Detective Controls lab to enable.","title":"Prerequisites"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html","text":"Level 300: Incident Response with AWS Console and CLI Authors Ben Potter, Security Lead, Well-Architected Table of Contents Getting Started Identity & Access Management Amazon VPC Knowledge Check 1. Getting Started 1.1 Install the AWS CLI Although instructions in this lab are written for both AWS Management console and AWS CLI, its best to install the AWS CLI on the machine you will be using as you can modify the example commands to run different scenarios easily and across multiple AWS accounts. Install the AWS CLI on macOS Install the AWS CLI on Linux Install the AWS CLI on Windows You will also need jq to parse json from the CLI: Install jq A best practice is to enforce the use of MFA, so if you misplace your AWS Management console password and/or access/secret key, there is nothing anyone can do without your MFA credentials. You can follow the instructions here to configure AWS CLI to assume a role with MFA enforced. 1.2 Amazon CloudWatch Logs Amazon CloudWatch Logs can be used to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Amazon Route 53, Amazon VPC Flow Logs, and other sources. It is a best practice to enable logging and analyze centrally, and develop investigation proceses. Using the AWS CLI and developing runbooks for investigation into different events can be significantly faster than using the console. If your logs are stored in Amazon S3 instead, you can use Amazon Athena to directly analyze data. To list the Amazon CloudWatch Logs Groups you have configured in each region, you can describe them. Note you must specify the region, if you need to query multiple regions you must run the command for each. You must use the region ID such as us-east-1 instead of the region name of US East (N. Virginia) that you see in the console. You can obtain a list of the regions by viewing them in the AWS Regions and Endpoints or using the CLI command: aws ec2 describe-regions . To list the log groups you have in a region, replace the example us-east-1 with your region: aws logs describe-log-groups --region us-east-1 The default output is json, and it will give you all details. If you want to list only the names in a table: aws logs describe-log-groups --output table --query 'logGroups[*].logGroupName' --region us-east-1 2. Identity & Access Management 2.1 Investigate AWS CloudTrail As AWS CloudTrail logs API activity for supported services , it provides an audit trail of your AWS account that you can use to track history of an adversary. For example, listing recent access denied attempts in AWS CloudTrail may indicate attempts to escalate privilege unsuccessfully. Note that some services such as Amazon S3 have their own logging, for example read more about Amazon S3 server access logging . You can enable AWS CloudTrail by following the Automated Deployment of Detective Controls lab. 2.1.1 AWS Console The AWS console provides a visual way of querying Amazon CloudWatch Logs, using CloudWatch Logs Insights and does not require any tools to be installed. Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region. From the left menu, choose Insights under Logs . From the dropdown near the top select your CloudTrail Logs group, then the relative time to search back on the right. Copy the following example queries below into the query input, then click Run query . IAM access denied attempts: To list all IAM access denied attempts you can use the following example. Each of the line item results allows you to drill down to reveal further details: filter errorCode like /Unauthorized|Denied|Forbidden/ | fields awsRegion, userIdentity.arn, eventSource, eventName, sourceIPAddress, userAgent IAM access key: If you need to search for what actions an access key has performed you can search for it e.g. AKIAIOSFODNN7EXAMPLE : filter userIdentity.accessKeyId =\"AKIAIOSFODNN7EXAMPLE\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent IAM source ip address: If you suspect a particular IP address as an adversary you can search such as 192.0.2.1 : filter sourceIPAddress = \"192.0.2.1\" | fields awsRegion, userIdentity.arn, eventSource, eventName, sourceIPAddress, userAgent IAM access key created An access key id will be part of the responseElements when its created so you can query that: filter responseElements.credentials.accessKeyId =\"AKIAIOSFODNN7EXAMPLE\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent IAM users and roles created Listing users and roles created can help identify unauthorized activity: filter eventName=\"CreateUser\" or eventName = \"CreateRole\" | fields requestParameters.userName, requestParameters.roleName, responseElements.user.arn, responseElements.role.arn, sourceIPAddress, eventTime, errorCode S3 List Buckets Listing buckets may indicate someone trying to gain access to your buckets. Note that Amazon S3 server access logging needs to be enabled on each bucket to gain further S3 access details: filter eventName =\"ListBuckets\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent 2.1.2 AWS CLI Remember you might need to update the --log-group-name , --region and/or --start-time parameter to a millisecond epoch start time of how far back you wish to search. You can use a web conversion tool such as www.epochconverter.com . IAM access denied attempts: To list all IAM access denied attempts you can use CloudWatch Logs with --filter-pattern parameter of AccessDenied for roles and Client.UnauthorizedOperation for users: aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern AccessDenied --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' IAM access key: If you need to search for what actions an access key has performed you can modify the --filter-pattern parameter to be the access key to search such as AKIAIOSFODNN7EXAMPLE : aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern AKIAIOSFODNN7EXAMPLE --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' IAM source ip address: If you suspect a particular IP address as an adversary you can modify the --filter-pattern parameter to be the IP address to search such as 192.0.2.1 : aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern 192.0.2.1 --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' S3 List Buckets Listing buckets may indicate someone trying to gain access to your buckets. Note that Amazon S3 server access logging needs to be enabled on each bucket to gain further S3 access details: aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern ListBuckets --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' 2.2 Block access in AWS IAM Blocking access to an IAM entity, that is a role, user or group can help when there is unauthorized activity as it will no longer be able to perform any actions. Be careful as blocking access may disrupt the operation of your workload, which is why it is important to practice in a non-production environment. Note that the AWS IAM entity may have created another entity, or other resources that may allow access to your account. You can use AWS CloudTrail that logs activity in your AWS account to determine the IAM entity that is performing the unauthorized operations. Additionally service last accessed data in the AWS Console can help you audit permissions. 2.3 List AWS IAM roles/users/groups If you need to confirm the name of a role, user or group you can list: 2.3.1 AWS Console Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left, the role will be displayed and you can use the search field. 2.3.2 AWS CLI aws iam list-roles This provides a full json formatted list of all roles, if you only want to display the RoleName use an output of table and query: aws iam list-roles --output table --query 'Roles[*].RoleName' List all users: aws iam list-users --output table --query 'Users[*].UserName' List all groups: aws iam list-groups --output table --query 'Groups[*].GroupName' 2.4 Attach inline deny policy Attaching an explicit deny policy to an AWS IAM role, user or group will quickly block ALL access for that entity which is useful if it is performing unauthorized operations. Please note that the role will still be able to call the sts API to obtain information on itself, e.g. using get-caller-identity will return the account ID, user ID and ARN. 2.4.1 AWS Console Sign in to the AWS Management Console as an AWS IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click either Groups , Users or Roles on the left, then click the name to modify. Click Permissions tab. Click Add inline policy . Click the JSON tab then replace the example with the following: { \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] } Click Review policy . Enter Name of DenyAll then click Create policy . Note that the console may incorrectly display the access level. 2.4.2 AWS CLI Block a role, modify ROLENAME to match your role name: aws iam put-role-policy --role-name ROLENAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }' Block a user, modify USERNAME to match your user name: aws iam put-user-policy --user-name USERNAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }' Block a group, modify GROUPNAME to match your user name: aws iam put-group-policy --group-name GROUPNAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }' 2.5 Delete inline deny policy To delete the policy you just attached and restore the original permissions the entity had: 2.5.1 AWS Console Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left. Click the checkbox next to the role to delete. Click Delete role . Confirm the role to delete then click Yes, delete 2.5.2 AWS CLI Delete policy from a role: aws iam delete-role-policy --role-name ROLENAME --policy-name DenyAll Delete policy from a user: aws iam delete-user-policy --user-name USERNAME --policy-name DenyAll Delete policy from a group: aws iam delete-group-policy --group-name GROUPNAME --policy-name DenyAll 3. Amazon VPC A Amazon VPC that has VPC Flow Logs enabled captures information about the IP traffic going to and from network interfaces in your Amazon VPC. This log information may help you investigate how Amazon EC2 instances and other resources in your VPC are communicating, and what they are communicating with. You can follow the Automated Deployment of VPC lab for creating a Amazon VPC with Flow Logs enabled. 3.1 Investigate Amazon VPC Flow Logs 3.1.1 AWS Management Console The AWS Management console provides a visual way of querying CloudWatch Logs, using CloudWatch Logs Insights and does not require any tools to be installed. Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region. From the left menu, choose Insights under Logs . From the dropdown near the top select your CloudTrail Logs group, then the relative time to search back on the right. Copy the following example queries below into the query input, then click Run query . Rejected requests by IP address: Rejected requests indicate attempts to gain access to your VPC, however there can often be noise from internet scanners. To count the rejected requests by source IP address: filter action=\"REJECT\" | stats count(*) as numRejections by srcAddr | sort numRejections desc Reject requests originating from inside your VPC Rejected requests that originate from inside your VPC may indicate your infrastructure in your VPC is attempting to connect to something it is not allowed to, e.g. a database instance is trying to connect to the internet and is blocked. This example uses regex to match the start of your VPC as 10. : filter action=\"REJECT\" and srcAddr like /^10\\./ | stats count(*) as numRejections by srcAddr | sort numRejections desc Requests from an IP address If you suspect an IP address and want to list all requests that originate, replace 192.0.2.1 with the IP you suspect: filter srcAddr = \"192.0.2.1\" | fields @timestamp, interfaceId, dstAddr, dstPort, action Request count from a private IP address by destination address If you want to list and count all connections by a private IP address, replace 10.1.1.1 with your private IP: filter srcAddr = \"10.1.1.1\" | stats count(*) as numConnections by dstAddr | sort numConnections desc 4. Knowledge Check The security best practices followed in this lab are: Analyze logs centrally Amazon CloudWatch is used to monitor, store, and access your log files. You can use AWS CloudWatch to analyze your logs centrally. Automate alerting on key indicators AWS CloudTrail, AWS Config,Amazon GuardDuty and Amazon VPC Flow Logs provide insights into your environment. Implement new security services and features: New features such as Amazon VPC Flow Logs have been adopted. Implement managed services: Managed services are utilized to increase your visibility and control of your environment. Identify Tooling Using the AWS Management Console and/or AWS CLI tools with prepared scripts will assist in your investigations. References & useful resources AWS CLI Command Reference AWS Identity and Access Management User Guide CloudWatch Logs Insights Query Syntax License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#level-300-incident-response-with-aws-console-and-cli","text":"","title":"Level 300: Incident Response with AWS Console and CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#table-of-contents","text":"Getting Started Identity & Access Management Amazon VPC Knowledge Check","title":"Table of Contents"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#1-getting-started","text":"","title":"1. Getting Started "},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#11-install-the-aws-cli","text":"Although instructions in this lab are written for both AWS Management console and AWS CLI, its best to install the AWS CLI on the machine you will be using as you can modify the example commands to run different scenarios easily and across multiple AWS accounts. Install the AWS CLI on macOS Install the AWS CLI on Linux Install the AWS CLI on Windows You will also need jq to parse json from the CLI: Install jq A best practice is to enforce the use of MFA, so if you misplace your AWS Management console password and/or access/secret key, there is nothing anyone can do without your MFA credentials. You can follow the instructions here to configure AWS CLI to assume a role with MFA enforced.","title":"1.1 Install the AWS CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#12-amazon-cloudwatch-logs","text":"Amazon CloudWatch Logs can be used to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Amazon Route 53, Amazon VPC Flow Logs, and other sources. It is a best practice to enable logging and analyze centrally, and develop investigation proceses. Using the AWS CLI and developing runbooks for investigation into different events can be significantly faster than using the console. If your logs are stored in Amazon S3 instead, you can use Amazon Athena to directly analyze data. To list the Amazon CloudWatch Logs Groups you have configured in each region, you can describe them. Note you must specify the region, if you need to query multiple regions you must run the command for each. You must use the region ID such as us-east-1 instead of the region name of US East (N. Virginia) that you see in the console. You can obtain a list of the regions by viewing them in the AWS Regions and Endpoints or using the CLI command: aws ec2 describe-regions . To list the log groups you have in a region, replace the example us-east-1 with your region: aws logs describe-log-groups --region us-east-1 The default output is json, and it will give you all details. If you want to list only the names in a table: aws logs describe-log-groups --output table --query 'logGroups[*].logGroupName' --region us-east-1","title":"1.2 Amazon CloudWatch Logs"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#2-identity-access-management","text":"","title":"2. Identity &amp; Access Management "},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#21-investigate-aws-cloudtrail","text":"As AWS CloudTrail logs API activity for supported services , it provides an audit trail of your AWS account that you can use to track history of an adversary. For example, listing recent access denied attempts in AWS CloudTrail may indicate attempts to escalate privilege unsuccessfully. Note that some services such as Amazon S3 have their own logging, for example read more about Amazon S3 server access logging . You can enable AWS CloudTrail by following the Automated Deployment of Detective Controls lab.","title":"2.1 Investigate AWS CloudTrail"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#211-aws-console","text":"The AWS console provides a visual way of querying Amazon CloudWatch Logs, using CloudWatch Logs Insights and does not require any tools to be installed. Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region. From the left menu, choose Insights under Logs . From the dropdown near the top select your CloudTrail Logs group, then the relative time to search back on the right. Copy the following example queries below into the query input, then click Run query . IAM access denied attempts: To list all IAM access denied attempts you can use the following example. Each of the line item results allows you to drill down to reveal further details: filter errorCode like /Unauthorized|Denied|Forbidden/ | fields awsRegion, userIdentity.arn, eventSource, eventName, sourceIPAddress, userAgent IAM access key: If you need to search for what actions an access key has performed you can search for it e.g. AKIAIOSFODNN7EXAMPLE : filter userIdentity.accessKeyId =\"AKIAIOSFODNN7EXAMPLE\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent IAM source ip address: If you suspect a particular IP address as an adversary you can search such as 192.0.2.1 : filter sourceIPAddress = \"192.0.2.1\" | fields awsRegion, userIdentity.arn, eventSource, eventName, sourceIPAddress, userAgent IAM access key created An access key id will be part of the responseElements when its created so you can query that: filter responseElements.credentials.accessKeyId =\"AKIAIOSFODNN7EXAMPLE\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent IAM users and roles created Listing users and roles created can help identify unauthorized activity: filter eventName=\"CreateUser\" or eventName = \"CreateRole\" | fields requestParameters.userName, requestParameters.roleName, responseElements.user.arn, responseElements.role.arn, sourceIPAddress, eventTime, errorCode S3 List Buckets Listing buckets may indicate someone trying to gain access to your buckets. Note that Amazon S3 server access logging needs to be enabled on each bucket to gain further S3 access details: filter eventName =\"ListBuckets\" | fields awsRegion, eventSource, eventName, sourceIPAddress, userAgent","title":"2.1.1 AWS Console"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#212-aws-cli","text":"Remember you might need to update the --log-group-name , --region and/or --start-time parameter to a millisecond epoch start time of how far back you wish to search. You can use a web conversion tool such as www.epochconverter.com . IAM access denied attempts: To list all IAM access denied attempts you can use CloudWatch Logs with --filter-pattern parameter of AccessDenied for roles and Client.UnauthorizedOperation for users: aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern AccessDenied --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' IAM access key: If you need to search for what actions an access key has performed you can modify the --filter-pattern parameter to be the access key to search such as AKIAIOSFODNN7EXAMPLE : aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern AKIAIOSFODNN7EXAMPLE --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' IAM source ip address: If you suspect a particular IP address as an adversary you can modify the --filter-pattern parameter to be the IP address to search such as 192.0.2.1 : aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern 192.0.2.1 --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements' S3 List Buckets Listing buckets may indicate someone trying to gain access to your buckets. Note that Amazon S3 server access logging needs to be enabled on each bucket to gain further S3 access details: aws logs filter-log-events --region us-east-1 --start-time 1551402000000 --log-group-name CloudTrail/DefaultLogGroup --filter-pattern ListBuckets --output json --query 'events[*].message'| jq -r '.[] | fromjson | .userIdentity, .sourceIPAddress, .responseElements'","title":"2.1.2 AWS CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#22-block-access-in-aws-iam","text":"Blocking access to an IAM entity, that is a role, user or group can help when there is unauthorized activity as it will no longer be able to perform any actions. Be careful as blocking access may disrupt the operation of your workload, which is why it is important to practice in a non-production environment. Note that the AWS IAM entity may have created another entity, or other resources that may allow access to your account. You can use AWS CloudTrail that logs activity in your AWS account to determine the IAM entity that is performing the unauthorized operations. Additionally service last accessed data in the AWS Console can help you audit permissions.","title":"2.2 Block access in AWS IAM"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#23-list-aws-iam-rolesusersgroups","text":"If you need to confirm the name of a role, user or group you can list:","title":"2.3 List AWS IAM roles/users/groups"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#231-aws-console","text":"Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left, the role will be displayed and you can use the search field.","title":"2.3.1 AWS Console"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#232-aws-cli","text":"aws iam list-roles This provides a full json formatted list of all roles, if you only want to display the RoleName use an output of table and query: aws iam list-roles --output table --query 'Roles[*].RoleName' List all users: aws iam list-users --output table --query 'Users[*].UserName' List all groups: aws iam list-groups --output table --query 'Groups[*].GroupName'","title":"2.3.2 AWS CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#24-attach-inline-deny-policy","text":"Attaching an explicit deny policy to an AWS IAM role, user or group will quickly block ALL access for that entity which is useful if it is performing unauthorized operations. Please note that the role will still be able to call the sts API to obtain information on itself, e.g. using get-caller-identity will return the account ID, user ID and ARN.","title":"2.4 Attach inline deny policy"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#241-aws-console","text":"Sign in to the AWS Management Console as an AWS IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click either Groups , Users or Roles on the left, then click the name to modify. Click Permissions tab. Click Add inline policy . Click the JSON tab then replace the example with the following: { \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] } Click Review policy . Enter Name of DenyAll then click Create policy . Note that the console may incorrectly display the access level.","title":"2.4.1 AWS Console"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#242-aws-cli","text":"Block a role, modify ROLENAME to match your role name: aws iam put-role-policy --role-name ROLENAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }' Block a user, modify USERNAME to match your user name: aws iam put-user-policy --user-name USERNAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }' Block a group, modify GROUPNAME to match your user name: aws iam put-group-policy --group-name GROUPNAME --policy-name DenyAll --policy-document '{ \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"*\", \"Resource\": \"*\" } ] }'","title":"2.4.2 AWS CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#25-delete-inline-deny-policy","text":"To delete the policy you just attached and restore the original permissions the entity had:","title":"2.5 Delete inline deny policy"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#251-aws-console","text":"Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the AWS IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left. Click the checkbox next to the role to delete. Click Delete role . Confirm the role to delete then click Yes, delete","title":"2.5.1 AWS Console"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#252-aws-cli","text":"Delete policy from a role: aws iam delete-role-policy --role-name ROLENAME --policy-name DenyAll Delete policy from a user: aws iam delete-user-policy --user-name USERNAME --policy-name DenyAll Delete policy from a group: aws iam delete-group-policy --group-name GROUPNAME --policy-name DenyAll","title":"2.5.2 AWS CLI"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#3-amazon-vpc","text":"A Amazon VPC that has VPC Flow Logs enabled captures information about the IP traffic going to and from network interfaces in your Amazon VPC. This log information may help you investigate how Amazon EC2 instances and other resources in your VPC are communicating, and what they are communicating with. You can follow the Automated Deployment of VPC lab for creating a Amazon VPC with Flow Logs enabled.","title":"3. Amazon VPC "},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#31-investigate-amazon-vpc-flow-logs","text":"","title":"3.1 Investigate Amazon VPC Flow Logs"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#311-aws-management-console","text":"The AWS Management console provides a visual way of querying CloudWatch Logs, using CloudWatch Logs Insights and does not require any tools to be installed. Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region. From the left menu, choose Insights under Logs . From the dropdown near the top select your CloudTrail Logs group, then the relative time to search back on the right. Copy the following example queries below into the query input, then click Run query . Rejected requests by IP address: Rejected requests indicate attempts to gain access to your VPC, however there can often be noise from internet scanners. To count the rejected requests by source IP address: filter action=\"REJECT\" | stats count(*) as numRejections by srcAddr | sort numRejections desc Reject requests originating from inside your VPC Rejected requests that originate from inside your VPC may indicate your infrastructure in your VPC is attempting to connect to something it is not allowed to, e.g. a database instance is trying to connect to the internet and is blocked. This example uses regex to match the start of your VPC as 10. : filter action=\"REJECT\" and srcAddr like /^10\\./ | stats count(*) as numRejections by srcAddr | sort numRejections desc Requests from an IP address If you suspect an IP address and want to list all requests that originate, replace 192.0.2.1 with the IP you suspect: filter srcAddr = \"192.0.2.1\" | fields @timestamp, interfaceId, dstAddr, dstPort, action Request count from a private IP address by destination address If you want to list and count all connections by a private IP address, replace 10.1.1.1 with your private IP: filter srcAddr = \"10.1.1.1\" | stats count(*) as numConnections by dstAddr | sort numConnections desc","title":"3.1.1 AWS Management Console"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#4-knowledge-check","text":"The security best practices followed in this lab are: Analyze logs centrally Amazon CloudWatch is used to monitor, store, and access your log files. You can use AWS CloudWatch to analyze your logs centrally. Automate alerting on key indicators AWS CloudTrail, AWS Config,Amazon GuardDuty and Amazon VPC Flow Logs provide insights into your environment. Implement new security services and features: New features such as Amazon VPC Flow Logs have been adopted. Implement managed services: Managed services are utilized to increase your visibility and control of your environment. Identify Tooling Using the AWS Management Console and/or AWS CLI tools with prepared scripts will assist in your investigations.","title":"4. Knowledge Check "},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#references-useful-resources","text":"AWS CLI Command Reference AWS Identity and Access Management User Guide CloudWatch Logs Insights Query Syntax","title":"References &amp; useful resources"},{"location":"Security/300_Incident_Response_with_AWS_Console_and_CLI/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/README.html","text":"Level 300: Lambda Cross Account Using Bucket Policy Introduction This lab demonstrates configuration of an S3 bucket policy (which is a type of resource based policy) in AWS account 2 (the destination) that enables a Lambda function in AWS account 1 (the origin) to list the objects in that bucket using Python boto SDK. If you only have 1 AWS account simply repeat the instructions in that account and use the same account id. If in classroom and you do not have 2 AWS accounts, buddy up to use each other's accounts, agree who will be account #1 and who will be account #2. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals S3 bucket policies Resource based policies versus identity based policies Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/README.html#level-300-lambda-cross-account-using-bucket-policy","text":"","title":"Level 300: Lambda Cross Account Using Bucket Policy"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/README.html#introduction","text":"This lab demonstrates configuration of an S3 bucket policy (which is a type of resource based policy) in AWS account 2 (the destination) that enables a Lambda function in AWS account 1 (the origin) to list the objects in that bucket using Python boto SDK. If you only have 1 AWS account simply repeat the instructions in that account and use the same account id. If in classroom and you do not have 2 AWS accounts, buddy up to use each other's accounts, agree who will be account #1 and who will be account #2. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/README.html#goals","text":"S3 bucket policies Resource based policies versus identity based policies","title":"Goals"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html","text":"Level 300: Lambda Cross Account Using Bucket Policy Authors Seth Eliot, Resiliency Lead, Well-Architected, AWS Table of Contents Identify (or create) S3 bucket in account 2 Create role for Lambda in account 1 Create bucket policy for the S3 bucket in account 2 Create Lambda in account 1 Tear Down This lab is best run using two AWS accounts Identify the AWS account number for account 1 (no dashes) Identify the AWS account number for account 2 (no dashes) If you only have one AWS account, then use the same AWS account number for both account1 and account2 1. Identify (or create) S3 bucket in account 2 In account 2 sign in to the S3 Management Console as an IAM user or role in your AWS account, and open the S3 console at https://console.aws.amazon.com/s3 Choose an S3 bucket that contains some objects. You will enable the ability to list the objects in this bucket from the other account. If you would rather create a new bucket to use, follow these directions Record the bucketname 2. Create role for Lambda in account 1 In account 1 sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ Click Roles on the left, then create role AWS service will be pre-selected, select Lambda , then click Next: Permissions Do not select any managed policies, click Next: Tags Click Next: Review Enter Lambda-List-S3-Role for the Role name then click Create role From the list of roles click the name of Lambda-List-S3-Role Click Add inline policy , then click JSON tab Replace the sample json with the following Replace account1 with the AWS Account number (no dashes) of account 1 Replace bucketname with the S3 bucket name from account 2 Then click Review Policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"S3ListBucket\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\" ], \"Resource\": \"arn:aws:s3:::bucketname\" }, { \"Sid\": \"logsstreamevent\", \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogStream\", \"logs:PutLogEvents\" ], \"Resource\": \"arn:aws:logs:us-east-1:account1:log-group:/aws/lambda/Lambda-List-S3*/*\" }, { \"Sid\": \"logsgroup\", \"Effect\": \"Allow\", \"Action\": \"logs:CreateLogGroup\", \"Resource\": \"*\" } ] } Name this policy Lambda-List-S3-Policy , then click Create policy 3. Create bucket policy for the S3 bucket in account 2 In account 2 sign in to the S3 Management Console as an IAM user or role in your AWS account, and open the S3 console at https://console.aws.amazon.com/s3 Click on the name of the bucket you will use for this workshop Go to the Permissions tab Click Bucket Policy Enter the following JSON policy Replace account1 with the AWS Account number (no dashes) of account 1 Replace bucketname with the S3 bucket name from account 2 Note: This policy uses least privilege. Only resources using the IAM role from account 1 will have access { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1565731301209\", \"Action\": [ \"s3:ListBucket\" ], \"Effect\": \"Allow\", \"Resource\": \"arn:aws:s3:::bucketname\", \"Principal\": { \"AWS\":\"arn:aws:iam::account1:role/Lambda-List-S3-Role\" }, \"Condition\": { \"StringLike\": { \"aws:UserAgent\": \"*AWS_Lambda_python*\" } } } ] } Click Save 4. Create Lambda in account 1 Open the Lambda console https://console.aws.amazon.com/lambda Click Create a function Accept the default Author from scratch Enter function name as Lambda-List-S3 Select Python 3.7 runtime Expand Permissions, click Use an existing role , then select the Lambda-List-S3-Role Click Create function Replace the example function code with the following Replace bucketname with the S3 bucket name from account 2 import json import boto3 import os import uuid def lambda_handler(event, context): try: # Create an S3 client s3 = boto3.client('s3') # Call S3 to list current buckets objlist = s3.list_objects( Bucket='bucketname', MaxKeys = 10) print (objlist['Contents']) return str(objlist['Contents']) except Exception as e: print(e) raise e Click Save . Click Test , accept the default event template, enter an event name for the test, then click Create Click Test again, and in a few seconds the function output should highlight green and you can expand the detail to see the response from the S3 API 5. Tear down this lab Remove the lambda function, then roles If you created a new S3 bucket, then you may remove it References & useful resources https://docs.aws.amazon.com/AmazonS3/latest/dev/using-iam-policies.html https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#level-300-lambda-cross-account-using-bucket-policy","text":"","title":"Level 300: Lambda Cross Account Using Bucket Policy"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#authors","text":"Seth Eliot, Resiliency Lead, Well-Architected, AWS","title":"Authors"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#table-of-contents","text":"Identify (or create) S3 bucket in account 2 Create role for Lambda in account 1 Create bucket policy for the S3 bucket in account 2 Create Lambda in account 1 Tear Down This lab is best run using two AWS accounts Identify the AWS account number for account 1 (no dashes) Identify the AWS account number for account 2 (no dashes) If you only have one AWS account, then use the same AWS account number for both account1 and account2","title":"Table of Contents"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#1-identify-or-create-s3-bucket-in-account-2","text":"In account 2 sign in to the S3 Management Console as an IAM user or role in your AWS account, and open the S3 console at https://console.aws.amazon.com/s3 Choose an S3 bucket that contains some objects. You will enable the ability to list the objects in this bucket from the other account. If you would rather create a new bucket to use, follow these directions Record the bucketname","title":"1. Identify (or create) S3 bucket in account 2 "},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#2-create-role-for-lambda-in-account-1","text":"In account 1 sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ Click Roles on the left, then create role AWS service will be pre-selected, select Lambda , then click Next: Permissions Do not select any managed policies, click Next: Tags Click Next: Review Enter Lambda-List-S3-Role for the Role name then click Create role From the list of roles click the name of Lambda-List-S3-Role Click Add inline policy , then click JSON tab Replace the sample json with the following Replace account1 with the AWS Account number (no dashes) of account 1 Replace bucketname with the S3 bucket name from account 2 Then click Review Policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"S3ListBucket\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\" ], \"Resource\": \"arn:aws:s3:::bucketname\" }, { \"Sid\": \"logsstreamevent\", \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogStream\", \"logs:PutLogEvents\" ], \"Resource\": \"arn:aws:logs:us-east-1:account1:log-group:/aws/lambda/Lambda-List-S3*/*\" }, { \"Sid\": \"logsgroup\", \"Effect\": \"Allow\", \"Action\": \"logs:CreateLogGroup\", \"Resource\": \"*\" } ] } Name this policy Lambda-List-S3-Policy , then click Create policy","title":"2. Create role for Lambda in account 1 "},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#3-create-bucket-policy-for-the-s3-bucket-in-account-2","text":"In account 2 sign in to the S3 Management Console as an IAM user or role in your AWS account, and open the S3 console at https://console.aws.amazon.com/s3 Click on the name of the bucket you will use for this workshop Go to the Permissions tab Click Bucket Policy Enter the following JSON policy Replace account1 with the AWS Account number (no dashes) of account 1 Replace bucketname with the S3 bucket name from account 2 Note: This policy uses least privilege. Only resources using the IAM role from account 1 will have access { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Stmt1565731301209\", \"Action\": [ \"s3:ListBucket\" ], \"Effect\": \"Allow\", \"Resource\": \"arn:aws:s3:::bucketname\", \"Principal\": { \"AWS\":\"arn:aws:iam::account1:role/Lambda-List-S3-Role\" }, \"Condition\": { \"StringLike\": { \"aws:UserAgent\": \"*AWS_Lambda_python*\" } } } ] } Click Save","title":"3. Create bucket policy for the S3 bucket in account 2 "},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#4-create-lambda-in-account-1","text":"Open the Lambda console https://console.aws.amazon.com/lambda Click Create a function Accept the default Author from scratch Enter function name as Lambda-List-S3 Select Python 3.7 runtime Expand Permissions, click Use an existing role , then select the Lambda-List-S3-Role Click Create function Replace the example function code with the following Replace bucketname with the S3 bucket name from account 2 import json import boto3 import os import uuid def lambda_handler(event, context): try: # Create an S3 client s3 = boto3.client('s3') # Call S3 to list current buckets objlist = s3.list_objects( Bucket='bucketname', MaxKeys = 10) print (objlist['Contents']) return str(objlist['Contents']) except Exception as e: print(e) raise e Click Save . Click Test , accept the default event template, enter an event name for the test, then click Create Click Test again, and in a few seconds the function output should highlight green and you can expand the detail to see the response from the S3 API","title":"4. Create Lambda in account 1 "},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#5-tear-down-this-lab","text":"Remove the lambda function, then roles If you created a new S3 bucket, then you may remove it","title":"5. Tear down this lab "},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#references-useful-resources","text":"https://docs.aws.amazon.com/AmazonS3/latest/dev/using-iam-policies.html https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_identity-vs-resource.html","title":"References &amp; useful resources"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Documentation/CreateNewS3Bucket.html","text":"Create New S3 Bucket These steps will guide you to create a bucket containing some objects. Go to the S3 console at https://console.aws.amazon.com/s3 Click Create bucket For Bucket name supply a name. This must be unique across all buckets in AWS Tip : Name the bucket <your_first_name>_<date in yyyymmm10 format> Click Next three times Review screen: click Create bucket Click on the name of the bucket you created Drag some files into the object upload area Click Next three times Click Upload Click here to return to the Lab Guide","title":"Create New S3 Bucket"},{"location":"Security/300_Lambda_Cross_Account_Bucket_Policy/Documentation/CreateNewS3Bucket.html#create-new-s3-bucket","text":"These steps will guide you to create a bucket containing some objects. Go to the S3 console at https://console.aws.amazon.com/s3 Click Create bucket For Bucket name supply a name. This must be unique across all buckets in AWS Tip : Name the bucket <your_first_name>_<date in yyyymmm10 format> Click Next three times Review screen: click Create bucket Click on the name of the bucket you created Drag some files into the object upload area Click Next three times Click Upload Click here to return to the Lab Guide","title":"Create New S3 Bucket"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/README.html","text":"Level 300: Lambda Cross Account IAM Role Assumption Introduction This lab demonstrates a Lambda function in AWS account 1 (the origin) using Python boto SDK to assume an IAM role in account 2 (the destination), then list the buckets. If you only have 1 AWS account simply repeat the instructions in that account and use the same account id. If in classroom and you do not have 2 AWS accounts, buddy up to use each other's accounts, agree who will be account #1 and who will be account #2. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Goals Cross account role assumption Lambda assuming another role Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Permissions required IAM User with AdministratorAccess AWS managed policy Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/README.html#level-300-lambda-cross-account-iam-role-assumption","text":"","title":"Level 300: Lambda Cross Account IAM Role Assumption"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/README.html#introduction","text":"This lab demonstrates a Lambda function in AWS account 1 (the origin) using Python boto SDK to assume an IAM role in account 2 (the destination), then list the buckets. If you only have 1 AWS account simply repeat the instructions in that account and use the same account id. If in classroom and you do not have 2 AWS accounts, buddy up to use each other's accounts, agree who will be account #1 and who will be account #2. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"Introduction"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/README.html#goals","text":"Cross account role assumption Lambda assuming another role","title":"Goals"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. An IAM user with MFA enabled that can assume roles in your AWS account. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/README.html#permissions-required","text":"IAM User with AdministratorAccess AWS managed policy","title":"Permissions required"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html","text":"Level 300: Lambda Cross Account IAM Role Assumption Authors Ben Potter, Security Lead, Well-Architected Table of Contents Create role for Lambda in account 2 Create role for Lambda in account 1 Create Lambda in account 1 Tear Down 1. Create role for Lambda in account 2 Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left, then create role. Click Another AWS account, enter the account id for account 1 (the origin), then click Next: Permissions. Do not select any managed policies, click Next: Tags. Click Next: Review. Enter LambdaS3ListBuckets for the Role name then click Create role. From the list of roles click the name of LambdaS3ListBuckets. Copy the Role ARN and store for use later in this lab. Click Add inline policy, then click JSON tab. Replace the sample json with the following, then click Review Policy. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"S3ListAllMyBuckets\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListAllMyBuckets\" ], \"Resource\": \"*\" } ] } Name this policy LambdaS3ListBucketsPolicy, then click Create policy. 2. Create role for Lambda in account 1 Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left, then create role. AWS service will be pre-selected, select Lambda, then click Next: Permissions. Do not select any managed policies, click Next: Tags. Click Next: Review. Enter Lambda-Assume-Roles for the Role name then click Create role. From the list of roles click the name of Lambda-Assume-Roles. Copy the Role ARN and store for use later in this lab. Click Add inline policy, then click JSON tab. Replace the sample json with the following, replacing account1 and account2 with your respective account id's, us-east-1 region with the region you are using, then click Review Policy. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"stsassumerole\", \"Effect\": \"Allow\", \"Action\": \"sts:AssumeRole\", \"Resource\": \"arn:aws:iam::account2:role/LambdaS3ListBuckets\", \"Condition\": { \"StringLike\": { \"aws:UserAgent\": \"*AWS_Lambda_python*\" } } }, { \"Sid\": \"logsstreamevent\", \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogStream\", \"logs:PutLogEvents\" ], \"Resource\": \"arn:aws:logs:us-east-1:account1:log-group:/aws/lambda/Lambda-Assume-Roles*/*\" }, { \"Sid\": \"logsgroup\", \"Effect\": \"Allow\", \"Action\": \"logs:CreateLogGroup\", \"Resource\": \"*\" } ] } Name this policy Lambda-Assume-Roles-Policy, then click Create policy. 3. Create Lambda in account 1 Open the Lambda console. Click Create a function. Accept the default Author from scratch. Enter function name as Lambda-Assume-Roles. Select Python 3.6 runtime. Expand Permissions, click Use an existing role, then select the Lambda-Assume-Roles role. Click Create function. Replace the example function code with the following, replacing the RoleArn with the one from account 2 you created previously. import json import boto3 import os import uuid def lambda_handler(event, context): try: client = boto3.client('sts') response = client.assume_role(RoleArn='arn:aws:iam::account2:role/LambdaS3ListBuckets',RoleSessionName=\"{}-s3\".format(str(uuid.uuid4())[:5])) session = boto3.Session(aws_access_key_id=response['Credentials']['AccessKeyId'],aws_secret_access_key=response['Credentials']['SecretAccessKey'],aws_session_token=response['Credentials']['SessionToken']) s3 = session.client('s3') s3list = s3.list_buckets() print (s3list) return str(s3list['Buckets']) except Exception as e: print(e) raise e Click Save. Click Test, accept the default event template, enter event name of test, then click Create. Click Test again, and in a few seconds the function output should highlight green and you can expand the detail to see the response from the S3 API. How could the example policies be improved? 4. Tear down this lab Remove the lambda function, then roles. References & useful resources https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#level-300-lambda-cross-account-iam-role-assumption","text":"","title":"Level 300: Lambda Cross Account IAM Role Assumption"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#table-of-contents","text":"Create role for Lambda in account 2 Create role for Lambda in account 1 Create Lambda in account 1 Tear Down","title":"Table of Contents"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#1-create-role-for-lambda-in-account-2","text":"Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left, then create role. Click Another AWS account, enter the account id for account 1 (the origin), then click Next: Permissions. Do not select any managed policies, click Next: Tags. Click Next: Review. Enter LambdaS3ListBuckets for the Role name then click Create role. From the list of roles click the name of LambdaS3ListBuckets. Copy the Role ARN and store for use later in this lab. Click Add inline policy, then click JSON tab. Replace the sample json with the following, then click Review Policy. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"S3ListAllMyBuckets\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListAllMyBuckets\" ], \"Resource\": \"*\" } ] } Name this policy LambdaS3ListBucketsPolicy, then click Create policy.","title":"1. Create role for Lambda in account 2 "},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#2-create-role-for-lambda-in-account-1","text":"Sign in to the AWS Management Console as an IAM user or role in your AWS account, and open the IAM console at https://console.aws.amazon.com/iam/ . Click Roles on the left, then create role. AWS service will be pre-selected, select Lambda, then click Next: Permissions. Do not select any managed policies, click Next: Tags. Click Next: Review. Enter Lambda-Assume-Roles for the Role name then click Create role. From the list of roles click the name of Lambda-Assume-Roles. Copy the Role ARN and store for use later in this lab. Click Add inline policy, then click JSON tab. Replace the sample json with the following, replacing account1 and account2 with your respective account id's, us-east-1 region with the region you are using, then click Review Policy. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"stsassumerole\", \"Effect\": \"Allow\", \"Action\": \"sts:AssumeRole\", \"Resource\": \"arn:aws:iam::account2:role/LambdaS3ListBuckets\", \"Condition\": { \"StringLike\": { \"aws:UserAgent\": \"*AWS_Lambda_python*\" } } }, { \"Sid\": \"logsstreamevent\", \"Effect\": \"Allow\", \"Action\": [ \"logs:CreateLogStream\", \"logs:PutLogEvents\" ], \"Resource\": \"arn:aws:logs:us-east-1:account1:log-group:/aws/lambda/Lambda-Assume-Roles*/*\" }, { \"Sid\": \"logsgroup\", \"Effect\": \"Allow\", \"Action\": \"logs:CreateLogGroup\", \"Resource\": \"*\" } ] } Name this policy Lambda-Assume-Roles-Policy, then click Create policy.","title":"2. Create role for Lambda in account 1 "},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#3-create-lambda-in-account-1","text":"Open the Lambda console. Click Create a function. Accept the default Author from scratch. Enter function name as Lambda-Assume-Roles. Select Python 3.6 runtime. Expand Permissions, click Use an existing role, then select the Lambda-Assume-Roles role. Click Create function. Replace the example function code with the following, replacing the RoleArn with the one from account 2 you created previously. import json import boto3 import os import uuid def lambda_handler(event, context): try: client = boto3.client('sts') response = client.assume_role(RoleArn='arn:aws:iam::account2:role/LambdaS3ListBuckets',RoleSessionName=\"{}-s3\".format(str(uuid.uuid4())[:5])) session = boto3.Session(aws_access_key_id=response['Credentials']['AccessKeyId'],aws_secret_access_key=response['Credentials']['SecretAccessKey'],aws_session_token=response['Credentials']['SessionToken']) s3 = session.client('s3') s3list = s3.list_buckets() print (s3list) return str(s3list['Buckets']) except Exception as e: print(e) raise e Click Save. Click Test, accept the default event template, enter event name of test, then click Create. Click Test again, and in a few seconds the function output should highlight green and you can expand the detail to see the response from the S3 API. How could the example policies be improved?","title":"3. Create Lambda in account 1 "},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#4-tear-down-this-lab","text":"Remove the lambda function, then roles.","title":"4. Tear down this lab "},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#references-useful-resources","text":"https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html","title":"References &amp; useful resources"},{"location":"Security/300_Lambda_Cross_Account_IAM_Role_Assumption/Lab_Guide.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html","text":"Quest: Loft - Introduction to Security About this Guide This quest is the guide for an AWS Loft Well-Architected Security introduction workshop. You can check your local loft schedule for upcoming Well-Architected events, or you can also run it on your own! The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Step 1 - New AWS Account Setup and Securing Root User Walkthrough This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. AWS Account and Root User Further Considerations Federate Identity Using SAML: Leveraging a SAML provider Separate production, non-production and different workloads using different AWS accounts: AWS Multiple Account Billing Strategy Step 2 - Basic Identity and Access Management User, Group, Role This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. Walkthrough Basic Identity and Access Management User, Group, Role Step 3 - CloudFront with WAF Protection This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. As CloudFront takes some time to update configuration in all edge locations, consider starting step 4 while its deploying. Walkthrough CloudFront with WAF Protection Step 4 - Automated Deployment of Detective Controls This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of AWS CloudTrail. Walkthrough Only complete step 2, GuardDuty from: Automated Deployment of Detective Controls License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Introduction to Security"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#quest-loft-introduction-to-security","text":"","title":"Quest: Loft - Introduction to Security"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#about-this-guide","text":"This quest is the guide for an AWS Loft Well-Architected Security introduction workshop. You can check your local loft schedule for upcoming Well-Architected events, or you can also run it on your own! The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#step-1-new-aws-account-setup-and-securing-root-user","text":"","title":"Step 1 - New AWS Account Setup and Securing Root User"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#walkthrough","text":"This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. AWS Account and Root User","title":"Walkthrough"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#further-considerations","text":"Federate Identity Using SAML: Leveraging a SAML provider Separate production, non-production and different workloads using different AWS accounts: AWS Multiple Account Billing Strategy","title":"Further Considerations"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#step-2-basic-identity-and-access-management-user-group-role","text":"This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.","title":"Step 2 - Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#walkthrough_1","text":"Basic Identity and Access Management User, Group, Role","title":"Walkthrough"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#step-3-cloudfront-with-waf-protection","text":"This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. As CloudFront takes some time to update configuration in all edge locations, consider starting step 4 while its deploying.","title":"Step 3 -  CloudFront with WAF Protection"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#walkthrough_2","text":"CloudFront with WAF Protection","title":"Walkthrough"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#step-4-automated-deployment-of-detective-controls","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of AWS CloudTrail.","title":"Step 4 - Automated Deployment of Detective Controls"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#walkthrough_3","text":"Only complete step 2, GuardDuty from: Automated Deployment of Detective Controls","title":"Walkthrough"},{"location":"Security/Quest_100_Loft_Introduction_to_Security/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html","text":"Quest: Quick Steps to Security Success Authors Byron Pogson, Solutions Architect About this Guide This quest is for you to improve your security posture. Every stakeholder involved in your organization and product or service is entitled to make use of a secure platform. Security is important to earn the trust with your customers and your providers. A secure environment also helps to protect your intellectual property. Each set of activities can be done in one day or split over a week in your lunch break. By the end of the quest we will have a set of accounts with security best practices applied, ready to develop your product in knowing that when you launch your workload you have secure foundations in place. For more context on this quest see Essential Security Patterns from Public Sector Summit Canberra 2019 and the associated slide deck on SlideShare Step 1 - Multi-Account Strategy Implementing multiple accounts for our workload improves our security by limiting the blast radius of any potential breaches and separating our workload into discrete accounts. Leverage AWS Organizations to create separate AWS accounts for a sandbox, your workload, a secure \u201cdata bunker\u201d for audit logs and backups, and a shared services account for common tools. If you currently only have one account, create a new AWS account for your organizations master AWS account and invite your existing account to join as your sandbox AWS account. By the end of this step you will have a separate AWS account for: Organizations root account \u2013 used only for identity and billing Shared services \u2013 for common tools such as deployment tooling Workload accounts \u2013 customers who have a single product will have a separate AWS account for each environment. If you have multiple workloads, or you rely on account separation for separation of customer data you will want to set up further accounts to reflect your structure Walkthrough Define your multi-account strategy . A suggested structure is shown below. If you do not current have AWS Organizations setup it is recommended that you use your existing account as a sandbox (If required) Sign up a new root account Create an AWS Organization in the root account Invite any existing accounts For each AWS account required Create a new account in organizations . Make note of the organizations account access role. Create a new IAM role in the root account that has permission to assume that role to access the new AWS account Consider applying best practices as a baseline such as lock away your AWS account root user access keys and using multi-factor authentication Step 2 - Identity Every user must leverage unique credentials so we can trace actions within our accounts. Setup your identity structure in the master account and use cross account access to access the child accounts. As you create roles for your users ensure that you are implementing least privilege access by ensuring that users only have access to perform actions required for their role. Be careful who you give IAM permissions to as they can create their own permissions. Walkthrough Perform a credentials audit, add multi factor authentication to root and ensure that details are up to date Federate Identity leveraging a SAML provider Use cross account access roles to access the accounts that we setup in part 1. Step 3 - Data Bunker Now we will create a data bunker account to store secure read only security logs and backups. In this step we will send our logs from CloudTrail to that account. The role for accessing this account will have read only access. Only ensure that this role can be accessed by those with a security role in your organization. Walkthrough Setup a security account, secure Amazon S3 bucket and turn on our AWS Organization CloudTrail Step 4 - Enable organizations policies AWS Organizations policies allow you to apply additional controls to accounts. In the examples given below these are attached to the root which will affect all accounts within the organization. You can also create specific service control policies for separate organizational units within your organization. Walkthrough (repeat for each policy below) Navigate to AWS Organization and select the Policies tab Click Create policy Enter a policy name for your policy and paste the policy JSON below into the policy editor Click Create policy Select the policy you have just created and in the right-hand panel select * roots Press Attach to attach the policy to your organizations root Policy to prevent users disabling CloudTrail { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"cloudtrail:StopLogging\", \"Resource\": \"*\" } ] } (Optional) Disable unused regions This policy specifically enables only us-east-1 and us-west-1 . Replace with the list of region codes you wish to allow access to. Note: Not all AWS global services are shown in this example policy. Replace the list of services in red italicized text with the global services used by accounts in your organization. Note: This example policy blocks access to the AWS Security Token Service global endpoint (sts.amazonaws.com). To use AWS STS with this policy, use regional endpoints or add \"sts:*\" to the NotAction element. For more information on AWS STS endpoints, see Activating and Deactivating AWS STS in an AWS Region in the IAM User Guide. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"DenyAllOutsideEU\", \"Effect\": \"Deny\", \"NotAction\": [ \"iam:*\", \"organizations:*\", \"route53:*\", \"budgets:*\", \"waf:*\", \"cloudfront:*\", \"globalaccelerator:*\", \"importexport:*\", \"support:*\" ], \"Resource\": \"*\", \"Condition\": { \"StringNotEquals\": { \"aws:RequestedRegion\": [ \"eu-central-1\", \"eu-west-1\" ] } } } ] } Step 5 - Disable public access to data in S3 Amazon Simple Storage Service (S3) allows you to upload objects to a \"bucket\" which are then accessible depending on the access control list implemented. It is important to consider how you make data public. By blocking public access your team will have to be deliberate when it exposes data. Note: The steps below apply at an individual account level. It is important to consider turning this on as you create additional accounts for your organization. At a minimum you should apply this to any account which hosts production data. Walkthrough For each account that you want to block public access to data stored in S3 - use a cross-account access role to block S3 public access Repeat the walkthrough in part 4 to apply the policy below. Instead of applying this policy to the root, apply this specifically to the accounts where you have blocked public access. Policy to block public { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Statement1\", \"Effect\": \"Deny\", \"Action\": [ \"s3:PutBucketPublicAccessBlock\", \"s3:PutAccountPublicAccessBlock\" ], \"Resource\": \"*\" } ] } Step 6 - Monitoring and Alerting Lastly, we will setup our foundations for monitoring the security status of our AWS environment and look at how we can build some basic alerting to security incidents. AWS Security Hub gives you a comprehensive view of the security of your account including compliance checks against best practices such as the Centre for Information Security AWS Foundational Benchmark . We will also enable Amazon GuardDuty - a threat detection service which leverages machine learning to detect anomalies across our AWS CloudTrail, Amazon VPC Flow Logs, and DNS logs. Walkthrough Enable AWS Security Hub . Leverage the AWS Security Hub Multiaccount Scripts to enable it across accounts. Enable Amazon GuardDuty . Leverage Amazon to enable it across accounts. Additional Resources and next steps Find further information on the AWS website around AWS Cloud Security and in particular what your responsibilities are under the shared security model Read more on how permission boundaries and service control policies allow you to delegate access across your organization Understand the Well Architected Framework and how applying it can improve your security posture License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Quick Steps to Security Success"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#quest-quick-steps-to-security-success","text":"","title":"Quest: Quick Steps to Security Success"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#authors","text":"Byron Pogson, Solutions Architect","title":"Authors"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#about-this-guide","text":"This quest is for you to improve your security posture. Every stakeholder involved in your organization and product or service is entitled to make use of a secure platform. Security is important to earn the trust with your customers and your providers. A secure environment also helps to protect your intellectual property. Each set of activities can be done in one day or split over a week in your lunch break. By the end of the quest we will have a set of accounts with security best practices applied, ready to develop your product in knowing that when you launch your workload you have secure foundations in place. For more context on this quest see Essential Security Patterns from Public Sector Summit Canberra 2019 and the associated slide deck on SlideShare","title":"About this Guide"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-1-multi-account-strategy","text":"Implementing multiple accounts for our workload improves our security by limiting the blast radius of any potential breaches and separating our workload into discrete accounts. Leverage AWS Organizations to create separate AWS accounts for a sandbox, your workload, a secure \u201cdata bunker\u201d for audit logs and backups, and a shared services account for common tools. If you currently only have one account, create a new AWS account for your organizations master AWS account and invite your existing account to join as your sandbox AWS account. By the end of this step you will have a separate AWS account for: Organizations root account \u2013 used only for identity and billing Shared services \u2013 for common tools such as deployment tooling Workload accounts \u2013 customers who have a single product will have a separate AWS account for each environment. If you have multiple workloads, or you rely on account separation for separation of customer data you will want to set up further accounts to reflect your structure","title":"Step 1 - Multi-Account Strategy"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough","text":"Define your multi-account strategy . A suggested structure is shown below. If you do not current have AWS Organizations setup it is recommended that you use your existing account as a sandbox (If required) Sign up a new root account Create an AWS Organization in the root account Invite any existing accounts For each AWS account required Create a new account in organizations . Make note of the organizations account access role. Create a new IAM role in the root account that has permission to assume that role to access the new AWS account Consider applying best practices as a baseline such as lock away your AWS account root user access keys and using multi-factor authentication","title":"Walkthrough"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-2-identity","text":"Every user must leverage unique credentials so we can trace actions within our accounts. Setup your identity structure in the master account and use cross account access to access the child accounts. As you create roles for your users ensure that you are implementing least privilege access by ensuring that users only have access to perform actions required for their role. Be careful who you give IAM permissions to as they can create their own permissions.","title":"Step 2 - Identity"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough_1","text":"Perform a credentials audit, add multi factor authentication to root and ensure that details are up to date Federate Identity leveraging a SAML provider Use cross account access roles to access the accounts that we setup in part 1.","title":"Walkthrough"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-3-data-bunker","text":"Now we will create a data bunker account to store secure read only security logs and backups. In this step we will send our logs from CloudTrail to that account. The role for accessing this account will have read only access. Only ensure that this role can be accessed by those with a security role in your organization.","title":"Step 3 - Data Bunker"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough_2","text":"Setup a security account, secure Amazon S3 bucket and turn on our AWS Organization CloudTrail","title":"Walkthrough"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-4-enable-organizations-policies","text":"AWS Organizations policies allow you to apply additional controls to accounts. In the examples given below these are attached to the root which will affect all accounts within the organization. You can also create specific service control policies for separate organizational units within your organization.","title":"Step 4 - Enable organizations policies"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough-repeat-for-each-policy-below","text":"Navigate to AWS Organization and select the Policies tab Click Create policy Enter a policy name for your policy and paste the policy JSON below into the policy editor Click Create policy Select the policy you have just created and in the right-hand panel select * roots Press Attach to attach the policy to your organizations root","title":"Walkthrough (repeat for each policy below)"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#policy-to-prevent-users-disabling-cloudtrail","text":"{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"cloudtrail:StopLogging\", \"Resource\": \"*\" } ] }","title":"Policy to prevent users disabling CloudTrail"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#optional-disable-unused-regions","text":"This policy specifically enables only us-east-1 and us-west-1 . Replace with the list of region codes you wish to allow access to. Note: Not all AWS global services are shown in this example policy. Replace the list of services in red italicized text with the global services used by accounts in your organization. Note: This example policy blocks access to the AWS Security Token Service global endpoint (sts.amazonaws.com). To use AWS STS with this policy, use regional endpoints or add \"sts:*\" to the NotAction element. For more information on AWS STS endpoints, see Activating and Deactivating AWS STS in an AWS Region in the IAM User Guide. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"DenyAllOutsideEU\", \"Effect\": \"Deny\", \"NotAction\": [ \"iam:*\", \"organizations:*\", \"route53:*\", \"budgets:*\", \"waf:*\", \"cloudfront:*\", \"globalaccelerator:*\", \"importexport:*\", \"support:*\" ], \"Resource\": \"*\", \"Condition\": { \"StringNotEquals\": { \"aws:RequestedRegion\": [ \"eu-central-1\", \"eu-west-1\" ] } } } ] }","title":"(Optional) Disable unused regions"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-5-disable-public-access-to-data-in-s3","text":"Amazon Simple Storage Service (S3) allows you to upload objects to a \"bucket\" which are then accessible depending on the access control list implemented. It is important to consider how you make data public. By blocking public access your team will have to be deliberate when it exposes data. Note: The steps below apply at an individual account level. It is important to consider turning this on as you create additional accounts for your organization. At a minimum you should apply this to any account which hosts production data.","title":"Step 5 - Disable public access to data in S3"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough_3","text":"For each account that you want to block public access to data stored in S3 - use a cross-account access role to block S3 public access Repeat the walkthrough in part 4 to apply the policy below. Instead of applying this policy to the root, apply this specifically to the accounts where you have blocked public access.","title":"Walkthrough"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#policy-to-block-public","text":"{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Statement1\", \"Effect\": \"Deny\", \"Action\": [ \"s3:PutBucketPublicAccessBlock\", \"s3:PutAccountPublicAccessBlock\" ], \"Resource\": \"*\" } ] }","title":"Policy to block public"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#step-6-monitoring-and-alerting","text":"Lastly, we will setup our foundations for monitoring the security status of our AWS environment and look at how we can build some basic alerting to security incidents. AWS Security Hub gives you a comprehensive view of the security of your account including compliance checks against best practices such as the Centre for Information Security AWS Foundational Benchmark . We will also enable Amazon GuardDuty - a threat detection service which leverages machine learning to detect anomalies across our AWS CloudTrail, Amazon VPC Flow Logs, and DNS logs.","title":"Step 6 - Monitoring and Alerting"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#walkthrough_4","text":"Enable AWS Security Hub . Leverage the AWS Security Hub Multiaccount Scripts to enable it across accounts. Enable Amazon GuardDuty . Leverage Amazon to enable it across accounts.","title":"Walkthrough"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#additional-resources-and-next-steps","text":"Find further information on the AWS website around AWS Cloud Security and in particular what your responsibilities are under the shared security model Read more on how permission boundaries and service control policies allow you to delegate access across your organization Understand the Well Architected Framework and how applying it can improve your security posture","title":"Additional Resources and next steps"},{"location":"Security/Quest_100_Quick_Steps_to_Security_Success/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_200_Incident_Response_Day/README.html","text":"Quest: AWS Incident Response Day About this Guide This quest is the guide for an AWS led event including incident response day. Using an AWS supplied, or your own AWS account, you will learn through hands-on labs in the AWS Well-Architected area of Incident Response . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Lab 1 - Automated Deployment of Detective Controls This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service. Start now! Lab 2 - Protecting workloads on AWS from the instance to the edge In this workshop, you will build an environment consisting of two Amazon Linux web servers behind an application load balancer. The web servers will be running a PHP web site that contains several vulnerabilities. You will then use AWS Web Application Firewall (WAF), Amazon Inspector and AWS Systems Manager to identify the vulnerabilities and remediate them. Start now! Lab 3 - Incident Response with AWS Console and CLI This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. Start now! Lab 4 - Getting Hands on with Amazon GuardDuty Walks you through a scenario covering threat detection and automated remediation using Amazon GuardDuty; a managed threat detection service. The scenario simulates an attack that spans a few threat vectors, representing just a small sample of the threats that GuardDuty is able to detect. Start now! Lab 5 - Open Source AWS Memory Forensics This lab consists of using an open source python module for orchestrating memory acquisitions and analysis using AWS Systems Manager . It analyzes the memory dump using Rekall with the most common plugins: psaux, pstree, netstat, ifconfig, pidhashtable. Start now! Further Learning AWS Security Incident Response Guide Find further information on the AWS website around AWS Cloud Security and in particular what your responsibilities are under the shared security model Authors Ben Potter, Security Lead, Well-Architected License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Incident Response Day"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#quest-aws-incident-response-day","text":"","title":"Quest: AWS Incident Response Day"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#about-this-guide","text":"This quest is the guide for an AWS led event including incident response day. Using an AWS supplied, or your own AWS account, you will learn through hands-on labs in the AWS Well-Architected area of Incident Response . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#lab-1-automated-deployment-of-detective-controls","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.","title":"Lab 1 - Automated Deployment of Detective Controls"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#start-now","text":"","title":"Start now!"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#lab-2-protecting-workloads-on-aws-from-the-instance-to-the-edge","text":"In this workshop, you will build an environment consisting of two Amazon Linux web servers behind an application load balancer. The web servers will be running a PHP web site that contains several vulnerabilities. You will then use AWS Web Application Firewall (WAF), Amazon Inspector and AWS Systems Manager to identify the vulnerabilities and remediate them.","title":"Lab 2 - Protecting workloads on AWS from the instance to the edge"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#start-now_1","text":"","title":"Start now!"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#lab-3-incident-response-with-aws-console-and-cli","text":"This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled.","title":"Lab 3 - Incident Response with AWS Console and CLI"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#start-now_2","text":"","title":"Start now!"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#lab-4-getting-hands-on-with-amazon-guardduty","text":"Walks you through a scenario covering threat detection and automated remediation using Amazon GuardDuty; a managed threat detection service. The scenario simulates an attack that spans a few threat vectors, representing just a small sample of the threats that GuardDuty is able to detect.","title":"Lab 4 - Getting Hands on with Amazon GuardDuty"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#start-now_3","text":"","title":"Start now!"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#lab-5-open-source-aws-memory-forensics","text":"This lab consists of using an open source python module for orchestrating memory acquisitions and analysis using AWS Systems Manager . It analyzes the memory dump using Rekall with the most common plugins: psaux, pstree, netstat, ifconfig, pidhashtable.","title":"Lab 5 - Open Source AWS Memory Forensics"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#start-now_4","text":"","title":"Start now!"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#further-learning","text":"AWS Security Incident Response Guide Find further information on the AWS website around AWS Cloud Security and in particular what your responsibilities are under the shared security model","title":"Further Learning"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_200_Incident_Response_Day/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html","text":"Quest: AWS Security Best Practices Day Authors Ben Potter, Security Lead, Well-Architected About this Guide This quest is the guide for an AWS led event including security best practices day. Using your own AWS account you will learn through hands-on labs including identity & access management, detective controls, infrastructure protection, data protection and incident response. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Lab 1: Identity & Access Management For Lab 1 choose labs based on your interest or experience, its important to secure your AWS account so start with the introductory ones if you have not already completed: Introductory Lab 1.1: AWS Account and Root User This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. AWS Account and Root User Lab 1.2 Basic Identity and Access Management User, Group, Role This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. Basic Identity and Access Management User, Group, Role Advanced Lab 1.3 - IAM Permission Boundaries Delegating Role Creation This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. IAM Permission Boundaries Delegating Role Creation Lab 1.4 - IAM Tag Based Access Control for EC2 This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. IAM Tag Based Access Control for EC2 Lab 2 - Automated Deployment of Detective Controls This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service. Automated Deployment of Detective Controls Lab 3 - Enable Security Hub AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings. Enable Security Hub Lab 4 - Automated Deployment of VPC This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: Application Load Balancer - named ALB1 Application instances - named App1 Shared services - named Shared1 Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Do not follow tear down instructions until you have completed this quest, as the EC2 lab requires this VPC. Automated Deployment of VPC Lab 5 - Automated Deployment of EC2 Web Application This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information. Automated Deployment of EC2 Web Application Lab 6 - Automated Deployment of Web Application Firewall This hands-on lab will guide you through the steps to protect a workload from network based attacks using AWS Web Application Firewall (WAF) integrated with Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy AWS Web Application Firewall (WAF) with CloudFront integration to apply defense in depth methods. Automated Deployment of Web Application Firewall Lab 7 - CloudFront for Web Application This hands-on lab will guide you through the steps to help protect a web application from network based attacks using Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront. CloudFront for Web Application Lab 8 - Incident Response with AWS Console and CLI This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework . Incident Response with AWS Console and CLI License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Security Best Practices Day"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#quest-aws-security-best-practices-day","text":"","title":"Quest: AWS Security Best Practices Day"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#about-this-guide","text":"This quest is the guide for an AWS led event including security best practices day. Using your own AWS account you will learn through hands-on labs including identity & access management, detective controls, infrastructure protection, data protection and incident response. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-1-identity-access-management","text":"For Lab 1 choose labs based on your interest or experience, its important to secure your AWS account so start with the introductory ones if you have not already completed:","title":"Lab 1: Identity &amp; Access Management"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#introductory","text":"","title":"Introductory"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-11-aws-account-and-root-user","text":"This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user.","title":"Lab 1.1: AWS Account and Root User"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#aws-account-and-root-user","text":"","title":"AWS Account and Root User"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-12-basic-identity-and-access-management-user-group-role","text":"This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.","title":"Lab 1.2 Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#basic-identity-and-access-management-user-group-role","text":"","title":"Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#advanced","text":"","title":"Advanced"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-13-iam-permission-boundaries-delegating-role-creation","text":"This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.","title":"Lab 1.3 - IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#iam-permission-boundaries-delegating-role-creation","text":"","title":"IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-14-iam-tag-based-access-control-for-ec2","text":"This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.","title":"Lab 1.4 - IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#iam-tag-based-access-control-for-ec2","text":"","title":"IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-2-automated-deployment-of-detective-controls","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.","title":"Lab 2 - Automated Deployment of Detective Controls"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#automated-deployment-of-detective-controls","text":"","title":"Automated Deployment of Detective Controls"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-3-enable-security-hub","text":"AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.","title":"Lab 3 - Enable Security Hub"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#enable-security-hub","text":"","title":"Enable Security Hub"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-4-automated-deployment-of-vpc","text":"This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: Application Load Balancer - named ALB1 Application instances - named App1 Shared services - named Shared1 Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Do not follow tear down instructions until you have completed this quest, as the EC2 lab requires this VPC.","title":"Lab 4 - Automated Deployment of VPC"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#automated-deployment-of-vpc","text":"","title":"Automated Deployment of VPC"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-5-automated-deployment-of-ec2-web-application","text":"This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information.","title":"Lab 5 - Automated Deployment of EC2 Web Application"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#automated-deployment-of-ec2-web-application","text":"","title":"Automated Deployment of EC2 Web Application"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-6-automated-deployment-of-web-application-firewall","text":"This hands-on lab will guide you through the steps to protect a workload from network based attacks using AWS Web Application Firewall (WAF) integrated with Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy AWS Web Application Firewall (WAF) with CloudFront integration to apply defense in depth methods.","title":"Lab 6 - Automated Deployment of Web Application Firewall"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#automated-deployment-of-web-application-firewall","text":"","title":"Automated Deployment of Web Application Firewall"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-7-cloudfront-for-web-application","text":"This hands-on lab will guide you through the steps to help protect a web application from network based attacks using Amazon CloudFront. You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront.","title":"Lab 7 - CloudFront for Web Application"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#cloudfront-for-web-application","text":"","title":"CloudFront for Web Application"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#lab-8-incident-response-with-aws-console-and-cli","text":"This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework .","title":"Lab 8 - Incident Response with AWS Console and CLI"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#incident-response-with-aws-console-and-cli","text":"","title":"Incident Response with AWS Console and CLI"},{"location":"Security/Quest_300_Security_Best_Practices_Day/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html","text":"Quest: AWS Security Best Practices Workshop Authors Ben Potter, Security Lead, Well-Architected Pierre Liddle, Principal Solutions Architect About this Guide This quest is the guide for an AWS led event including AWS Summits security best practices workshop. Using your own AWS account you will learn through hands-on labs in securing an Amazon EC2-based web application covering identity & access management, detective controls, infrastructure protection, data protection and incident response. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Lab 1 - Identity & Access Management For Lab 1 choose one of labs to run based on your interest or experience: Lab 1a - IAM Permission Boundaries Delegating Role Creation This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. IAM Permission Boundaries Delegating Role Creation Lab 1b - IAM Tag Based Access Control for EC2 This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. IAM Tag Based Access Control for EC2 Lab 2 - Automated Deployment of VPC This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: Application Load Balancer - named ALB1 Application instances - named App1 Shared services - named Shared1 Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Do not follow tear down instructions until you have completed this quest, as the EC2 lab requires this VPC. Automated Deployment of VPC Lab 3 - Automated Deployment of EC2 Web Application This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information. Automated Deployment of EC2 Web Application Lab 4 - Automated Deployment of Detective Controls This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service. Automated Deployment of Detective Controls Lab 5 - Enable Security Hub AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings. Enable Security Hub Lab 6 - Incident Response with AWS Console and CLI This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework . Incident Response with AWS Console and CLI License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Security Best Practices Workshop"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#quest-aws-security-best-practices-workshop","text":"","title":"Quest: AWS Security Best Practices Workshop"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected Pierre Liddle, Principal Solutions Architect","title":"Authors"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#about-this-guide","text":"This quest is the guide for an AWS led event including AWS Summits security best practices workshop. Using your own AWS account you will learn through hands-on labs in securing an Amazon EC2-based web application covering identity & access management, detective controls, infrastructure protection, data protection and incident response. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-1-identity-access-management","text":"For Lab 1 choose one of labs to run based on your interest or experience:","title":"Lab 1 -  Identity &amp; Access Management"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-1a-iam-permission-boundaries-delegating-role-creation","text":"This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.","title":"Lab 1a - IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#iam-permission-boundaries-delegating-role-creation","text":"","title":"IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-1b-iam-tag-based-access-control-for-ec2","text":"This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.","title":"Lab 1b - IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#iam-tag-based-access-control-for-ec2","text":"","title":"IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-2-automated-deployment-of-vpc","text":"This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. The example CloudFormation template will deploy a completely new VPC incorporating a number of AWS security best practices which are: Networking subnets created in multiple availability zones for the following network tiers: Application Load Balancer - named ALB1 Application instances - named App1 Shared services - named Shared1 Databases - named DB1 VPC endpoints are created for private connectivity to AWS services. NAT Gateways are created to allow different subnets in the VPC to connect to the internet, without any direct ingress access being possible due to Route Table configurations. Network ACLs control access at each subnet layer. While VPC Flow Logs captures information about IP traffic and stores it in Amazon CloudWatch Logs. Do not follow tear down instructions until you have completed this quest, as the EC2 lab requires this VPC.","title":"Lab 2 - Automated Deployment of VPC"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#automated-deployment-of-vpc","text":"","title":"Automated Deployment of VPC"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-3-automated-deployment-of-ec2-web-application","text":"This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. The WordPress example CloudFormation template will deploy a basic WordPress content management system, incorporating a number of AWS security best practices. This example is not intended to be a comprehensive WordPress system, please consult Build a WordPress Website for more information.","title":"Lab 3 - Automated Deployment of EC2 Web Application"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#automated-deployment-of-ec2-web-application","text":"","title":"Automated Deployment of EC2 Web Application"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-4-automated-deployment-of-detective-controls","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.","title":"Lab 4 - Automated Deployment of Detective Controls"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#automated-deployment-of-detective-controls","text":"","title":"Automated Deployment of Detective Controls"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-5-enable-security-hub","text":"AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.","title":"Lab 5 - Enable Security Hub"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#enable-security-hub","text":"","title":"Enable Security Hub"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#lab-6-incident-response-with-aws-console-and-cli","text":"This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. You can find more best practices by reading the Security Pillar of the AWS Well-Architected Framework .","title":"Lab 6 - Incident Response with AWS Console and CLI"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#incident-response-with-aws-console-and-cli","text":"","title":"Incident Response with AWS Console and CLI"},{"location":"Security/Quest_300_Security_Best_Practices_Workshop_EC2/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Classify_Data/README.html","text":"Quest: Classify Data Labs coming soon Check out: Amazon Macie User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Classify Data"},{"location":"Security/Quest_Classify_Data/README.html#quest-classify-data","text":"","title":"Quest: Classify Data"},{"location":"Security/Quest_Classify_Data/README.html#labs-coming-soon","text":"Check out: Amazon Macie User Guide","title":"Labs coming soon"},{"location":"Security/Quest_Classify_Data/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Control_Human_Access/README.html","text":"Quest: Control Human Access Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Identity & Access Management . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Basic Identity and Access Management User, Group, Role Walkthrough This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. Basic Identity and Access Management User, Group, Role IAM Permission Boundaries Delegating Role Creation Walkthrough This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. IAM Permission Boundaries Delegating Role Creation IAM Tag Based Access Control for EC2 Walkthrough This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. IAM Tag Based Access Control for EC2 License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Control Human Access"},{"location":"Security/Quest_Control_Human_Access/README.html#quest-control-human-access","text":"","title":"Quest: Control Human Access"},{"location":"Security/Quest_Control_Human_Access/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_Control_Human_Access/README.html#about-this-guide","text":"This guide will help you improve your security in the AWS Well-Architected area of Identity & Access Management . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_Control_Human_Access/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_Control_Human_Access/README.html#basic-identity-and-access-management-user-group-role","text":"","title":"Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_Control_Human_Access/README.html#walkthrough","text":"This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.","title":"Walkthrough"},{"location":"Security/Quest_Control_Human_Access/README.html#basic-identity-and-access-management-user-group-role_1","text":"","title":"Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_Control_Human_Access/README.html#iam-permission-boundaries-delegating-role-creation","text":"","title":"IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_Control_Human_Access/README.html#walkthrough_1","text":"This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.","title":"Walkthrough"},{"location":"Security/Quest_Control_Human_Access/README.html#iam-permission-boundaries-delegating-role-creation_1","text":"","title":"IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_Control_Human_Access/README.html#iam-tag-based-access-control-for-ec2","text":"","title":"IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_Control_Human_Access/README.html#walkthrough_2","text":"This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.","title":"Walkthrough"},{"location":"Security/Quest_Control_Human_Access/README.html#iam-tag-based-access-control-for-ec2_1","text":"","title":"IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_Control_Human_Access/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Control_Programmatic_Access/README.html","text":"Quest: Control Programmatic Access Labs coming soon License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Control Programmatic Access"},{"location":"Security/Quest_Control_Programmatic_Access/README.html#quest-control-programmatic-access","text":"","title":"Quest: Control Programmatic Access"},{"location":"Security/Quest_Control_Programmatic_Access/README.html#labs-coming-soon","text":"","title":"Labs coming soon"},{"location":"Security/Quest_Control_Programmatic_Access/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Defend_Against_New_Threats/README.html","text":"Quest: Defend Against New Threats Labs coming soon Check out: AWS Security Blog AWS Security Bulletins License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Defend Against New Threats"},{"location":"Security/Quest_Defend_Against_New_Threats/README.html#quest-defend-against-new-threats","text":"","title":"Quest: Defend Against New Threats"},{"location":"Security/Quest_Defend_Against_New_Threats/README.html#labs-coming-soon","text":"Check out: AWS Security Blog AWS Security Bulletins","title":"Labs coming soon"},{"location":"Security/Quest_Defend_Against_New_Threats/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html","text":"Quest: Detect & Investigate Events Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Detective Controls . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Automated Deployment of Detective Controls Introduction This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service. Start the Lab! Enable Security Hub Introduction AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings. Start the Lab! Further Learning: AWS CloudTrail User Guide AWS CloudFormation User Guide Amazon GuardDuty User Guide AWS Config User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Detect and Investigate Events"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#quest-detect-investigate-events","text":"","title":"Quest: Detect &amp; Investigate Events"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#about-this-guide","text":"This guide will help you improve your security in the AWS Well-Architected area of Detective Controls . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#automated-deployment-of-detective-controls","text":"","title":"Automated Deployment of Detective Controls"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#introduction","text":"This hands-on lab will guide you through how to use AWS CloudFormation to automatically configure detective controls including AWS CloudTrail, AWS Config, and Amazon GuardDuty. You will use the AWS Management Console and AWS CloudFormation to guide you through how to automate the configuration of each service.","title":"Introduction"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#enable-security-hub","text":"","title":"Enable Security Hub"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#introduction_1","text":"AWS Security Hub gives you a comprehensive view of your high-priority security alerts and compliance status across AWS accounts. There are a range of powerful security tools at your disposal, from firewalls and endpoint protection to vulnerability and compliance scanners. But oftentimes this leaves your team switching back-and-forth between these tools to deal with hundreds, and sometimes thousands, of security alerts every day. With Security Hub, you now have a single place that aggregates, organizes, and prioritizes your security alerts, or findings, from multiple AWS services, such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie, as well as from AWS Partner solutions. Your findings are visually summarized on integrated dashboards with actionable graphs and tables. You can also continuously monitor your environment using automated compliance checks based on the AWS best practices and industry standards your organization follows. Get started with AWS Security Hub in just a few clicks in the Management Console and once enabled, Security Hub will begin aggregating and prioritizing findings.","title":"Introduction"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#start-the-lab_1","text":"","title":"Start the Lab!"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#further-learning","text":"AWS CloudTrail User Guide AWS CloudFormation User Guide Amazon GuardDuty User Guide AWS Config User Guide","title":"Further Learning:"},{"location":"Security/Quest_Detect_and_Investigate_Events/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Incident_Response/README.html","text":"Quest: Incident Response Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Incident Response . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Incident Response with AWS Console and CLI Introduction This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled. Start the Lab! Further Learning AWS Security Incident Response Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Incident Response"},{"location":"Security/Quest_Incident_Response/README.html#quest-incident-response","text":"","title":"Quest: Incident Response"},{"location":"Security/Quest_Incident_Response/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_Incident_Response/README.html#about-this-guide","text":"This guide will help you improve your security in the AWS Well-Architected area of Incident Response . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_Incident_Response/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_Incident_Response/README.html#incident-response-with-aws-console-and-cli","text":"","title":"Incident Response with AWS Console and CLI"},{"location":"Security/Quest_Incident_Response/README.html#introduction","text":"This hands-on lab will guide you through a number of examples of how you could use the AWS Console and Command Line Interface (CLI) for responding to a security incident. It is a best practice to be prepared for an incident, and have appropriate detective controls enabled.","title":"Introduction"},{"location":"Security/Quest_Incident_Response/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/Quest_Incident_Response/README.html#further-learning","text":"AWS Security Incident Response Guide","title":"Further Learning"},{"location":"Security/Quest_Incident_Response/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html","text":"Quest: Managing Credentials & Authentication Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Identity & Access Management . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . New AWS Account Setup and Securing Root User Walkthrough This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user. AWS Account and Root User Further Considerations Federate Identity Using SAML: Leveraging a SAML provider Separate production, non-production and different workloads using different AWS accounts: AWS Multiple Account Billing Strategy Basic Identity and Access Management User, Group, Role Walkthrough This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access. Basic Identity and Access Management User, Group, Role Automated IAM User Cleanup Walkthrough This hands-on lab will guide you through the steps to deploy an AWS Lambda function with AWS Serverless Application Model (SAM) to provide regular insights on IAM User/s and AWS Access Key usage within your account. IAM Tag Based Access Control for EC2 IAM Permission Boundaries Delegating Role Creation Walkthrough This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create. IAM Permission Boundaries Delegating Role Creation IAM Tag Based Access Control for EC2 Walkthrough This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag. IAM Tag Based Access Control for EC2 License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Managing Credentials and Authentication"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#quest-managing-credentials-authentication","text":"","title":"Quest: Managing Credentials &amp; Authentication"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#about-this-guide","text":"This guide will help you improve your security in the AWS Well-Architected area of Identity & Access Management . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#new-aws-account-setup-and-securing-root-user","text":"","title":"New AWS Account Setup and Securing Root User"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#walkthrough","text":"This hands-on lab will guide you through the introductory steps to configure a new AWS account and secure the root user.","title":"Walkthrough"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#aws-account-and-root-user","text":"","title":"AWS Account and Root User"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#further-considerations","text":"Federate Identity Using SAML: Leveraging a SAML provider Separate production, non-production and different workloads using different AWS accounts: AWS Multiple Account Billing Strategy","title":"Further Considerations"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#basic-identity-and-access-management-user-group-role","text":"","title":"Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#walkthrough_1","text":"This hands-on lab will guide you through the introductory steps to configure AWS Identity and Access Management (IAM). You will use the AWS Management Console to guide you through how to configure your first IAM user, group and role for administrative access.","title":"Walkthrough"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#basic-identity-and-access-management-user-group-role_1","text":"","title":"Basic Identity and Access Management User, Group, Role"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#automated-iam-user-cleanup","text":"","title":"Automated IAM User Cleanup"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#walkthrough_2","text":"This hands-on lab will guide you through the steps to deploy an AWS Lambda function with AWS Serverless Application Model (SAM) to provide regular insights on IAM User/s and AWS Access Key usage within your account.","title":"Walkthrough"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#iam-tag-based-access-control-for-ec2","text":"","title":"IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#iam-permission-boundaries-delegating-role-creation","text":"","title":"IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#walkthrough_3","text":"This hands-on lab will guide you through the steps to configure an example AWS Identity and Access Management (IAM) permission boundary. AWS supports permissions boundaries for IAM entities (users or roles). A permissions boundary is an advanced feature in which you use a managed policy to set the maximum permissions that an identity-based policy can grant to an IAM entity. When you set a permissions boundary for an entity, the entity can perform only the actions that are allowed by the policy. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as a developer, the developer can then use this role to create additional user roles that are restricted to specific services and regions. This allows you to delegate access to create IAM roles and policies, without them exceeding the permissions in the permission boundary. We will also use a naming standard with a prefix, making it easier to control and organize policies and roles that your developers create.","title":"Walkthrough"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#iam-permission-boundaries-delegating-role-creation_1","text":"","title":"IAM Permission Boundaries Delegating Role Creation"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#iam-tag-based-access-control-for-ec2_1","text":"","title":"IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#walkthrough_4","text":"This hands-on lab will guide you through the steps to configure example AWS Identity and Access Management (IAM) policies, and a AWS IAM role with associated permissions to use EC2 resource tags for access control. Using tags is powerful as it helps you scale your permission management, however you need to be careful about the management of the tags which you will learn in this lab. In this lab you will create a series of policies attached to a role that can be assumed by an individual such as an EC2 administrator. This allows the EC2 administrator to create tags when creating resources only if they match the requirements, and control which existing resources and values they can tag.","title":"Walkthrough"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#iam-tag-based-access-control-for-ec2_2","text":"","title":"IAM Tag Based Access Control for EC2"},{"location":"Security/Quest_Managing_Credentials_and_Authentication/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Protect_Compute/README.html","text":"Quest: Protect Compute Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Infrastructure Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Automated Deployment of VPC Introduction This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab. Start the Lab! Automated Deployment of EC2 Web Application Introduction This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. You must have first deployed the Automated Deployment of VPC lab. Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Protect Compute"},{"location":"Security/Quest_Protect_Compute/README.html#quest-protect-compute","text":"","title":"Quest: Protect Compute"},{"location":"Security/Quest_Protect_Compute/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_Protect_Compute/README.html#about-this-guide","text":"This guide will help you improve your security in the AWS Well-Architected area of Infrastructure Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_Protect_Compute/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_Protect_Compute/README.html#automated-deployment-of-vpc","text":"","title":"Automated Deployment of VPC"},{"location":"Security/Quest_Protect_Compute/README.html#introduction","text":"This hands-on lab will guide you through the steps to configure an Amazon VPC and outline some of the AWS security features. AWS CloudFormation will be used to automate the deployment and provide a repeatable way to re-use the template after this lab.","title":"Introduction"},{"location":"Security/Quest_Protect_Compute/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/Quest_Protect_Compute/README.html#automated-deployment-of-ec2-web-application","text":"","title":"Automated Deployment of EC2 Web Application"},{"location":"Security/Quest_Protect_Compute/README.html#introduction_1","text":"This hands-on lab will guide you through the steps to configure a web application in Amazon EC2 with a defense in depth approach. You must have first deployed the Automated Deployment of VPC lab.","title":"Introduction"},{"location":"Security/Quest_Protect_Compute/README.html#start-the-lab_1","text":"","title":"Start the Lab!"},{"location":"Security/Quest_Protect_Compute/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html","text":"Quest: Protect Data at Rest Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Data Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . Create a Data Bunker Account Introduction In this lab we will create a secure data bunker. A data bunker is a secure account which will hold important security data in a secure location. Ensure that only members of your security team have access to this account. In this lab we will create a new security account, create a secure S3 bucket in that account and then turn on CloudTrail for our organisation tp send these logs to th bucket in the secure data account. You may want to also think about what other data you need in there such as secure backups. Start the Lab! Further Learning S3: Protecting Data Using Server-Side Encryption with AWS KMS\u2013Managed Keys Opt-in to Default Encryption for New EBS Volumes License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Protect Data at Rest"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#quest-protect-data-at-rest","text":"","title":"Quest: Protect Data at Rest"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#about-this-guide","text":"This guide will help you improve your security in the AWS Well-Architected area of Data Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#create-a-data-bunker-account","text":"","title":"Create a Data Bunker Account"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#introduction","text":"In this lab we will create a secure data bunker. A data bunker is a secure account which will hold important security data in a secure location. Ensure that only members of your security team have access to this account. In this lab we will create a new security account, create a secure S3 bucket in that account and then turn on CloudTrail for our organisation tp send these logs to th bucket in the secure data account. You may want to also think about what other data you need in there such as secure backups.","title":"Introduction"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#further-learning","text":"S3: Protecting Data Using Server-Side Encryption with AWS KMS\u2013Managed Keys Opt-in to Default Encryption for New EBS Volumes","title":"Further Learning"},{"location":"Security/Quest_Protect_Data_at_Rest/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Protect_Data_in_Transit/README.html","text":"Quest: Protect Data in Transit Labs coming soon Check out: AWS Certificate Manager - Getting Started AWS Encryption SDK AWS Site-to-Site VPN User Guide AWS Client VPN User Guide License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Protect Data in Transit"},{"location":"Security/Quest_Protect_Data_in_Transit/README.html#quest-protect-data-in-transit","text":"","title":"Quest: Protect Data in Transit"},{"location":"Security/Quest_Protect_Data_in_Transit/README.html#labs-coming-soon","text":"Check out: AWS Certificate Manager - Getting Started AWS Encryption SDK AWS Site-to-Site VPN User Guide AWS Client VPN User Guide","title":"Labs coming soon"},{"location":"Security/Quest_Protect_Data_in_Transit/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Security/Quest_Protect_Networks/README.html","text":"Quest: Protect Networks Authors Ben Potter, Security Lead, Well-Architected About this Guide This guide will help you improve your security in the AWS Well-Architected area of Infrastructure Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework . Prerequisites An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier . CloudFront with WAF Protection Introduction This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration. Start the Lab! License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Protect Networks"},{"location":"Security/Quest_Protect_Networks/README.html#quest-protect-networks","text":"","title":"Quest: Protect Networks"},{"location":"Security/Quest_Protect_Networks/README.html#authors","text":"Ben Potter, Security Lead, Well-Architected","title":"Authors"},{"location":"Security/Quest_Protect_Networks/README.html#about-this-guide","text":"This guide will help you improve your security in the AWS Well-Architected area of Infrastructure Protection . The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework .","title":"About this Guide"},{"location":"Security/Quest_Protect_Networks/README.html#prerequisites","text":"An AWS account that you are able to use for testing, that is not used for production or other purposes. NOTE: You will be billed for any applicable AWS resources used if you complete this lab that are not covered in the AWS Free Tier .","title":"Prerequisites"},{"location":"Security/Quest_Protect_Networks/README.html#cloudfront-with-waf-protection","text":"","title":"CloudFront with WAF Protection"},{"location":"Security/Quest_Protect_Networks/README.html#introduction","text":"This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration.","title":"Introduction"},{"location":"Security/Quest_Protect_Networks/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Security/Quest_Protect_Networks/README.html#license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"Well-ArchitectedTool/README.html","text":"AWS Well-Architected Tool Labs https://wellarchitectedlabs.com Introduction This repository contains documentation and code in the format of hands-on-labs to help you learn how to learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about the Well-Architected tool, read the AWS Well-Architected Tool documentation . Labs Level 100: Walkthrough of the AWS Well-Architected Tool License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Overview"},{"location":"Well-ArchitectedTool/README.html#aws-well-architected-tool-labs","text":"https://wellarchitectedlabs.com","title":"AWS Well-Architected Tool Labs"},{"location":"Well-ArchitectedTool/README.html#introduction","text":"This repository contains documentation and code in the format of hands-on-labs to help you learn how to learn, measure, and build using architectural best practices. The labs are categorized into levels, where 100 is introductory, 200/300 is intermediate and 400 is advanced. For more information about the Well-Architected tool, read the AWS Well-Architected Tool documentation .","title":"Introduction"},{"location":"Well-ArchitectedTool/README.html#labs","text":"Level 100: Walkthrough of the AWS Well-Architected Tool","title":"Labs"},{"location":"Well-ArchitectedTool/README.html#license","text":"","title":"License"},{"location":"Well-ArchitectedTool/README.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Well-ArchitectedTool/README.html#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html","text":"Level 100: Walkthrough of the Well-Architected Tool https://wellarchitectedlabs.com Introduction The purpose if this lab is to walk you through the features of the AWS Well-Architected Tool. You will create a workload, review the Reliability Pillar questions, save the workload, take a milestone, and examine and download the Well-Architected Review report. The knowledge you acquire will help you build Well-Architected workloads in alignment with the AWS Well-Architected Framework Goals: Learn where resources about the questions and best practices are located. Learn how to use milestones to track your progress again high and medium risks over time. Learn how to generate a report or view the results of the review in the Well-Architected Tool. Prequisites: An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to usee Well-Architected Tool (WellArchitectedConsoleFullAccess managed policy). Start the Lab! License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code LicenseLicensed under the Apache 2.0 and MITnoAttr License. Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Overview"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#level-100-walkthrough-of-the-well-architected-tool","text":"https://wellarchitectedlabs.com","title":"Level 100: Walkthrough of the Well-Architected Tool"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#introduction","text":"The purpose if this lab is to walk you through the features of the AWS Well-Architected Tool. You will create a workload, review the Reliability Pillar questions, save the workload, take a milestone, and examine and download the Well-Architected Review report. The knowledge you acquire will help you build Well-Architected workloads in alignment with the AWS Well-Architected Framework","title":"Introduction"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#goals","text":"Learn where resources about the questions and best practices are located. Learn how to use milestones to track your progress again high and medium risks over time. Learn how to generate a report or view the results of the review in the Well-Architected Tool.","title":"Goals:"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#prequisites","text":"An AWS Account that you are able to use for testing, that is not used for production or other purposes. An Identity and Access Management (IAM) user or federated credentials into that account that has permissions to usee Well-Architected Tool (WellArchitectedConsoleFullAccess managed policy).","title":"Prequisites:"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#start-the-lab","text":"","title":"Start the Lab!"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#license","text":"","title":"License"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/README.html#code-licenselicensed-under-the-apache-20-and-mitnoattr-license","text":"Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code LicenseLicensed under the Apache 2.0 and MITnoAttr License."},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html","text":"Level 100: Walkthrough of the Well-Architected Tool Authors Rodney Lester, Reliability Lead, Well-Architected, AWS Table of Contents Navigating to the console Creating a workload Performing a review Saving a milestone Viewing and downloading the report Tear Down 1. Navigating to the console The AWS Well-Architected Tool is in the AWS Console. You simply need to login to the console and navigate to the tool. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled or in a federated Role, and open the Well-Architected console at https://console.aws.amazon.com/wellarchitected/ . If you are already in the console, click Services on the tool bar along the top of the console to bring up the service search. Start typing Well Architected into the search box and select the AWS Well-Architected Tool : 2. Creating a workload Well-Architected Reviews are conducted per workload . A workload identifies a set of components that deliver business value. The workload is usually the level of detail that business and technology leaders communicate about. Click the Define Workload button on the landing page: If you had existing workloads, then you will land at the Workloads listing. In this interface, click the Define Workload button: On the Define Workload interface, enter the necessary information: Name: Workload for AWS Workshop Description: This is an example for the AWS Workshop Industry Type: InfoTech Industry: Internet Environment: Select \"Pre-production\" Regions: Select AWS Regions, and choose US West (Oregon)/us-west-2 Click on the Define workload button: 3. Performing a review From the detail page for the workload, click the Start review button: In this walkthrough, we are only going to complete the Reliability Pillar questions. Collapse the Operational Excellence questions by selecting the collapsing icon on the left of the words Operation Excellence on the left: Expand the Reliability Questions by selecting the expanding icon to the left of the word Reliability : Select the first question: REL 1. How do you manage service limits? Answer the REL 1 to REL 9 questions as you understand your current ability. You can use the Info links to help you understand what the answers mean, and watch the video to get more context on the questions. As you complete the question, select the Next Button at the bottom of the answers: When you get to the last Reliability question, or the first Performance Pillar question, select Save and Exit: 4. Saving a milestone From the detail page for the workload, click the Save milestone button : Enter a name for the milestone as AWS Workshop Milestone and click the Save button: Click on the Milestones tab: This will display the milestone and data about it: 5. Viewing and downloading the report From the detail page for the workload, click the Improvement Plan tab: This will display the number of high and medium risk items and allow you to update the Improvement status: You can also edit the improvement plan configuration. Click on the Edit button next to the words Improvement plan configuration : Move the Reliability Pillar up by clicking the up icon to the right of the word, Reliability: Click the Save button to save this configuration: Click on the Review tab to get the option to download the improvement plan: Click the Generate report button to generate and download the report: You can either open the file or save it to view it. 6. Tear down this lab In order to take down the lab environment, you simply delete the workload you created. 1. Select Workloads on the left navigation: Select the radio button next to the Workload for AWS Workshop and then click the Delete button. Confirm the deletion by clicking the Delete button on the dialog: References & useful resources: License Documentation License Licensed under the Creative Commons Share Alike 4.0 license. Code License Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Lab Guide"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#level-100-walkthrough-of-the-well-architected-tool","text":"","title":"Level 100: Walkthrough of the Well-Architected Tool"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#authors","text":"Rodney Lester, Reliability Lead, Well-Architected, AWS","title":"Authors"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#table-of-contents","text":"Navigating to the console Creating a workload Performing a review Saving a milestone Viewing and downloading the report Tear Down","title":"Table of Contents"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#1-navigating-to-the-console","text":"The AWS Well-Architected Tool is in the AWS Console. You simply need to login to the console and navigate to the tool. 1. Sign in to the AWS Management Console as an IAM user with MFA enabled or in a federated Role, and open the Well-Architected console at https://console.aws.amazon.com/wellarchitected/ . If you are already in the console, click Services on the tool bar along the top of the console to bring up the service search. Start typing Well Architected into the search box and select the AWS Well-Architected Tool :","title":"1. Navigating to the console "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#2-creating-a-workload","text":"Well-Architected Reviews are conducted per workload . A workload identifies a set of components that deliver business value. The workload is usually the level of detail that business and technology leaders communicate about. Click the Define Workload button on the landing page: If you had existing workloads, then you will land at the Workloads listing. In this interface, click the Define Workload button: On the Define Workload interface, enter the necessary information: Name: Workload for AWS Workshop Description: This is an example for the AWS Workshop Industry Type: InfoTech Industry: Internet Environment: Select \"Pre-production\" Regions: Select AWS Regions, and choose US West (Oregon)/us-west-2 Click on the Define workload button:","title":"2. Creating a workload "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#3-performing-a-review","text":"From the detail page for the workload, click the Start review button: In this walkthrough, we are only going to complete the Reliability Pillar questions. Collapse the Operational Excellence questions by selecting the collapsing icon on the left of the words Operation Excellence on the left: Expand the Reliability Questions by selecting the expanding icon to the left of the word Reliability : Select the first question: REL 1. How do you manage service limits? Answer the REL 1 to REL 9 questions as you understand your current ability. You can use the Info links to help you understand what the answers mean, and watch the video to get more context on the questions. As you complete the question, select the Next Button at the bottom of the answers: When you get to the last Reliability question, or the first Performance Pillar question, select Save and Exit:","title":"3. Performing a review "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#4-saving-a-milestone","text":"From the detail page for the workload, click the Save milestone button : Enter a name for the milestone as AWS Workshop Milestone and click the Save button: Click on the Milestones tab: This will display the milestone and data about it:","title":"4. Saving a milestone "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#5-viewing-and-downloading-the-report","text":"From the detail page for the workload, click the Improvement Plan tab: This will display the number of high and medium risk items and allow you to update the Improvement status: You can also edit the improvement plan configuration. Click on the Edit button next to the words Improvement plan configuration : Move the Reliability Pillar up by clicking the up icon to the right of the word, Reliability: Click the Save button to save this configuration: Click on the Review tab to get the option to download the improvement plan: Click the Generate report button to generate and download the report: You can either open the file or save it to view it.","title":"5. Viewing and downloading the report "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#6-tear-down-this-lab","text":"In order to take down the lab environment, you simply delete the workload you created. 1. Select Workloads on the left navigation: Select the radio button next to the Workload for AWS Workshop and then click the Delete button. Confirm the deletion by clicking the Delete button on the dialog:","title":"6. Tear down this lab "},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#references-useful-resources","text":"","title":"References &amp; useful resources:"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#license","text":"","title":"License"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#documentation-license","text":"Licensed under the Creative Commons Share Alike 4.0 license.","title":"Documentation License"},{"location":"Well-ArchitectedTool/100_Walkthrough_of_the_Well-Architected_Tool/Lab_Guide.html#code-license","text":"Licensed under the Apache 2.0 and MITnoAttr License. Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at https://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Code License"},{"location":"common/documentation/AWS_Credentials.html","text":"Setup AWS credentials and configuration You will supply configuration and credentials used by the AWS CLI and AWS SDK to access your AWS account. Choose an option Select the appropriate option for configuration of your AWS credentials: Option 1 - Using AWS instructor supplied accounts with Linux-style environment variables Option 2 - Using AWS CLI Option 3 - Creating configuration files manually Option 4 - Using PowerShell commands for Windows Option 1 For instructor supplied AWS accounts If BOTH of the following are true then you may use Option 1 If you are attending an in-person workshop and were provided with an AWS account by the instructor then you should use this option You are running the workshop on a system where environment variables are set using the export command, such as Bash on Amazon Linux Otherwise you should choose Option 2 or Option 3 You should have already copied the credentials for your account. If not then follow the directions here The copied credentials are already in the form of export statements. Run these from your shell command line. Use your values, not the example ones below export AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE export AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY export AWS_SESSION_TOKEN=FQoGZXIvYXdzEDwaIIAMFAKEn0LVImWNQHiLuAWKe+KFkLeIvpOHEruWjyCjrEdyjtW8WCbnmJGM1ES20xq1fcaS5TERHDUabZJ60Kk6nc9uHoCDb1QKHi+MerRIcKJTi3OKz0QMVPAGVqVWgvOBBSQ2lylLVjtMMSQF+yLZsP1bvehQ0ke/Bl/X6RJySOHg2TZGyESPL/INqJiZyEHi+MelAnThepVgWUKFPD5mESBVlpy2LVCE3xPpHFqOm0Q79svRSSW2jLj5NkRXL+xhkcvt+g8vNt1ODEwixwMGpFB2sBHryv6EXNeX6c88vxJ8Zyfkmsqi0xmCW1f9jWAPIXNkt/nEYWEXAMPLETOKEN= export AWS_DEFAULT_REGION=us-east-2 Also run this command as written below export AWS_DEFAULT_OUTPUT=json Note that if you end your bash session, or start a new one, you will need to re-execute the export statements If you completed Option 1 then STOP HERE and return to the Lab Guide Option 2 AWS CLI This option uses the AWS CLI. You should use Option 3 if you do not or cannot install the AWS CLI. To see if the AWS CLI is installed: $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.1 or higher is fine If you instead got command not found then either install the AWS CLI or use Option 3 Run aws configure and provide the following values: $ aws configure AWS Access Key ID [*************xxxx]: <Your AWS Access Key ID> AWS Secret Access Key [**************xxxx]: <Your AWS Secret Access Key> Default region name: [us-east-2]: us-east-2 Default output format [None]: json Option 3 Manually creating credential files create a .aws directory under your home directory mkdir ~/.aws Change directory to there cd ~/.aws Use a text editor (vim, emacs, notepad) to create a text file (no extension) named credentials . In this file you should have the following text. [default] aws_access_key_id = <Your access key> aws_secret_access_key = <Your secret key> Create a text file (no extension) named config . In this file you should have the following text: [default] region = us-east-2 output = json Configure a session token as part of your credentials If you used Option 2 or Option 3 , please follow these steps: Determine if you need to configure a session token as part of your credentials AWS Account Do you need a session token? You are attending an in-person workshop and were provided with an AWS account by the instructor yes You are using your own AWS account, and using credentials from an IAM User (most common case) no You are using your own AWS account, and using credentials from an IAM Role yes Do this only if \"yes\", you need to configure a session token Edit the file ~/.aws/credentials The default profile will already be present. Under it add an entry for aws_session_token [default] aws_access_key_id = <Your access key> aws_secret_access_key = <Your secret key> aws_session_token = <your session token> Clear environment variables If you used option 2 or option 3 then you have put your credentials into files that will be used by the AWS CLI or AWS SDK. However these will preferentially use credentials and configuration in environment variables. Therefore ensure that the following env variables are not set: AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN AWS_DEFAULT_REGION AWS_DEFAULT_OUTPUT AWS_PROFILE How to do this varies depending on system. For Linux: # Use echo $varname to see if it is set $ echo $AWS_ACCESS_KEY_ID ASIATWOQ3L72RPLOP222 # use unset $ unset AWS_ACCESS_KEY_ID # This now returns no value $ echo $AWS_ACCESS_KEY_ID For your convenience: unset AWS_ACCESS_KEY_ID unset AWS_SECRET_ACCESS_KEY unset AWS_SESSION_TOKEN unset AWS_DEFAULT_REGION unset AWS_DEFAULT_OUTPUT unset AWS_PROFILE Option 4 (PowerShell) Configuration of AWS credentials 1. If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ . Start a Windows PowerShell for AWS session. If prompted for AWS Secret Key during initialization, type Control-C to break out of the dialog. Configure your AWS credentials with the following PowerShell commands. Note that if you are using an instructor supplied AWS account, you must include the optional SessionToken flag and value as shown below in brackets (omit the brackets when running the command): Set-AWSCredentials -AccessKey <Your access key> -SecretKey <Your secret key> \\ [ -SessionToken <your session key> ] -StoreAs <SomeProfileName> Initialize-AWSDefaults -ProfileName <SomeProfileName> -Region us-east-2 Return to the Lab Guide to continue the lab","title":"Setup AWS credentials and configuration"},{"location":"common/documentation/AWS_Credentials.html#setup-aws-credentials-and-configuration","text":"You will supply configuration and credentials used by the AWS CLI and AWS SDK to access your AWS account.","title":"Setup AWS credentials and configuration"},{"location":"common/documentation/AWS_Credentials.html#choose-an-option","text":"Select the appropriate option for configuration of your AWS credentials: Option 1 - Using AWS instructor supplied accounts with Linux-style environment variables Option 2 - Using AWS CLI Option 3 - Creating configuration files manually Option 4 - Using PowerShell commands for Windows","title":"Choose an option"},{"location":"common/documentation/AWS_Credentials.html#option-1-for-instructor-supplied-aws-accounts","text":"If BOTH of the following are true then you may use Option 1 If you are attending an in-person workshop and were provided with an AWS account by the instructor then you should use this option You are running the workshop on a system where environment variables are set using the export command, such as Bash on Amazon Linux Otherwise you should choose Option 2 or Option 3 You should have already copied the credentials for your account. If not then follow the directions here The copied credentials are already in the form of export statements. Run these from your shell command line. Use your values, not the example ones below export AWS_ACCESS_KEY_ID=AKIAIOSFODNN7EXAMPLE export AWS_SECRET_ACCESS_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY export AWS_SESSION_TOKEN=FQoGZXIvYXdzEDwaIIAMFAKEn0LVImWNQHiLuAWKe+KFkLeIvpOHEruWjyCjrEdyjtW8WCbnmJGM1ES20xq1fcaS5TERHDUabZJ60Kk6nc9uHoCDb1QKHi+MerRIcKJTi3OKz0QMVPAGVqVWgvOBBSQ2lylLVjtMMSQF+yLZsP1bvehQ0ke/Bl/X6RJySOHg2TZGyESPL/INqJiZyEHi+MelAnThepVgWUKFPD5mESBVlpy2LVCE3xPpHFqOm0Q79svRSSW2jLj5NkRXL+xhkcvt+g8vNt1ODEwixwMGpFB2sBHryv6EXNeX6c88vxJ8Zyfkmsqi0xmCW1f9jWAPIXNkt/nEYWEXAMPLETOKEN= export AWS_DEFAULT_REGION=us-east-2 Also run this command as written below export AWS_DEFAULT_OUTPUT=json Note that if you end your bash session, or start a new one, you will need to re-execute the export statements If you completed Option 1 then STOP HERE and return to the Lab Guide","title":"Option 1 For instructor supplied AWS accounts"},{"location":"common/documentation/AWS_Credentials.html#option-2-aws-cli","text":"This option uses the AWS CLI. You should use Option 3 if you do not or cannot install the AWS CLI. To see if the AWS CLI is installed: $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.1 or higher is fine If you instead got command not found then either install the AWS CLI or use Option 3 Run aws configure and provide the following values: $ aws configure AWS Access Key ID [*************xxxx]: <Your AWS Access Key ID> AWS Secret Access Key [**************xxxx]: <Your AWS Secret Access Key> Default region name: [us-east-2]: us-east-2 Default output format [None]: json","title":"Option 2 AWS CLI"},{"location":"common/documentation/AWS_Credentials.html#option-3-manually-creating-credential-files","text":"create a .aws directory under your home directory mkdir ~/.aws Change directory to there cd ~/.aws Use a text editor (vim, emacs, notepad) to create a text file (no extension) named credentials . In this file you should have the following text. [default] aws_access_key_id = <Your access key> aws_secret_access_key = <Your secret key> Create a text file (no extension) named config . In this file you should have the following text: [default] region = us-east-2 output = json","title":"Option 3 Manually creating credential files"},{"location":"common/documentation/AWS_Credentials.html#configure-a-session-token-as-part-of-your-credentials","text":"If you used Option 2 or Option 3 , please follow these steps: Determine if you need to configure a session token as part of your credentials AWS Account Do you need a session token? You are attending an in-person workshop and were provided with an AWS account by the instructor yes You are using your own AWS account, and using credentials from an IAM User (most common case) no You are using your own AWS account, and using credentials from an IAM Role yes Do this only if \"yes\", you need to configure a session token Edit the file ~/.aws/credentials The default profile will already be present. Under it add an entry for aws_session_token [default] aws_access_key_id = <Your access key> aws_secret_access_key = <Your secret key> aws_session_token = <your session token>","title":"Configure a session token as part of your credentials"},{"location":"common/documentation/AWS_Credentials.html#clear-environment-variables","text":"If you used option 2 or option 3 then you have put your credentials into files that will be used by the AWS CLI or AWS SDK. However these will preferentially use credentials and configuration in environment variables. Therefore ensure that the following env variables are not set: AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_SESSION_TOKEN AWS_DEFAULT_REGION AWS_DEFAULT_OUTPUT AWS_PROFILE How to do this varies depending on system. For Linux: # Use echo $varname to see if it is set $ echo $AWS_ACCESS_KEY_ID ASIATWOQ3L72RPLOP222 # use unset $ unset AWS_ACCESS_KEY_ID # This now returns no value $ echo $AWS_ACCESS_KEY_ID For your convenience: unset AWS_ACCESS_KEY_ID unset AWS_SECRET_ACCESS_KEY unset AWS_SESSION_TOKEN unset AWS_DEFAULT_REGION unset AWS_DEFAULT_OUTPUT unset AWS_PROFILE","title":"Clear environment variables"},{"location":"common/documentation/AWS_Credentials.html#option-4-powershell","text":"Configuration of AWS credentials 1. If you do not have the AWS Tools for Powershell, download and install them following the instructions here. https://aws.amazon.com/powershell/ . Start a Windows PowerShell for AWS session. If prompted for AWS Secret Key during initialization, type Control-C to break out of the dialog. Configure your AWS credentials with the following PowerShell commands. Note that if you are using an instructor supplied AWS account, you must include the optional SessionToken flag and value as shown below in brackets (omit the brackets when running the command): Set-AWSCredentials -AccessKey <Your access key> -SecretKey <Your secret key> \\ [ -SessionToken <your session key> ] -StoreAs <SomeProfileName> Initialize-AWSDefaults -ProfileName <SomeProfileName> -Region us-east-2 Return to the Lab Guide to continue the lab","title":"Option 4 (PowerShell)"},{"location":"common/documentation/CreateNewS3Bucket.html","text":"Create new S3 bucket These steps will guide you to create a bucket Go to the S3 console at https://console.aws.amazon.com/s3 Click Create bucket For Bucket name supply a name. This must be unique across all buckets in AWS Tip : Name the bucket <first_name><last_initial>_<date in yyyymmdd format> (do NOT include the angle brackets) Click Next three times Review screen: click Create bucket Add object(s) to an S3 bucket Use these instructions to add one or more objects to an S3 bucket Note : You have the option to make the object(s) publically readable . Do NOT do this for S3 buckets used in production, or containing sensitive data. It is recommended you create a new test S3 bucket if you want to host publically readable objects. Click on the name of the bucket you are using (this can be the one you created above) If, and only if, you want to make the uploaded object(s) publically readable then: Click on the Permissions tab Clear both ...access control lists (ACLs) checkboxes (or verify they are already cleared) Click Save Type confirm Click Confirm Click on the Overview tab Drag the file(s) you want to upload to the bucket into the object upload area Click Next If, and only if, you want to make this object(s) publically readable then under Manage public permissions select Grant public read access to this object(s) Click Next two more times Click Upload Return to the Lab Guide to continue the lab","title":"Create new S3 bucket"},{"location":"common/documentation/CreateNewS3Bucket.html#create-new-s3-bucket","text":"These steps will guide you to create a bucket Go to the S3 console at https://console.aws.amazon.com/s3 Click Create bucket For Bucket name supply a name. This must be unique across all buckets in AWS Tip : Name the bucket <first_name><last_initial>_<date in yyyymmdd format> (do NOT include the angle brackets) Click Next three times Review screen: click Create bucket","title":"Create new S3 bucket"},{"location":"common/documentation/CreateNewS3Bucket.html#add-objects-to-an-s3-bucket","text":"Use these instructions to add one or more objects to an S3 bucket Note : You have the option to make the object(s) publically readable . Do NOT do this for S3 buckets used in production, or containing sensitive data. It is recommended you create a new test S3 bucket if you want to host publically readable objects. Click on the name of the bucket you are using (this can be the one you created above) If, and only if, you want to make the uploaded object(s) publically readable then: Click on the Permissions tab Clear both ...access control lists (ACLs) checkboxes (or verify they are already cleared) Click Save Type confirm Click Confirm Click on the Overview tab Drag the file(s) you want to upload to the bucket into the object upload area Click Next If, and only if, you want to make this object(s) publically readable then under Manage public permissions select Grant public read access to this object(s) Click Next two more times Click Upload Return to the Lab Guide to continue the lab","title":"Add object(s) to an S3 bucket"},{"location":"common/documentation/Software_Install.html","text":"Software Install This reference will help you install software necessary to setup your workshop environment AWS CLI jq AWS CLI The AWS Command Line Interface (AWS CLI) is a unified tool that provides a consistent interface for interacting with all parts of AWS. Linux This includes: All native Linux installs MacOS Windows Subsystem for Linux (WSL) Run the following command $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.0 or higher is fine If you instead got command not found then you need to install awscli : $ pip3 install awscli --upgrade --user ...(lots of output)... Successfully installed... * If that succeeded, then you are finished. Return to the Lab Guide If that does not work, then do the following: See the detailed installation instructions here Other environments (not Linux) See the instructions here https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html STOP HERE and return to the Lab Guide jq jq is a command-line JSON processor. is like sed for JSON data. It is used in the workshop bash scripts to parse AWS CLI output. Run the following command $ jq --version jq-1.5-1-a5b5cbe Any version is fine. If you instead got command not found then you need to install jq . Follow the instructions at https://stedolan.github.io/jq/download/ If that succeeded, then you are finished. Return to the Lab Guide Alternate instructions for Linux If the steps above did not work, and you are running Linux, then try the following Download the jq executable $ wget https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 [...lots of output...] jq-linux64 100%[=================================================>] 3.77M 1.12MB/s in 3.5s 2019-10-11 17:41:42 (1.97 MB/s) - \u2018jq-linux64\u2019 saved [3953824/3953824] You can find out what your execution path is with the following command. $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable. $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory. $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq Return to the Lab Guide to continue the lab","title":"Software Install"},{"location":"common/documentation/Software_Install.html#software-install","text":"This reference will help you install software necessary to setup your workshop environment AWS CLI jq","title":"Software Install"},{"location":"common/documentation/Software_Install.html#aws-cli","text":"The AWS Command Line Interface (AWS CLI) is a unified tool that provides a consistent interface for interacting with all parts of AWS.","title":"AWS CLI "},{"location":"common/documentation/Software_Install.html#linux","text":"This includes: All native Linux installs MacOS Windows Subsystem for Linux (WSL) Run the following command $ aws --version aws-cli/1.16.249 Python/3.6.8... AWS CLI version 1.0 or higher is fine If you instead got command not found then you need to install awscli : $ pip3 install awscli --upgrade --user ...(lots of output)... Successfully installed... * If that succeeded, then you are finished. Return to the Lab Guide If that does not work, then do the following: See the detailed installation instructions here","title":"Linux"},{"location":"common/documentation/Software_Install.html#other-environments-not-linux","text":"See the instructions here https://docs.aws.amazon.com/cli/latest/userguide/install-cliv1.html STOP HERE and return to the Lab Guide","title":"Other environments (not Linux)"},{"location":"common/documentation/Software_Install.html#jq","text":"jq is a command-line JSON processor. is like sed for JSON data. It is used in the workshop bash scripts to parse AWS CLI output. Run the following command $ jq --version jq-1.5-1-a5b5cbe Any version is fine. If you instead got command not found then you need to install jq . Follow the instructions at https://stedolan.github.io/jq/download/ If that succeeded, then you are finished. Return to the Lab Guide","title":"jq"},{"location":"common/documentation/Software_Install.html#alternate-instructions-for-linux","text":"If the steps above did not work, and you are running Linux, then try the following Download the jq executable $ wget https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64 [...lots of output...] jq-linux64 100%[=================================================>] 3.77M 1.12MB/s in 3.5s 2019-10-11 17:41:42 (1.97 MB/s) - \u2018jq-linux64\u2019 saved [3953824/3953824] You can find out what your execution path is with the following command. $ echo $PATH /usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin If you have sudo rights, then copy the executable to /usr/local/bin/jq and make it executable. $ sudo cp jq-linux64 /usr/local/bin/jq $ sudo chmod 755 /usr/local/bin/jq If you do not have sudo rights, then copy it into your home directory under a /bin directory. $ cp jq-linux64 ~/bin/jq $ chmod 755 ~/bin/jq Return to the Lab Guide to continue the lab","title":"Alternate instructions for Linux"},{"location":"common/documentation/Workshop_AWS_Account.html","text":"Accessing your instructor-provided AWS account Go to https://dashboard.eventengine.run/login Enter the 12 character hashcode you were provided and click \"Proceed\" [optional] assign a name to your account (this is referred to as \"Team name\") click \"Set Team Name\" Enter a name and click \"Set Team Name\" Click \"AWS Console\" AWS credentials IMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop Copy the whole code block corresponding to the system you are using. Access the AWS console Click \"Open Console\". The AWS Console will open. Return to the Lab Guide to continue the lab","title":"Accessing your instructor-provided AWS account"},{"location":"common/documentation/Workshop_AWS_Account.html#accessing-your-instructor-provided-aws-account","text":"Go to https://dashboard.eventengine.run/login Enter the 12 character hashcode you were provided and click \"Proceed\" [optional] assign a name to your account (this is referred to as \"Team name\") click \"Set Team Name\" Enter a name and click \"Set Team Name\" Click \"AWS Console\"","title":"Accessing your instructor-provided AWS account"},{"location":"common/documentation/Workshop_AWS_Account.html#aws-credentials","text":"IMPORTANT Copy the provided credentials and save them. You wil need these to complete the workshop Copy the whole code block corresponding to the system you are using.","title":"AWS credentials"},{"location":"common/documentation/Workshop_AWS_Account.html#access-the-aws-console","text":"Click \"Open Console\". The AWS Console will open. Return to the Lab Guide to continue the lab","title":"Access the AWS console"}]}